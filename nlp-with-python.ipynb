{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":799971,"sourceType":"datasetVersion","datasetId":150},{"sourceId":1043323,"sourceType":"datasetVersion","datasetId":576263}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/nlp-with-python?scriptVersionId=157025833\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-29T17:21:40.825998Z","iopub.execute_input":"2023-12-29T17:21:40.826453Z","iopub.status.idle":"2023-12-29T17:21:41.272988Z","shell.execute_reply.started":"2023-12-29T17:21:40.826408Z","shell.execute_reply":"2023-12-29T17:21:41.272161Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/religious-and-philosophical-texts/35895-0.txt\n/kaggle/input/religious-and-philosophical-texts/pg2800.txt\n/kaggle/input/religious-and-philosophical-texts/pg2680.txt\n/kaggle/input/religious-and-philosophical-texts/pg10.txt\n/kaggle/input/religious-and-philosophical-texts/pg17.txt\n/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL\n/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL\n/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to populate the documents dictionary**","metadata":{}},{"cell_type":"code","source":"def read_documents ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL\")\n    merged = \" \"\n    # the string variable merged keeps the result of merging the field identifier with its content\n    \n    for a_line in f.readlines ():\n        if a_line.startswith (\".\"):\n            merged += \"\\n\" + a_line.strip ()\n        else:\n            merged += \" \" + a_line.strip ()\n    # updates the merged variable using a for-loop\n    \n    documents = {}\n    \n    content = \"\"\n    doc_id = \"\"\n    # each entry in the dictioanry contains key = doc_id and value = content\n    \n    for a_line in merged.split (\"\\n\"):\n        if a_line.startswith (\".I\"):\n            doc_id = a_line.split (\" \") [1].strip()\n        elif a_line.startswith (\".X\"):\n            documents[doc_id] = content\n            content = \"\"\n            doc_id = \"\"\n        else:\n            content += a_line.strip ()[3:] + \" \"\n    f.close ()\n    return documents\n\n# print out the size of the dictionary and the content of the very first article\ndocuments = read_documents ()\nprint (len (documents))\nprint (documents.get (\"1\"))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:41.274663Z","iopub.execute_input":"2023-12-29T17:21:41.275342Z","iopub.status.idle":"2023-12-29T17:21:41.389565Z","shell.execute_reply.started":"2023-12-29T17:21:41.275309Z","shell.execute_reply":"2023-12-29T17:21:41.388105Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"1460\n 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to populate the queries dictionary**","metadata":{}},{"cell_type":"code","source":"def read_queries ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY\")\n    merged = \"\"\n    \n    # merge the conten of each field with its identifier and separate different fields with lune breaks\n    for a_line in f.readlines ():\n        if a_line.startswith (\".\"):\n            merged += \"\\n\" + a_line.strip ()\n        else:\n            merged += \" \" + a_line.strip ()\n    \n    queries = {}\n    \n    # initialize queries dictionary with key = qry_id and value=content for each query in the dataset\n    content = \"\"\n    qry_id = \"\"\n    \n    for a_line in merged.split (\"\\n\"):\n        if a_line.startswith (\".I\"):\n            if not content == \"\":\n                queries [qry_id] = content\n                content = \"\"\n                qry_id = \"\"\n            # add an enrty to the dictionary when you encounter an .I identifier\n            qry_id = a_line.split(\" \")[1].strip ()\n        # otherwise, keep adding content to the content variable\n        elif a_line.startswith (\".W\") or a_line.startswith (\".T\"):\n            content += a_line.strip ()[3:] + \" \"\n    queries [qry_id] = content\n    f.close ()\n    return queries\n\n# print out the length of the dictionary and the content of the first query\nqueries = read_queries ()\nprint (len (queries))\nprint (queries.get(\"1\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:41.391449Z","iopub.execute_input":"2023-12-29T17:21:41.392301Z","iopub.status.idle":"2023-12-29T17:21:41.413079Z","shell.execute_reply.started":"2023-12-29T17:21:41.392256Z","shell.execute_reply":"2023-12-29T17:21:41.411988Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"112\nWhat problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to populate the mappings dictionary**","metadata":{}},{"cell_type":"code","source":"def read_mappings ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL\")\n    mappings = {}\n    \n    for a_line in f.readlines ():\n        voc = a_line.strip ().split ()\n        key = voc[0].strip ()\n        current_value = voc[1].strip()\n        value = []\n        # update the entry in the mappings dictionary with the current value\n        if key in mappings.keys ():\n            value = mappings.get (key)\n        value.append (current_value)\n        mappings [key] = value\n    f.close ()\n    return mappings\n\n# print out some information about the mapping data structure\nmappings = read_mappings ()\nprint (len (mappings))\nprint (mappings.keys ())\nprint (mappings.get (\"1\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:41.414963Z","iopub.execute_input":"2023-12-29T17:21:41.416007Z","iopub.status.idle":"2023-12-29T17:21:41.44157Z","shell.execute_reply.started":"2023-12-29T17:21:41.415965Z","shell.execute_reply":"2023-12-29T17:21:41.440375Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"76\ndict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Preprocess the data in documents and queries**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport nltk\nfrom nltk import word_tokenize\n\n# text is converted to lowercase and split into words\ndef get_words (text):\n    word_list = [word for word in word_tokenize (text.lower ())]\n    return word_list\n    \ndoc_words = {}\nqry_words = {}\n\nfor doc_id in documents.keys ():\n    doc_words [doc_id] = get_words (documents.get (doc_id))\nfor qry_id in queries.keys ():\n    # entries in both documents and queries are represented as word lists\n    qry_words [qry_id] = get_words (queries.get (qry_id))\n    \n# print out the length of the dictionaries and check the first document and the fisrt query\nprint (len (doc_words))\nprint (doc_words.get (\"1\"))\nprint (len (doc_words.get (\"1\")))\nprint (len (qry_words))\nprint (qry_words.get (\"1\"))\nprint (len (qry_words.get(\"1\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:41.445078Z","iopub.execute_input":"2023-12-29T17:21:41.446256Z","iopub.status.idle":"2023-12-29T17:21:44.52251Z","shell.execute_reply.started":"2023-12-29T17:21:41.44618Z","shell.execute_reply":"2023-12-29T17:21:44.521356Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1460\n['18', 'editions', 'of', 'the', 'dewey', 'decimal', 'classifications', 'comaromi', ',', 'j.p.', 'the', 'present', 'study', 'is', 'a', 'history', 'of', 'the', 'dewey', 'decimal', 'classification', '.', 'the', 'first', 'edition', 'of', 'the', 'ddc', 'was', 'published', 'in', '1876', ',', 'the', 'eighteenth', 'edition', 'in', '1971', ',', 'and', 'future', 'editions', 'will', 'continue', 'to', 'appear', 'as', 'needed', '.', 'in', 'spite', 'of', 'the', 'ddc', \"'s\", 'long', 'and', 'healthy', 'life', ',', 'however', ',', 'its', 'full', 'story', 'has', 'never', 'been', 'told', '.', 'there', 'have', 'been', 'biographies', 'of', 'dewey', 'that', 'briefly', 'describe', 'his', 'system', ',', 'but', 'this', 'is', 'the', 'first', 'attempt', 'to', 'provide', 'a', 'detailed', 'history', 'of', 'the', 'work', 'that', 'more', 'than', 'any', 'other', 'has', 'spurred', 'the', 'growth', 'of', 'librarianship', 'in', 'this', 'country', 'and', 'abroad', '.']\n113\n112\n['what', 'problems', 'and', 'concerns', 'are', 'there', 'in', 'making', 'up', 'descriptive', 'titles', '?', 'what', 'difficulties', 'are', 'involved', 'in', 'automatically', 'retrieving', 'articles', 'from', 'approximate', 'titles', '?', 'what', 'is', 'the', 'usual', 'relevance', 'of', 'the', 'content', 'of', 'articles', 'to', 'their', 'titles', '?']\n38\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Simple Biilean search algorithm**","metadata":{}},{"cell_type":"code","source":"# iterate through the documents\ndef retrieve_documents (doc_words, query):\n    docs = []\n    for doc_id in doc_words.keys ():\n        found = False\n        i = 0\n        while i<len(query) and not found: \n            word = query [i]\n            if word in doc_words.get (doc_id):\n                docs.append (doc_id)\n                found = True\n            else:\n                i+=1\n    return docs\n\n# check the results\ndocs = retrieve_documents (doc_words, qry_words.get(\"3\"))\nprint (docs [:100])\nprint (len (docs))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:44.523807Z","iopub.execute_input":"2023-12-29T17:21:44.524129Z","iopub.status.idle":"2023-12-29T17:21:44.543609Z","shell.execute_reply.started":"2023-12-29T17:21:44.524101Z","shell.execute_reply":"2023-12-29T17:21:44.542794Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102']\n1397\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Begin the preprocessing - remove stopwords and punctuation marks**","metadata":{}},{"cell_type":"code","source":"# import python's string module that will help remove punctuation marks\nimport string\n\n# import the stopwords list\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef process (text):\n    stoplist = set (stopwords.words ('english'))\n    # only add tthe words if they are not included in the stoplist and are not puctuation marks\n    word_list = [word for word in word_tokenize (text.lower())\n                if not word in stoplist and not word in string.punctuation]\n    return word_list\n\n# check the results of these preprocessing steps on some documents or queries\nword_list = process (documents.get (\"1\"))\nprint (word_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:44.5449Z","iopub.execute_input":"2023-12-29T17:21:44.545419Z","iopub.status.idle":"2023-12-29T17:21:44.558181Z","shell.execute_reply.started":"2023-12-29T17:21:44.54539Z","shell.execute_reply":"2023-12-29T17:21:44.557128Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['18', 'editions', 'dewey', 'decimal', 'classifications', 'comaromi', 'j.p.', 'present', 'study', 'history', 'dewey', 'decimal', 'classification', 'first', 'edition', 'ddc', 'published', '1876', 'eighteenth', 'edition', '1971', 'future', 'editions', 'continue', 'appear', 'needed', 'spite', 'ddc', \"'s\", 'long', 'healthy', 'life', 'however', 'full', 'story', 'never', 'told', 'biographies', 'dewey', 'briefly', 'describe', 'system', 'first', 'attempt', 'provide', 'detailed', 'history', 'work', 'spurred', 'growth', 'librarianship', 'country', 'abroad']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Next step in preprocessing - stemming**","metadata":{}},{"cell_type":"code","source":"# import the stemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\ndef process (text):\n    stoplist = set (stopwords.words ('english'))\n    # initialize the LancasterStemmer\n    st = LancasterStemmer ()\n    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n                if not word in stoplist and not word in string.punctuation]\n    return word_list\n\n# check the results on some document, query, or on a list of words\nword_list = process (documents.get(\"26\"))\nprint (word_list)\nword_list = process (\"organize, organizing, organizational, organ, organic, organizer\")\nprint (word_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:44.560372Z","iopub.execute_input":"2023-12-29T17:21:44.560806Z","iopub.status.idle":"2023-12-29T17:21:44.573479Z","shell.execute_reply.started":"2023-12-29T17:21:44.560768Z","shell.execute_reply":"2023-12-29T17:21:44.572432Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['index', 'abstract', 'assocy', 'doyl', 'l.b', 'artic', 'discuss', 'poss', 'exploit', 'stat', 'word', 'co-occurrence', 'text', 'purpos', 'docu', 'retriev', 'co-occurrence', 'defin', 'rel', 'ment', 'process', 'auth', 'read', 'sev', 'mean', 'quantit', 'meas', 'word', 'co-occurrence', 'scrutinized', 'shown', 'strongly', 'co-occurring', 'word', 'pair', 'theref', '``', 'assocy', \"''\", 'stat', 'sens', 'repres', 'form', '``', 'assocy', 'map', \"''\", 'last', 'half', 'artic', 'pres', 'two', 'mod', 'us', 'assocy', 'map', 'lit', 'search']\n['org', 'org', 'org', 'org', 'org', 'org']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Estimate term frequency in documents and queries**","metadata":{}},{"cell_type":"code","source":"def get_terms (text):\n    stoplist = set (stopwords.words ('english'))\n    terms = {}\n    st = LancasterStemmer ()\n    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n                if not word in stoplist and not word in string.punctuation]\n    for word in word_list:\n        terms [word] = terms.get (word, 0) + 1\n    return terms\n\ndoc_terms = {}\nqry_terms = {}\nfor doc_id in documents.keys ():\n    doc_terms [doc_id] = get_terms (documents.get (doc_id))\nfor qry_id in queries.keys ():\n    # populate the term frequency dictionaries for all documents and all queries\n    qry_terms [qry_id] = get_terms (queries.get (qry_id))\n    \n# check the results\nprint (len (doc_terms))\nprint (doc_terms.get (\"1\"))\nprint (len (doc_terms.get(\"1\")))\nprint (len (qry_terms))\nprint (qry_terms.get(\"1\"))\nprint (len (qry_terms.get(\"1\")))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:44.575018Z","iopub.execute_input":"2023-12-29T17:21:44.575644Z","iopub.status.idle":"2023-12-29T17:21:49.049751Z","shell.execute_reply.started":"2023-12-29T17:21:44.575604Z","shell.execute_reply":"2023-12-29T17:21:49.048235Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"1460\n{'18': 1, 'edit': 4, 'dewey': 3, 'decim': 2, 'class': 2, 'comarom': 1, 'j.p.': 1, 'pres': 1, 'study': 1, 'hist': 2, 'first': 2, 'ddc': 2, 'publ': 1, '1876': 1, 'eighteen': 1, '1971': 1, 'fut': 1, 'continu': 1, 'appear': 1, 'nee': 1, 'spit': 1, \"'s\": 1, 'long': 1, 'healthy': 1, 'lif': 1, 'howev': 1, 'ful': 1, 'story': 1, 'nev': 1, 'told': 1, 'biograph': 1, 'brief': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'grow': 1, 'libr': 1, 'country': 1, 'abroad': 1}\n43\n112\n{'problem': 1, 'concern': 1, 'mak': 1, 'describ': 1, 'titl': 3, 'difficul': 1, 'involv': 1, 'autom': 1, 'retriev': 1, 'artic': 2, 'approxim': 1, 'us': 1, 'relev': 1, 'cont': 1}\n14\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to represent the datya in a shared space**","metadata":{}},{"cell_type":"code","source":"# collect the shared vocabulary of terms from documents and queries and return it as a sorted list\ndef collect_vocabulary ():\n    all_terms = []\n    for doc_id in doc_terms.keys ():\n        for term in doc_terms.get (doc_id).keys():\n            all_terms.append (term)\n    for qry_id in qry_terms.keys ():\n        for term in qry_terms.keys():\n            for term in qry_terms.get(qry_id).keys():\n                all_terms.append (term)\n    return sorted (set (all_terms))\n\n# print out the length of the shared vocabulary and check the first several terms in the vocabulary\nall_terms = collect_vocabulary ()\nprint (len (all_terms))\nprint (all_terms [:10])\n\ndef vectorize (input_features, vocabulary):\n    output = {}\n    for item_id in input_features.keys ():\n        features = input_features.get (item_id)\n        output_vector = []\n        for word in vocabulary:\n            if word in features.keys ():\n                output_vector.append (int (features.get (word)))\n            else:\n                output_vector.append (0)\n        output [item_id] = output_vector\n    return output\n\ndoc_vectors = vectorize (doc_terms, all_terms)\nqry_vectors = vectorize (qry_terms, all_terms)\n\n# print out some statistics on these data structures\nprint (len (doc_vectors))\nprint (len (doc_vectors.get (\"1450\")))\nprint (len (qry_vectors))\nprint (len (qry_vectors.get (\"110\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:49.051176Z","iopub.execute_input":"2023-12-29T17:21:49.051672Z","iopub.status.idle":"2023-12-29T17:21:52.111918Z","shell.execute_reply.started":"2023-12-29T17:21:49.05164Z","shell.execute_reply":"2023-12-29T17:21:52.11071Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"8881\n[\"''\", \"'60\", \"'70\", \"'a\", \"'anyhow\", \"'apparent\", \"'b\", \"'basic\", \"'better\", \"'bibliograph\"]\n1460\n8881\n112\n8881\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Calculate and apply inverse document frequency weighting**","metadata":{}},{"cell_type":"code","source":"# import library for math\nimport math\n\ndef calculate_idfs (vocabulary, doc_features):\n    doc_idfs = {}\n    for term in vocabulary:\n        doc_count = 0\n        for doc_id in doc_features.keys ():\n            terms = doc_features.get (doc_id)\n            if term in terms.keys ():\n                doc_count += 1\n        doc_idfs [term] = math.log (float (len (doc_features.keys ()))/\n                                    float (1 + doc_count), 10)\n    return doc_idfs\n\n# check the results - we should have idf values for all terms from the vocabulary\ndoc_idfs = calculate_idfs (all_terms, doc_terms)\nprint (len (doc_idfs))\nprint (doc_idfs.get (\"system\"))\n\n# define a function to apply idf weighing to the input_terms data structure\ndef vectorize_idf (input_terms, input_idfs, vocabulary):\n    output = {}\n    for item_id in input_terms.keys ():\n        terms = input_terms.get (item_id)\n        output_vector = []\n        for term in vocabulary:\n            if term in terms.keys ():\n                # multiply the term frequencies with idf weights if the term is present in document\n                output_vector.append (\n                input_idfs.get (term) * float (terms.get (term)))\n            else:\n                output_vector.append (float (0))\n        output [item_id] = output_vector\n    return output\n\n# apply idf weighing to doc_terms\ndoc_vectors = vectorize_idf (doc_terms, doc_idfs, all_terms)\n\n# print out some statistics, such as the number of documents and terms\nprint (len (doc_vectors))\nprint (len (doc_vectors.get (\"1460\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:52.113472Z","iopub.execute_input":"2023-12-29T17:21:52.113974Z","iopub.status.idle":"2023-12-29T17:21:59.520665Z","shell.execute_reply.started":"2023-12-29T17:21:52.113921Z","shell.execute_reply":"2023-12-29T17:21:59.519489Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"8881\n0.43844122348938885\n1460\n8881\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Run search algorithm for a given query on the set of the documents**","metadata":{}},{"cell_type":"code","source":"# the operator's itemgetter functionality helps sort Python dictionaries by keys or values\nfrom operator import itemgetter\n\n# calculate the length of the input vector\ndef length (vector):\n    sq_length = 0\n    for index in range (0, len(vector)):\n        sq_length += math.pow (vector [index], 2)\n    return math.sqrt (sq_length)\n\n# calculate the dot product of two vectors\ndef dot_product (vector1, vector2):\n    if len (vector1) == len (vector2):\n        dot_prod = 0\n        for index in range (0, len(vector1)):\n            if not vector1 [index] == 0 and not vector2 [index] == 0:\n                dot_prod += vector1 [index] * vector2 [index]\n        return dot_prod\n    else:\n        return \"Unmatching dimensionality\"\n    \ndef calculate_cosine (query, document):\n    cosine = dot_product (query, document) / (length (query) * length (document))\n    return cosine\n\nquery = qry_vectors.get (\"3\")\nresults = {}\n\nfor doc_id in doc_vectors.keys ():\n    document = doc_vectors.get (doc_id)\n    cosine = calculate_cosine (query, document)\n    results [doc_id] = cosine\n    \n# sort the results dictionary by cosine values in descending order and return the top n results\nfor items in sorted (results.items (), key = itemgetter (1), reverse = True) [:44]:\n    print (items [0])","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:21:59.522466Z","iopub.execute_input":"2023-12-29T17:21:59.522902Z","iopub.status.idle":"2023-12-29T17:22:06.066081Z","shell.execute_reply.started":"2023-12-29T17:21:59.522864Z","shell.execute_reply":"2023-12-29T17:22:06.065Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"469\n1179\n1142\n1181\n1190\n445\n1116\n85\n540\n599\n372\n60\n1030\n241\n1161\n965\n1191\n899\n137\n535\n456\n803\n95\n1077\n560\n1095\n166\n544\n1133\n1080\n640\n163\n837\n686\n1082\n1297\n839\n1111\n1428\n1330\n110\n440\n132\n1137\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Estimate precision@k and ratio of cases with at least one relevant document**","metadata":{}},{"cell_type":"code","source":"# calculate the proportion of relevant documents from the gold standard in the top k returned results\ndef calculate_precision (model_output, gold_standard):\n    true_pos = 0\n    for item in model_output:\n        if item in gold_standard:\n            true_pos += 1\n    return float (true_pos) / float (len (model_output))\n\ndef calculate_found (model_output, gold_standard):\n    found = 0\n    for item in model_output:\n        if item in gold_standard:\n            found = 1\n    return float (found)\n\nprecision_all = 0.0\nfound_all = 0.0\nfor query_id in mappings.keys ():\n    # calculate mean values across all queries\n    gold_standard = mappings.get (str (query_id))\n    query = qry_vectors.get (str (query_id))\n    results = {}\n    model_output = []\n    for doc_id in doc_vectors.keys ():\n        document = doc_vectors.get (doc_id)\n        cosine = calculate_cosine (query, document)\n        # for each document, esimate its relevance to the query with cosine similarity as before\n        results [doc_id] = cosine\n    # sort the results and consider only top k (top 5) most relevant documents\n    for items in sorted (results.items (), key = itemgetter (1), reverse = True) [:5]:\n        model_output.append (items [0])\n    precision = calculate_precision (model_output, gold_standard)\n    found = calculate_found (model_output, gold_standard)\n    print (f\"{str (query_id)} : {str(precision)}\")\n    precision_all += precision\n    found_all += found\n    \n# estimate the mean values for all queries\nprint (precision_all / float (len (mappings.keys ())))\nprint (found_all / float (len (mappings.keys ())))    ","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:22:06.067945Z","iopub.execute_input":"2023-12-29T17:22:06.06892Z","iopub.status.idle":"2023-12-29T17:30:12.431273Z","shell.execute_reply.started":"2023-12-29T17:22:06.068874Z","shell.execute_reply":"2023-12-29T17:30:12.430257Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"1 : 1.0\n2 : 0.2\n3 : 0.8\n4 : 0.0\n5 : 0.0\n6 : 0.0\n7 : 0.0\n8 : 0.0\n9 : 0.6\n10 : 0.6\n11 : 0.4\n12 : 0.0\n13 : 0.6\n14 : 0.0\n15 : 0.2\n16 : 0.0\n17 : 0.2\n18 : 0.2\n19 : 0.2\n20 : 0.4\n21 : 0.0\n22 : 0.0\n23 : 0.4\n24 : 1.0\n25 : 0.0\n26 : 0.8\n27 : 0.6\n28 : 0.6\n29 : 0.6\n30 : 0.8\n31 : 0.6\n32 : 0.2\n33 : 0.0\n34 : 0.4\n35 : 0.2\n37 : 0.4\n39 : 0.2\n41 : 0.6\n42 : 0.4\n43 : 0.2\n44 : 0.6\n45 : 0.4\n46 : 0.6\n49 : 0.2\n50 : 0.8\n52 : 0.8\n54 : 0.2\n55 : 1.0\n56 : 0.6\n57 : 0.0\n58 : 0.8\n61 : 0.2\n62 : 0.8\n65 : 0.6\n66 : 1.0\n67 : 0.2\n69 : 0.4\n71 : 0.4\n76 : 1.0\n79 : 0.2\n81 : 0.2\n82 : 0.2\n84 : 0.0\n90 : 0.0\n92 : 0.6\n95 : 0.6\n96 : 0.0\n97 : 0.4\n98 : 0.6\n99 : 0.6\n100 : 0.0\n101 : 0.0\n102 : 0.8\n104 : 0.2\n109 : 0.6\n111 : 0.6\n0.38947368421052636\n0.7631578947368421\n","output_type":"stream"}]},{"cell_type":"markdown","source":"On some queries the algorithm perform very well. For example, \"1 : 1.0\" shows that all top 5 documents returned for query 1 are relevant. However, on other queries the alforithm does not perform well.","metadata":{}},{"cell_type":"markdown","source":"**Estimate mean reciprocal rank**","metadata":{}},{"cell_type":"code","source":"rank_all = 0.0\nfor query_id in mappings.keys ():\n    gold_standard = mappings.get (str (query_id))\n    query = qry_vectors.get (str (query_id))\n    results = {}\n    for doc_id in doc_vectors.keys ():\n        document = doc_vectors.get (doc_id)\n        cosine = calculate_cosine (query, document)\n        results [doc_id] = cosine\n    sorted_results = sorted (results.items (),\n                            key=itemgetter (1), reverse = True)\n    index = 0\n    found = False\n    while found == False:\n        # set the flag found to False and switch it to True when we find the first relevant document\n        item = sorted_results [index]\n        # increment the index with each document in the results\n        index += 1\n        if index == len (sorted_results):\n            found = True\n        if item [0] in gold_standard:\n            # the document ID is the first element in the sorted tuples oof (document_id, similarity score)\n            found = True\n            print (f\"{str(query_id)}: {str(float (1) / float (index))}\")\n            rank_all += float(1) / float (index)\n            \n# print out the mean valur across all queries\nprint (rank_all / float (len (mappings.keys ())))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:30:12.435847Z","iopub.execute_input":"2023-12-29T17:30:12.436292Z","iopub.status.idle":"2023-12-29T17:38:23.528511Z","shell.execute_reply.started":"2023-12-29T17:30:12.436262Z","shell.execute_reply":"2023-12-29T17:38:23.527449Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"1: 1.0\n2: 0.3333333333333333\n3: 1.0\n4: 0.08333333333333333\n5: 0.125\n6: 0.04\n7: 0.05555555555555555\n8: 0.03571428571428571\n9: 0.5\n10: 1.0\n11: 1.0\n12: 0.14285714285714285\n13: 1.0\n14: 0.011764705882352941\n15: 0.2\n16: 0.02857142857142857\n17: 0.25\n18: 0.25\n19: 0.25\n20: 0.5\n21: 0.0625\n22: 0.09090909090909091\n23: 0.3333333333333333\n24: 1.0\n25: 0.14285714285714285\n26: 1.0\n27: 1.0\n28: 1.0\n29: 1.0\n30: 1.0\n31: 0.5\n32: 0.3333333333333333\n33: 0.07142857142857142\n34: 1.0\n35: 0.5\n37: 1.0\n39: 1.0\n41: 1.0\n42: 1.0\n43: 0.2\n44: 0.5\n45: 1.0\n46: 0.5\n49: 0.3333333333333333\n50: 1.0\n52: 1.0\n54: 0.3333333333333333\n55: 1.0\n56: 1.0\n57: 0.1111111111111111\n58: 1.0\n61: 0.5\n62: 1.0\n65: 1.0\n66: 1.0\n67: 0.2\n69: 1.0\n71: 0.25\n76: 1.0\n79: 0.3333333333333333\n81: 1.0\n82: 0.5\n84: 0.08333333333333333\n90: 0.14285714285714285\n92: 1.0\n95: 0.5\n96: 0.058823529411764705\n97: 1.0\n98: 1.0\n99: 0.5\n100: 0.1111111111111111\n101: 0.027777777777777776\n102: 1.0\n104: 0.25\n109: 0.5\n111: 1.0\n0.575993490298831\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Example how to run spaCy's processing pipeline**","metadata":{}},{"cell_type":"code","source":"# import library\nimport spacy\n\n# the spacy.load command initializes the nlp pipeline\nnlp = spacy.load (\"en_core_web_sm\")\ndoc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\nrows = []\n\n# print the output in a tabular format and add a header to the printout for clarity\nrows.append ([\"Word\", \"Position\", \"Lowercase\", \"Lemma\", \"POS\", \"Alphanumeric\", \"Stopword\"])\n\nfor token in doc:\n    rows.append ([token.text, str(token.i), token.lower_, token.lemma_,\n                 token.pos_, str(token.is_alpha), str (token.is_stop)])\n    \n# Python's zip function allows to reformat input from row representation\ncolumns = zip (*rows)\ncolumn_widths = [max (len (item) for item in col)\n                for col in columns]\n\n# calculate the maximum length of strings in each column to allow enough space in the printout\nfor row in rows:\n    print (''.join(' {:{width}} '.format (\n        row [i], width = column_widths [i])\n                  for i in range (0, len (row))))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:23.529742Z","iopub.execute_input":"2023-12-29T17:38:23.530097Z","iopub.status.idle":"2023-12-29T17:38:29.686367Z","shell.execute_reply.started":"2023-12-29T17:38:23.530065Z","shell.execute_reply":"2023-12-29T17:38:29.685549Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":" Word         Position  Lowercase    Lemma        POS    Alphanumeric  Stopword \n On           0         on           on           ADP    True          True     \n monday       1         monday       monday       PROPN  True          False    \n students     2         students     student      NOUN   True          False    \n meet         3         meet         meet         VERB   True          False    \n with         4         with         with         ADP    True          True     \n researchers  5         researchers  researcher   NOUN   True          False    \n              6                                   SPACE  False         False    \n and          7         and          and          CCONJ  True          True     \n discuss      8         discuss      discuss      VERB   True          False    \n future       9         future       future       ADJ    True          False    \n development  10        development  development  NOUN   True          False    \n their        11        their        their        PRON   True          True     \n research     12        research     research     NOUN   True          False    \n .            13        .            .            PUNCT  False         False    \n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Identify all groups of nouns and the way they are realted to each other**","metadata":{}},{"cell_type":"code","source":"doc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\n\n# we can access noun phrases by doc.noun_chunks\nfor chunk in doc.noun_chunks:\n    # print out the phrase, its head, the type of relation to the next most important word, and the word itself\n    print ('\\t'.join ([chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text]))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.687396Z","iopub.execute_input":"2023-12-29T17:38:29.688482Z","iopub.status.idle":"2023-12-29T17:38:29.704104Z","shell.execute_reply.started":"2023-12-29T17:38:29.68845Z","shell.execute_reply":"2023-12-29T17:38:29.703121Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"monday students\tstudents\tnsubj\tmeet\nresearchers\tresearchers\tpobj\twith\nfuture development\tdevelopment\tdobj\tdiscuss\ntheir research\tresearch\tdobj\tdiscuss\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Visualize the dependency information**","metadata":{}},{"cell_type":"code","source":"# import spaCy's visualization tool displaCy\nfrom spacy import displacy\n# path helps define the location for the file to store the visualization\nfrom pathlib import Path\n\n# use displaCy to visualize dependecies over the input text with approptiate arguments\nsvg = displacy.render (doc, style = 'dep', jupyter = False)\nfile_name = '-'.join ([w.text for w in doc if not w.is_punct]) + \".svg\"\n\n# the the output us stored to simply uses the words from the sentence in its name\noutput_path = Path (file_name)\noutput_path.open (\"w\", encoding=\"utf-8\").write(svg)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.705349Z","iopub.execute_input":"2023-12-29T17:38:29.705643Z","iopub.status.idle":"2023-12-29T17:38:29.720913Z","shell.execute_reply.started":"2023-12-29T17:38:29.705618Z","shell.execute_reply":"2023-12-29T17:38:29.720074Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"10561"},"metadata":{}}]},{"cell_type":"markdown","source":"**Print out the information about head and dependents for each word**","metadata":{}},{"cell_type":"code","source":"# coode assumes that spaCy is imported and input text is already fed into the pipeline\nfor token in doc:\n    print (token.text, token.dep_, token.head.text,\n          token.head.pos_, [child for child in token.children])","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.722409Z","iopub.execute_input":"2023-12-29T17:38:29.723006Z","iopub.status.idle":"2023-12-29T17:38:29.729895Z","shell.execute_reply.started":"2023-12-29T17:38:29.722975Z","shell.execute_reply":"2023-12-29T17:38:29.728698Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"On prep meet VERB []\nmonday compound students NOUN []\nstudents nsubj meet VERB [monday]\nmeet ROOT meet VERB [On, students, with, and, discuss, .]\nwith prep meet VERB [researchers]\nresearchers pobj with ADP [ ]\n  dep researchers NOUN []\nand cc meet VERB []\ndiscuss conj meet VERB [development, research]\nfuture amod development NOUN []\ndevelopment dobj discuss VERB [future]\ntheir poss research NOUN []\nresearch dobj discuss VERB [their]\n. punct meet VERB []\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Extarct participants of the actions**","metadata":{}},{"cell_type":"code","source":"# code assumes that spaCy is imported and input text is already fed into pipeline\nfor token in doc:\n    # check that the ROOT of the sentence is a verb with the base form (lemma) \"meet\"\n    if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\"\n       and token.dep_ == \"ROOT\"):\n        # this verb expresses the action itself\n        action = token.text\n        # extract the list of all dependents of this verb using token.children\n        children = [child for child in token.children]\n        participant1 = \"\"\n        participant2 = \"\"\n        for child1 in children:\n            if child1.dep_ == \"nsubj\":\n                participant1 = \" \".join (\n                [attr.text for attr in child1.children]\n                ) + \" \" + child1.text\n            elif child1.text == \"with\":\n                # check if the verb has preposition \"with\" as one of its dependents\n                action += \" \" + child1.text\n                child1_children = [child for child in child1.children]\n                for child2 in child1_children:\n                    if child2.pos_ == \"NOUN\":\n                        participant2 = \" \".join (\n                        [attr.text for attr in child2.children]\n                        ) + \" \" + child2.text\n                    \n# print out the results\nprint (f\"Participant1 = {participant1}\")\nprint (f\"Action = {action}\")\nprint (f\"Participant2 = {participant2}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.731482Z","iopub.execute_input":"2023-12-29T17:38:29.732015Z","iopub.status.idle":"2023-12-29T17:38:29.746462Z","shell.execute_reply.started":"2023-12-29T17:38:29.731966Z","shell.execute_reply":"2023-12-29T17:38:29.745238Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Participant1 = monday students\nAction = meet with\nParticipant2 =   researchers\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Build information extractor**","metadata":{}},{"cell_type":"code","source":"# provide diverse set of sentences\nsentences = [\"On monday students meet with researchers \" + \" and discuss future development their research.\", \n            \" Warren Baffet met with the President last week.\",\n            \"Elon Musk met with the President an White House.\",\n            \"The two bussinesmans also posed for photographs and \" + \n            \"the Vice President talked to reporters.\"]\n\n# define a function to apply all the steps in the information extraction algorithm\ndef extract_information (doc):\n    action = \"\"\n    participant1 = \"\"\n    for token in doc: \n         if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\" \n            and token.dep_ == \"ROOT\"):\n                action = token.text\n                children = [child for child in token.children]\n                for child1 in children:\n                    if child1.dep_ == \"nsubj\": \n                        patricipant1 = \" \".join (\n                [attr.text for attr in child1.children]\n                ) + \" \" + child1.text\n                    elif child1.text == \"with\":\n                        action += \" \" + child1.text\n                        child1_children = [child for child in child1.children]\n                        for child2 in child1_children:\n                            # extract participants expressed with proper nouns (PROPN) and common nouns (NOUN)\n                            if (child2.pos_ == \"NOUN\"\n                            or child2.pos_ == \"PROPN\"):\n                                participant2 = \" \".join (\n                        [attr.text for attr in child2.children]\n                        ) + \" \" + child2.text\n                    elif (child1.dep_ == \"dobj\"\n                        and (child1.pos_ == \"NOUN\"\n                            or child1.pos_ == \"PROPN\")):\n                        participant2 = \" \".join (\n                            [attr.text for attr in child1.children]\n                            ) + \" \" + child1.text\n    \n        \n# apply extract_information function to each sentence and print out the actions and participants\nfor sent in sentences:\n    print (f\"\\nSentence = {sent}\")\n    doc = nlp (sent)\n    extract_information (doc)\n    print (f\"Participant1 = {participant1}\")\n    print (f\"Action = {action}\")\n    print (f\"Participant2 = {participant2}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.748553Z","iopub.execute_input":"2023-12-29T17:38:29.749008Z","iopub.status.idle":"2023-12-29T17:38:29.799779Z","shell.execute_reply.started":"2023-12-29T17:38:29.748945Z","shell.execute_reply":"2023-12-29T17:38:29.798836Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nSentence = On monday students meet with researchers  and discuss future development their research.\nParticipant1 = monday students\nAction = meet with\nParticipant2 =   researchers\n\nSentence =  Warren Baffet met with the President last week.\nParticipant1 = monday students\nAction = meet with\nParticipant2 =   researchers\n\nSentence = Elon Musk met with the President an White House.\nParticipant1 = monday students\nAction = meet with\nParticipant2 =   researchers\n\nSentence = The two bussinesmans also posed for photographs and the Vice President talked to reporters.\nParticipant1 = monday students\nAction = meet with\nParticipant2 =   researchers\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to extract literary works from Project Gutenberg**","metadata":{}},{"cell_type":"code","source":"nltk.download ('gutenberg')\nfrom nltk.corpus import gutenberg\n\n# print out the names of files\ngutenberg.fileids ()","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.80115Z","iopub.execute_input":"2023-12-29T17:38:29.801586Z","iopub.status.idle":"2023-12-29T17:38:29.896119Z","shell.execute_reply.started":"2023-12-29T17:38:29.801549Z","shell.execute_reply":"2023-12-29T17:38:29.89502Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package gutenberg to /usr/share/nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['austen-emma.txt',\n 'austen-persuasion.txt',\n 'austen-sense.txt',\n 'bible-kjv.txt',\n 'blake-poems.txt',\n 'bryant-stories.txt',\n 'burgess-busterbrown.txt',\n 'carroll-alice.txt',\n 'chesterton-ball.txt',\n 'chesterton-brown.txt',\n 'chesterton-thursday.txt',\n 'edgeworth-parents.txt',\n 'melville-moby_dick.txt',\n 'milton-paradise.txt',\n 'shakespeare-caesar.txt',\n 'shakespeare-hamlet.txt',\n 'shakespeare-macbeth.txt',\n 'whitman-leaves.txt']"},"metadata":{}}]},{"cell_type":"markdown","source":"**Define training and test sets**","metadata":{}},{"cell_type":"code","source":"nltk.download ('punkt')\n\nauthor1_train = gutenberg.sents ('chesterton-ball.txt') + gutenberg.sents ('chesterton-brown.txt')\nprint (author1_train)\nprint (len (author1_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:29.8978Z","iopub.execute_input":"2023-12-29T17:38:29.898339Z","iopub.status.idle":"2023-12-29T17:38:30.486534Z","shell.execute_reply.started":"2023-12-29T17:38:29.8983Z","shell.execute_reply":"2023-12-29T17:38:30.485716Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[['[', 'The', 'Ball', 'and', 'The', 'Cross', 'by', 'G', '.', 'K', '.', 'Chesterton', '1909', ']'], ['I', '.'], ...]\n8585\n","output_type":"stream"}]},{"cell_type":"code","source":"# initialize the test set with the sentences from the third work by the author\nauthor1_test = gutenberg.sents ('chesterton-thursday.txt')\nprint (author1_test)\nprint (len (author1_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:30.487596Z","iopub.execute_input":"2023-12-29T17:38:30.488168Z","iopub.status.idle":"2023-12-29T17:38:30.700412Z","shell.execute_reply.started":"2023-12-29T17:38:30.488135Z","shell.execute_reply":"2023-12-29T17:38:30.699279Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[['[', 'The', 'Man', 'Who', 'Was', 'Thursday', 'by', 'G', '.', 'K', '.', 'Chesterton', '1908', ']'], ['To', 'Edmund', 'Clerihew', 'Bentley'], ...]\n3742\n","output_type":"stream"}]},{"cell_type":"code","source":"author2_train = gutenberg.sents ('shakespeare-caesar.txt') + gutenberg.sents ('shakespeare-hamlet.txt')\nprint (author2_train)\nprint (len (author2_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:30.701738Z","iopub.execute_input":"2023-12-29T17:38:30.702496Z","iopub.status.idle":"2023-12-29T17:38:30.933015Z","shell.execute_reply.started":"2023-12-29T17:38:30.702462Z","shell.execute_reply":"2023-12-29T17:38:30.93196Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']'], ['Actus', 'Primus', '.'], ...]\n5269\n","output_type":"stream"}]},{"cell_type":"code","source":"author2_test = gutenberg.sents ('shakespeare-macbeth.txt')\nprint (author2_test)\nprint (len (author2_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:30.934392Z","iopub.execute_input":"2023-12-29T17:38:30.934717Z","iopub.status.idle":"2023-12-29T17:38:31.0235Z","shell.execute_reply.started":"2023-12-29T17:38:30.934689Z","shell.execute_reply":"2023-12-29T17:38:31.02265Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]\n1907\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Calculate simple statistics on texts**","metadata":{}},{"cell_type":"code","source":"def statistics (gutenberg_data):\n    for work in gutenberg_data:\n        # use NLTK's functionality to calculate statistics\n        num_chars = len (gutenberg.raw (work))\n        num_words = len (gutenberg.words (work))\n        num_sents = len (gutenberg.sents (work))\n        num_vocab = len (set (w.lower ()\n                             for w in gutenberg.words (work)))\n        print (round (num_chars / num_words),\n              round (num_words / num_sents),\n              round (num_words / num_vocab),\n              work)\n        \ngutenberg_data = ['chesterton-ball.txt','chesterton-brown.txt','chesterton-thursday.txt', \n                  'shakespeare-caesar.txt','shakespeare-hamlet.txt','shakespeare-macbeth.txt']\nstatistics (gutenberg_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:31.024943Z","iopub.execute_input":"2023-12-29T17:38:31.025317Z","iopub.status.idle":"2023-12-29T17:38:32.697572Z","shell.execute_reply.started":"2023-12-29T17:38:31.025287Z","shell.execute_reply":"2023-12-29T17:38:32.696143Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"5 20 12 chesterton-ball.txt\n5 23 11 chesterton-brown.txt\n5 18 11 chesterton-thursday.txt\n4 12 9 shakespeare-caesar.txt\n4 12 8 shakespeare-hamlet.txt\n4 12 7 shakespeare-macbeth.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Run StratifiedShufflingSplit on the data**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport random\nimport sklearn\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nall_sents = [(sent, 'chesterton') for sent in author1_train]\nall_sents += [(sent, 'shakespeare') for sent in author2_train]\n# combine all sentences into a single list called all_sents, keeping the author label\nprint (f\"Dataset size = {str (len (all_sents))} sentences\")\n\n# keep the set of labels (authors) as values\nvalues = [author for (sent, author) in all_sents]\nsplit = StratifiedShuffleSplit (n_splits = 1, test_size = 0.2, random_state = 42)\nstrat_train_set = []\nstrat_pretest_set = []\nfor train_index, pretest_index in split.split (all_sents, values):\n    strat_train_set= [all_sents [index] for index in train_index]\n    strat_pretest_set = [all_sents [index]\n                        for index in pretest_index]","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:32.699391Z","iopub.execute_input":"2023-12-29T17:38:32.699815Z","iopub.status.idle":"2023-12-29T17:38:33.475149Z","shell.execute_reply.started":"2023-12-29T17:38:32.699777Z","shell.execute_reply":"2023-12-29T17:38:33.474075Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Dataset size = 13854 sentences\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Check the proportions of the data in the two classes**","metadata":{}},{"cell_type":"code","source":"# calculate the proportion of the entries in each class (category) in the given dataset data\ndef cat_proportions (data, cat):\n    count = 0\n    for item in data:\n        if item [1] == cat:\n            count += 1\n    return float (count) / float (len (data))\n\ncategories = ['chesterton', 'shakespeare']\nrows = []\nrows.append ([\"Category\", \"Overall\", \"Stratified train\", \"Stratified pretest\"])\n\nfor cat in categories:\n    rows.append ([cat, f\"{cat_proportions (all_sents, cat):.6f}\",\n                 f\"{cat_proportions (strat_train_set, cat):.6f}\",\n                 f\"{cat_proportions (strat_pretest_set, cat):.6f}\"])\n    \ncolumns = zip (*rows)\ncolumn_widths = [max (len (item) for item in col) for col in columns]\nfor row in rows:\n    print (''.join (' {:{width}} '.format (row [i], width = column_widths [i])\n                   for i in range (0, len (row))))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:33.476663Z","iopub.execute_input":"2023-12-29T17:38:33.477296Z","iopub.status.idle":"2023-12-29T17:38:33.494161Z","shell.execute_reply.started":"2023-12-29T17:38:33.47726Z","shell.execute_reply":"2023-12-29T17:38:33.493208Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":" Category     Overall   Stratified train  Stratified pretest \n chesterton   0.619677  0.619688          0.619632           \n shakespeare  0.380323  0.380312          0.380368           \n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create the test_set data structure**","metadata":{}},{"cell_type":"code","source":"test_set = [(sent, \"chesterton\") for sent in author1_test]\ntest_set += [(sent, \"shakespeare\") for sent in author2_test]\n\n# extract words as features\ndef get_features (text):\n    features = {}\n    word_list = [word for word in text]\n    for word in word_list:\n        features [word] = True\n    return features\n\n# extract features from training and pretest sets\ntrain_features = [(get_features (sents), label)\n                 for (sents, label) in strat_train_set]\npretest_features = [(get_features (sents), label)\n                   for (sents, label) in strat_pretest_set]\n\n# run some checks to see what the data contains\nprint (len (train_features))\nprint (train_features [0] [0])\nprint (train_features [100] [0])","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:33.495434Z","iopub.execute_input":"2023-12-29T17:38:33.49574Z","iopub.status.idle":"2023-12-29T17:38:34.196187Z","shell.execute_reply.started":"2023-12-29T17:38:33.495714Z","shell.execute_reply":"2023-12-29T17:38:34.195188Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"11083\n{'\"': True, 'Oh': True, ',': True, 'you': True, 'know': True, 'what': True, 'I': True, 'mean': True, ',\"': True, 'said': True, 'Turnbull': True, 'impatiently': True, '.': True}\n{'The': True, 'shop': True, 'and': True, 'the': True, 'Cross': True, 'were': True, 'equally': True, 'uplifted': True, 'alone': True, 'in': True, 'empty': True, 'heavens': True, '.': True}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Train the Naive Bayes classifier on train and test on pretest set**","metadata":{}},{"cell_type":"code","source":"# import the classifier\nfrom nltk import NaiveBayesClassifier, classify\n\n# train the classifier on the training set\nprint (f\"Training set size = {str (len (train_features))} sentences\")\nprint (f\"Pretest set size = {str (len (pretest_features))} sentences\")\nclassifier = NaiveBayesClassifier.train (train_features)\n\nprint (f\"Accuracy on the training set = {str (classify.accuracy (classifier, train_features))}\")\nprint (f\"Accuracy on the pretest set = \" + \n      f\"{str (classify.accuracy (classifier, pretest_features))}\")\nclassifier.show_most_informative_features (50)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:34.19747Z","iopub.execute_input":"2023-12-29T17:38:34.197796Z","iopub.status.idle":"2023-12-29T17:38:36.321967Z","shell.execute_reply.started":"2023-12-29T17:38:34.197768Z","shell.execute_reply":"2023-12-29T17:38:36.320819Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Training set size = 11083 sentences\nPretest set size = 2771 sentences\nAccuracy on the training set = 0.9770820175042858\nAccuracy on the pretest set = 0.9581378563695416\nMost Informative Features\n                  Caesar = True           shakes : cheste =     61.7 : 1.0\n                   voice = True           cheste : shakes =     56.3 : 1.0\n                      st = True           shakes : cheste =     51.6 : 1.0\n                    thee = True           shakes : cheste =     51.2 : 1.0\n                    thou = True           shakes : cheste =     49.4 : 1.0\n                   quite = True           cheste : shakes =     41.1 : 1.0\n                    been = True           cheste : shakes =     40.6 : 1.0\n                 because = True           cheste : shakes =     39.5 : 1.0\n                     thy = True           shakes : cheste =     37.5 : 1.0\n                    said = True           cheste : shakes =     35.6 : 1.0\n                   white = True           cheste : shakes =     35.4 : 1.0\n                     sea = True           cheste : shakes =     34.6 : 1.0\n                 looking = True           cheste : shakes =     34.2 : 1.0\n                  behind = True           cheste : shakes =     33.8 : 1.0\n                     got = True           cheste : shakes =     31.3 : 1.0\n                    mean = True           cheste : shakes =     31.3 : 1.0\n                 Caesars = True           shakes : cheste =     29.9 : 1.0\n                       O = True           shakes : cheste =     29.5 : 1.0\n                     has = True           cheste : shakes =     28.8 : 1.0\n                   small = True           cheste : shakes =     28.8 : 1.0\n                    grey = True           cheste : shakes =     26.8 : 1.0\n                     own = True           cheste : shakes =     25.9 : 1.0\n                   cried = True           cheste : shakes =     25.7 : 1.0\n                     Nay = True           shakes : cheste =     25.5 : 1.0\n                   think = True           cheste : shakes =     24.2 : 1.0\n                    back = True           cheste : shakes =     23.6 : 1.0\n                  moment = True           cheste : shakes =     23.0 : 1.0\n                  Mother = True           shakes : cheste =     22.5 : 1.0\n                    want = True           cheste : shakes =     22.5 : 1.0\n                       d = True           shakes : cheste =     22.1 : 1.0\n                   large = True           cheste : shakes =     19.8 : 1.0\n                     far = True           cheste : shakes =     19.8 : 1.0\n                   front = True           cheste : shakes =     19.0 : 1.0\n                    mind = True           cheste : shakes =     19.0 : 1.0\n                      ha = True           shakes : cheste =     19.0 : 1.0\n                    talk = True           cheste : shakes =     18.2 : 1.0\n                   broke = True           cheste : shakes =     17.8 : 1.0\n                    side = True           cheste : shakes =     17.1 : 1.0\n                  except = True           cheste : shakes =     17.0 : 1.0\n                       4 = True           shakes : cheste =     16.8 : 1.0\n                 whether = True           cheste : shakes =     16.2 : 1.0\n                    seen = True           cheste : shakes =     15.8 : 1.0\n                    ship = True           cheste : shakes =     15.8 : 1.0\n                    wild = True           cheste : shakes =     15.8 : 1.0\n                  coming = True           cheste : shakes =     15.8 : 1.0\n                     bee = True           shakes : cheste =     15.7 : 1.0\n                   World = True           shakes : cheste =     15.7 : 1.0\n                 instant = True           cheste : shakes =     15.6 : 1.0\n                answered = True           cheste : shakes =     15.5 : 1.0\n                     low = True           cheste : shakes =     14.9 : 1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to extract words and sentence length statistics**","metadata":{}},{"cell_type":"code","source":"def avg_number_chars (text):\n    total_chars = 0.0\n    for word in text:\n        total_chars += len (word)\n    return float (total_chars) / float (len(text))\n\n# calculate the sentence length in terms of the number of words\ndef number_words (text):\n    return float (len (text))\n\nprint (avg_number_chars ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))\nprint (number_words ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:36.323939Z","iopub.execute_input":"2023-12-29T17:38:36.32428Z","iopub.status.idle":"2023-12-29T17:38:36.33133Z","shell.execute_reply.started":"2023-12-29T17:38:36.324249Z","shell.execute_reply":"2023-12-29T17:38:36.330485Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"3.5714285714285716\n7.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Code to extract features and map them to the labels**","metadata":{}},{"cell_type":"code","source":"# argument source denotes the dataset we are applying the feature extraction\ndef initialize_dataset (source):\n    all_features = []\n    targets = []\n    # iterate through all (sent, label) pairs in the given dataset\n    for (sent, label) in source:\n        feature_list = []\n        feature_list.append (avg_number_chars (sent))\n        feature_list.append (number_words (sent))\n        all_features.append (feature_list)\n        if label == \"chesterton\": targets.append (0)\n        else: targets.append (1)\n    return all_features, targets\n\ntrain_data, train_targets = initialize_dataset (strat_train_set)\npretest_data, pretest_targets = initialize_dataset (strat_pretest_set)\ntest_data, test_targets = initialize_dataset (test_set)\n\n# print out thr length of the structures\nprint (len (train_data), len (train_targets))\nprint (len (pretest_data), len (pretest_targets))\nprint (len (test_data), len (test_targets))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:36.332636Z","iopub.execute_input":"2023-12-29T17:38:36.333742Z","iopub.status.idle":"2023-12-29T17:38:36.420555Z","shell.execute_reply.started":"2023-12-29T17:38:36.333699Z","shell.execute_reply":"2023-12-29T17:38:36.419794Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"11083 11083\n2771 2771\n5649 5649\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Train and test a classifier with sklearn**","metadata":{}},{"cell_type":"code","source":"# import decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# initialization\ntext_clf = DecisionTreeClassifier (random_state = 42)\n\n# train the classifier using the fit method\ntext_clf.fit (train_data, train_targets)\n\n# test the classifier using the predict method\npredicted = text_clf.predict (pretest_data)\n\n# evaluating the classifier\n# import numpy and sklearn's metrics funcvtionality\nimport numpy as np\nfrom sklearn import metrics\n\ndef evaluate (predicted, targets):\n    # use numpy.mean to estimate the accuracy of the classifier\n    print (np.mean (predicted == targets))\n    print (metrics.confusion_matrix (targets, predicted))\n    print (metrics.classification_report (targets, predicted))\n    \nevaluate (predicted, pretest_targets)\n\n# apply the same routine to the test set\npredicted = text_clf.predict (test_data)\nevaluate (predicted, test_targets)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:36.421862Z","iopub.execute_input":"2023-12-29T17:38:36.422414Z","iopub.status.idle":"2023-12-29T17:38:36.583659Z","shell.execute_reply.started":"2023-12-29T17:38:36.422383Z","shell.execute_reply":"2023-12-29T17:38:36.582561Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0.7470227354745579\n[[1485  232]\n [ 469  585]]\n              precision    recall  f1-score   support\n\n           0       0.76      0.86      0.81      1717\n           1       0.72      0.56      0.63      1054\n\n    accuracy                           0.75      2771\n   macro avg       0.74      0.71      0.72      2771\nweighted avg       0.74      0.75      0.74      2771\n\n0.7603115595680652\n[[3194  548]\n [ 806 1101]]\n              precision    recall  f1-score   support\n\n           0       0.80      0.85      0.83      3742\n           1       0.67      0.58      0.62      1907\n\n    accuracy                           0.76      5649\n   macro avg       0.73      0.72      0.72      5649\nweighted avg       0.75      0.76      0.76      5649\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Calculate the number and proportion of times certain words occur**","metadata":{}},{"cell_type":"code","source":"def word_counts(text):\n    counts = {}\n    for word in text:\n        counts[word.lower()] = counts.get(word.lower(), 0) + 1\n    return counts\n\ndef proportion_words(text, wordlist):\n    count = 0\n    for word in text:\n        if word.lower() in wordlist:\n            count += 1\n    return float(count)/float(len(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:36.585124Z","iopub.execute_input":"2023-12-29T17:38:36.585853Z","iopub.status.idle":"2023-12-29T17:38:36.591388Z","shell.execute_reply.started":"2023-12-29T17:38:36.585823Z","shell.execute_reply":"2023-12-29T17:38:36.59049Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**Adding stopword counts and proportion as features**","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# add spaCys functionality to the code and upload the stopwords list\nnlp = spacy.load('en_core_web_lg')\ndef initialize_dataset(source):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list=[]\n        feature_list.append(avg_number_chars(sent))\n        feature_list.append(number_words(sent))\n        counts = word_counts(sent)\n        for word in STOP_WORDS:\n            if word in counts.keys():\n                feature_list.append(counts.get(word))\n            else:\n                feature_list.append(0)\n        feature_list.append(proportion_words(sent, STOP_WORDS))\n        all_features.append(feature_list)\n        if label==\"austen\": targets.append(0)\n        else: targets.append(1)\n    return all_features, targets\n\ntrain_data, train_targets = initialize_dataset(strat_train_set)\npretest_data, pretest_targets = initialize_dataset(strat_pretest_set)\ntest_data, test_targets = initialize_dataset(test_set)\n\n# Print out the length of the feature lists and targets lists\nprint (len(train_data), len(train_targets))\nprint (len(pretest_data), len(pretest_targets))\nprint (len(test_data), len(test_targets))","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:36.592762Z","iopub.execute_input":"2023-12-29T17:38:36.593662Z","iopub.status.idle":"2023-12-29T17:38:43.487938Z","shell.execute_reply.started":"2023-12-29T17:38:36.593624Z","shell.execute_reply":"2023-12-29T17:38:43.486833Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"11083 11083\n2771 2771\n5649 5649\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Evaluate the results**","metadata":{}},{"cell_type":"code","source":"# train the classifier on the training data\ntext_clf = DecisionTreeClassifier(random_state=42)\ntext_clf.fit(train_data, train_targets)\n\n# test on the pretest set\npredicted = text_clf.predict(pretest_data)\nevaluate(predicted, pretest_targets)\n\n# apply the same routine to the test set\npredicted = text_clf.predict(test_data)\nevaluate(predicted, test_targets)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:43.489379Z","iopub.execute_input":"2023-12-29T17:38:43.490336Z","iopub.status.idle":"2023-12-29T17:38:43.92481Z","shell.execute_reply.started":"2023-12-29T17:38:43.490302Z","shell.execute_reply":"2023-12-29T17:38:43.923749Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"1.0\n[[2771]]\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00      2771\n\n    accuracy                           1.00      2771\n   macro avg       1.00      1.00      1.00      2771\nweighted avg       1.00      1.00      1.00      2771\n\n1.0\n[[5649]]\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00      5649\n\n    accuracy                           1.00      5649\n   macro avg       1.00      1.00      1.00      5649\nweighted avg       1.00      1.00      1.00      5649\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Applying spaCy preprocessing**","metadata":{}},{"cell_type":"code","source":"# provide the preprocess function with the original sentences from the datasets\ndef preprocess(source):\n    source_docs = {}\n    index = 0\n    for (sent, label) in source:\n        text = \" \".join(sent)\n        source_docs[text] = nlp(text)\n        if index>0 and (index%2000)==0:\n            print(str(index) + \" texts processed\")\n        index += 1\n    print(\"Dataset processed\")\n    return source_docs\n\n# apply the preprocess function to the three original datasets\ntrain_docs = preprocess(strat_train_set)\npretest_docs = preprocess(strat_pretest_set)\ntest_docs = preprocess(test_set)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T17:38:43.926293Z","iopub.execute_input":"2023-12-29T17:38:43.927037Z","iopub.status.idle":"2023-12-29T17:41:44.900018Z","shell.execute_reply.started":"2023-12-29T17:38:43.926987Z","shell.execute_reply":"2023-12-29T17:41:44.89881Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"2000 texts processed\n4000 texts processed\n6000 texts processed\n8000 texts processed\n10000 texts processed\nDataset processed\n2000 texts processed\nDataset processed\n2000 texts processed\n4000 texts processed\nDataset processed\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Adding distribution of part-of-speech tags as features**","metadata":{}},{"cell_type":"code","source":"# import Pythons Counter functionality to simplify counting procedures\nfrom collections import Counter\npos_list = [\"C\", \"D\", \"E\", \"F\", \"I\", \"J\", \"M\",\n            \"N\", \"P\", \"R\", \"T\", \"U\", \"V\", \"W\"]\n\ndef pos_counts(text, source_docs, pos_list):\n    pos_counts = {}\n    doc = source_docs.get(\" \".join(text))\n    tags = []\n    for word in doc:\n        tags.append(str(word.tag_)[0])\n    counts = Counter(tags)\n    for pos in pos_list:\n        if pos in counts.keys():\n            pos_counts[pos] = counts.get(pos)\n        # Populate the pos_counts dictionary using the counts\n        # of the part-of-speech tags or inserting 0    \n        else: pos_counts[pos] = 0\n    return pos_counts\n\ndef initialize_dataset(source, source_docs):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list=[]\n        feature_list.append(avg_number_chars(sent))\n        feature_list.append(number_words(sent))\n        counts = word_counts(sent)\n        for word in STOP_WORDS:\n            if word in counts.keys():\n                feature_list.append(counts.get(word))\n            else:\n                feature_list.append(0)\n        feature_list.append(proportion_words(sent, STOP_WORDS))\n        # extract the previous 308 features as before\n        p_counts = pos_counts(sent, source_docs, pos_list)\n        for pos in p_counts.keys():\n            feature_list.append(float(p_counts.get(pos))/float(len(sent)))\n        all_features.append(feature_list)\n        if label==\"austen\": targets.append(0)\n        else: targets.append(1)\n    return all_features, targets","metadata":{"execution":{"iopub.status.busy":"2023-12-29T18:13:24.399212Z","iopub.execute_input":"2023-12-29T18:13:24.399608Z","iopub.status.idle":"2023-12-29T18:13:24.411543Z","shell.execute_reply.started":"2023-12-29T18:13:24.399579Z","shell.execute_reply":"2023-12-29T18:13:24.410375Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"**Collecting the most frequent suffixes from the data**","metadata":{}},{"cell_type":"code","source":"# import python operator functionality\nimport operator\n\n# create the function\ndef select_suffixes (cutoff):\n    all_suffixes = []\n    # iterate through the list of values in the train_docs.values ()\n    for doc in train_docs.values ():\n        for word in doc:\n            all_suffixes.append (str (word.suffix_).lower ())\n    counts = Counter (all_suffixes)\n    # store the frequency of all the suffixes in the counts dictionary and the sort it\n    sorted_counts = sorted (counts.items (), key = operator.itemgetter (1),\n                           reverse = True)\n    selected_suffixes = []\n    for i in range (0, round (len (counts)*cutoff)):\n        selected_suffixes.append (sorted_counts [i][0])\n    return selected_suffixes\n\nselected_suffixes = select_suffixes (0.4)\nprint (len (selected_suffixes))\nprint (selected_suffixes)","metadata":{"execution":{"iopub.status.busy":"2023-12-29T18:17:05.631171Z","iopub.execute_input":"2023-12-29T18:17:05.631609Z","iopub.status.idle":"2023-12-29T18:17:05.824737Z","shell.execute_reply.started":"2023-12-29T18:17:05.631578Z","shell.execute_reply":"2023-12-29T18:17:05.823678Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"690\n[',', 'the', '.', 'and', '\"', 'of', 'a', 'ing', 'to', 'hat', 'his', 'in', 'i', 'he', \"'\", 'it', 'you', 'was', 'ere', 'ith', 'her', ';', 'all', 'is', 'but', 'as', '?', 'not', ':', 'our', 'ght', 'for', 'ent', 'ion', 'man', 'had', '-', 'aid', 'on', 'are', 'out', 'ill', 'hen', 'one', 'him', 'my', 'be', 'ome', 'at', 'nce', 'uld', 'ter', 'ted', 's', 'red', 'ore', 'own', 'ike', 'or', 'hey', 'now', 'ave', 'ell', 'so', 'by', 'if', 'me', 'we', 'ull', 'ich', 'ble', 'ked', 'ers', 'ery', 'an', 'ood', 'ver', 'ace', 'ess', 'lly', 'est', 'rom', 'een', 't', 'ned', 'ame', 'no', 'ong', 'ose', 'ost', 'do', 'ine', 'ord', 'ian', 'nly', 'ure', 'old', 'uch', 'up', '--', 'ous', 'tly', 'ant', 'led', 'ake', 'ust', 'who', 'ied', 'ite', 'ate', 'd', 'ain', 'ard', 'ead', 'use', 'der', 'ely', 'can', 'aue', 'hem', 'ugh', 'did', 'ast', 'ice', 'let', 'und', 'nto', 'tle', 'rds', 'ity', 'eir', '!', 'way', 'yes', 'ack', 'men', 'ind', 'ven', 'any', 'nds', 'med', 'ons', 'end', 'ide', 'ies', 'sed', 'how', 'ade', 'two', 'low', 'say', 'ile', 'ded', 'ook', 'hed', 'art', 'age', 'see', 'ngs', 'han', 'elf', 'ger', 'ius', 'ink', 'eat', 'ive', 'ath', 'am', 'don', 'why', 'les', 'ime', 'ort', 'mes', 'ish', 'den', 'she', 'pon', 'ear', 'oke', 'son', 'rst', 'may', 'res', 'has', 'nts', 'ree', 'air', 'hou', 'ily', 'its', 'ped', 'ean', 'sse', 'ple', 'oor', 'yet', 'oth', 'ung', 'ire', 'too', 'ary', 'nes', 'god', 'thy', 'dly', 'ese', 'rse', 'sar', 'wer', 'us', 'eet', 'ady', 'iue', '(', 'ses', ')', 'go', 'eed', 'ful', 'mr', 'ged', 'tic', 'ect', 'ist', 'rth', 'sir', 'ved', 'rld', 'kes', 'ass', 'ase', 'ole', 'ise', 'tes', 'ife', 'ken', 'ene', 'tus', 'new', 'oss', 'tor', 'day', 'saw', 'ays', 'ner', 'hee', 'oh', 'ars', 'ody', 'int', 'cke', 'ten', 'lfe', 'per', 'ond', 'ces', 'nge', 'ove', 'ves', 'wne', 'van', 'sts', 'o', 'ark', 'cal', 'ach', 'oes', 'row', 'nst', 'off', 'ory', 'ral', 'dge', 'act', 'sea', 'wed', 'ial', 'rly', 'ane', 'uth', 'uer', 'eau', 'nal', 'ors', 'ony', 'ows', 'tch', '_', 'ets', 'ick', 'put', 'oue', 'vs', 'eak', 'ues', 'lay', 'set', 'ber', 'get', 'gan', 'alf', 'nke', 'ept', 'got', 'oad', 'sly', 'nor', 'ced', 'lar', 'dow', 'ges', 'rey', 'des', 'lse', 'hes', 'uen', 'lso', 'ans', 'oom', 'ler', 'try', 'urs', 'wes', 'eal', 'ued', 'tis', 'bly', 'rge', 'rch', 'lue', 'nch', 'hin', 'eft', 'ale', 'hip', 'ock', 'ply', 'igh', 'ual', 'aps', 'hts', 'ins', 'st', 'alk', 'ier', 'm', 'lic', 'rke', 'pen', 'mad', 'eep', 'nde', 'ern', 'mon', 'ens', 'ape', 'hus', 'nse', 'als', 'ray', 'ees', 'top', 'zed', 'lls', 'eye', 'ran', 'rry', 'elt', 'ild', 'ude', 'rue', 'ker', 'dle', 've', 'bed', 'tal', 'pes', 'rne', '---', '...', 'irl', 'rew', 'tie', 'oat', 'eve', 'eel', 'epe', 'rty', 'few', 'oon', 'els', 'ank', 'ope', 'big', 'ods', 'urn', 're', 'ash', 'ask', 'eld', 'doe', 'ont', 'sky', 'rts', 'th', 'far', '`', 'ote', 'vp', 'gue', 'ule', 'unt', 'yed', 'uck', 'avy', 'ird', 'oks', 'hop', 'oud', 'ism', 'ext', 'gly', 'gle', 'cks', 'll', 'len', 'hot', 'hom', 'run', 'dr', 'die', 'ute', 'ren', 'cut', 'ety', 'fer', 'ney', 'vil', 'ool', 'ret', 'cts', 'ete', 'mer', 'awn', 'ket', 'ari', 'orm', 'ois', 'rer', 'car', 'ots', 'uel', 'eem', 'ncy', 'ems', 'ang', 'odd', 'ubt', 'ech', 'irs', 'arm', 'ize', 'eth', 'nne', 'bad', 'lit', 'ppy', 'ics', 'rms', 'ael', 'ilt', 'orn', 'tue', '*', 'ton', 'rit', 'bit', 'rce', 'ork', 'eer', 'haw', 'uge', 'ult', 'ats', 'emy', 'xed', 'lid', 'ail', 'ron', 'til', 'ldn', 'nty', 'sat', 'sun', 'ief', 'elp', 'ict', 'urt', 'fit', 'hly', 'ror', 'ric', 'rie', 'fly', 'ley', 'tty', 'kly', 'hew', 'boy', 'ads', 'pty', 'ury', 'lty', 'rre', 'eck', 'roy', 'tio', 'eds', 'sty', 'afe', 'iss', 'erg', 'lad', 'eme', 'tre', 'ils', 'eam', 'yle', 'nel', 'ska', 'rme', 'gon', 'law', 'cry', 'nks', 'bid', 'bow', 'lip', 'cle', 'asy', 'mly', 'lia', 'nay', 'l', 'gin', 'ths', 'ush', 'lus', 'lum', 'ha', 'idn', 'gun', 'nth', 'lye', 'fic', 'wee', 'efe', 'ert', 'oft', 'apt', 'uke', 'net', 'ign', 'oll', 'sch', 'gar', 'vel', '&', 'xit', 'mpt', 'gth', 'sit', 'lke', 'ero', 'ips', 'lds', 'aes', 'eek', 'oms', 'ser', 'ume', 'oul', 'mal', 'wly', 'rel', 'esh', 'tar', 'eue', 'ein', 'amp', 'won', 'ipe', 'war', 'sie', 'oot', 'ece', 'ede', 'idd', 'yre', 'uty', 'hut', 'iry', '[', ']', 'eas', 'rim', 'hty', 'key', 'que', 'nia', 'ago', 'jor', 'iew', 'ilk', 'wig', 'eny', 'pot', 'egs', 'mit', 'yer', 'raw', 'esn', 'ye', 'aze', 'eur', 'iet', 'udy', 'gry', 'eke', 'oin', 'dam', 'uds', 'nct', 'pet', 'six', 'rks', 'iar', 'inn', 'ode', 'ait', 'ste', 'oet', 'sen', 'une', 'rve', 'owd', 'sin', 'ala', 'sor', 'eps', 'rop', 'phy', 'tay', 'arf', 'bee', 'lat', 'iff', 'rap', 'alt', 'gge', 'cht', 'met', 'ohn', 'ift', 'upt', 'vse', 'lts', 'mrs', 'uce', 'ato', 'che', 'isn', 'iny', 'tep', 'eap', 'zen', 'ele', 'ald', 'nna', 'mid', 'dry', 'las', 'hur', 'ndy', 'kle', 'opy', 'gne', 'som', 'ops']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Add new, suffix-based features**","metadata":{}},{"cell_type":"code","source":"# create function that returns the counts of suffixes from suffix_list in the given sentence (text)\ndef suffix_counts (text, source_docs, suffix_list):\n    suffix_counts = {}\n    doc = source_docs.get (\" \".join (text))\n    suffixes = []\n    for word in doc:\n        suffixes.append (str (word.suffix_))\n    counts = Counter (suffixes)\n    for suffix in suffix_list:\n        if suffix in counts.keys ():\n            suffix_counts [suffix] = counts.get(suffix)\n        else: suffix_counts [suffix] = 0\n    return suffix_counts\n\ndef initialize_dataset (source, source_docs):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list = []\n        feature_list.append (avg_number_chars (sent))\n        feature_list.append (number_words (sent))\n        counts = word_counts (sent)\n        for word in STOP_WORDS:\n            if word in counts.keys ():\n                feature_list.append (counts.get (word))\n            else:\n                feature_list.append (0)\n        feature_list.append (proportion_words (sent, STOP_WORDS))\n        p_counts = pos_counts (sent, source_docs, pos_list)\n        for pos in p_counts.keys ():\n            feature_list.append (float (p_counts.get (pos))/float (len (sent)))\n        s_counts = suffix_counts (sent, source_docs, selected_suffixes)\n        for suffix in s_counts.keys ():\n            # append the new 690 suffix distribution features by calculating the proportion\n            # of words containing the suffixes\n            feature_list.append (float (s_counts.get (suffix))/float (len (sent)))\n            \n        all_features.append (feature_list)\n        if label == 'chesterton': targets.append (0)\n        else: targets.append (1)\n    return all_features, targets\n\n# apply the train-test-evaluate routine\nrun ()","metadata":{"execution":{"iopub.status.busy":"2023-12-29T18:42:20.232568Z","iopub.execute_input":"2023-12-29T18:42:20.23296Z","iopub.status.idle":"2023-12-29T18:42:39.020409Z","shell.execute_reply.started":"2023-12-29T18:42:20.232928Z","shell.execute_reply":"2023-12-29T18:42:39.019267Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"11083 11083\n2771 2771\n5649 5649\n\n0.9556116925297726\n[[1654   63]\n [  60  994]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      1717\n           1       0.94      0.94      0.94      1054\n\n    accuracy                           0.96      2771\n   macro avg       0.95      0.95      0.95      2771\nweighted avg       0.96      0.96      0.96      2771\n\n0.9410515135422198\n[[3536  206]\n [ 127 1780]]\n              precision    recall  f1-score   support\n\n           0       0.97      0.94      0.96      3742\n           1       0.90      0.93      0.91      1907\n\n    accuracy                           0.94      5649\n   macro avg       0.93      0.94      0.93      5649\nweighted avg       0.94      0.94      0.94      5649\n\n","output_type":"stream"}]}]}