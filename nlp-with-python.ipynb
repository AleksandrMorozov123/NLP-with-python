{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":799971,"sourceType":"datasetVersion","datasetId":150},{"sourceId":1043323,"sourceType":"datasetVersion","datasetId":576263}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/nlp-with-python?scriptVersionId=155293542\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T17:35:36.142675Z","iopub.execute_input":"2023-11-29T17:35:36.14355Z","iopub.status.idle":"2023-11-29T17:35:36.594788Z","shell.execute_reply.started":"2023-11-29T17:35:36.14351Z","shell.execute_reply":"2023-11-29T17:35:36.593671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to populate the documents dictionary**","metadata":{}},{"cell_type":"code","source":"def read_documents ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL\")\n    merged = \" \"\n    # the string variable merged keeps the result of merging the field identifier with its content\n    \n    for a_line in f.readlines ():\n        if a_line.startswith (\".\"):\n            merged += \"\\n\" + a_line.strip ()\n        else:\n            merged += \" \" + a_line.strip ()\n    # updates the merged variable using a for-loop\n    \n    documents = {}\n    \n    content = \"\"\n    doc_id = \"\"\n    # each entry in the dictioanry contains key = doc_id and value = content\n    \n    for a_line in merged.split (\"\\n\"):\n        if a_line.startswith (\".I\"):\n            doc_id = a_line.split (\" \") [1].strip()\n        elif a_line.startswith (\".X\"):\n            documents[doc_id] = content\n            content = \"\"\n            doc_id = \"\"\n        else:\n            content += a_line.strip ()[3:] + \" \"\n    f.close ()\n    return documents\n\n# print out the size of the dictionary and the content of the very first article\ndocuments = read_documents ()\nprint (len (documents))\nprint (documents.get (\"1\"))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:44:35.410615Z","iopub.execute_input":"2023-12-10T15:44:35.411542Z","iopub.status.idle":"2023-12-10T15:44:35.548907Z","shell.execute_reply.started":"2023-12-10T15:44:35.411505Z","shell.execute_reply":"2023-12-10T15:44:35.548185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to populate the queries dictionary**","metadata":{}},{"cell_type":"code","source":"def read_queries ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY\")\n    merged = \"\"\n    \n    # merge the conten of each field with its identifier and separate different fields with lune breaks\n    for a_line in f.readlines ():\n        if a_line.startswith (\".\"):\n            merged += \"\\n\" + a_line.strip ()\n        else:\n            merged += \" \" + a_line.strip ()\n    \n    queries = {}\n    \n    # initialize queries dictionary with key = qry_id and value=content for each query in the dataset\n    content = \"\"\n    qry_id = \"\"\n    \n    for a_line in merged.split (\"\\n\"):\n        if a_line.startswith (\".I\"):\n            if not content == \"\":\n                queries [qry_id] = content\n                content = \"\"\n                qry_id = \"\"\n            # add an enrty to the dictionary when you encounter an .I identifier\n            qry_id = a_line.split(\" \")[1].strip ()\n        # otherwise, keep adding content to the content variable\n        elif a_line.startswith (\".W\") or a_line.startswith (\".T\"):\n            content += a_line.strip ()[3:] + \" \"\n    queries [qry_id] = content\n    f.close ()\n    return queries\n\n# print out the length of the dictionary and the content of the first query\nqueries = read_queries ()\nprint (len (queries))\nprint (queries.get(\"1\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:44:38.045606Z","iopub.execute_input":"2023-12-10T15:44:38.045967Z","iopub.status.idle":"2023-12-10T15:44:38.067082Z","shell.execute_reply.started":"2023-12-10T15:44:38.045939Z","shell.execute_reply":"2023-12-10T15:44:38.065687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to populate the mappings dictionary**","metadata":{}},{"cell_type":"code","source":"def read_mappings ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL\")\n    mappings = {}\n    \n    for a_line in f.readlines ():\n        voc = a_line.strip ().split ()\n        key = voc[0].strip ()\n        current_value = voc[1].strip()\n        value = []\n        # update the entry in the mappings dictionary with the current value\n        if key in mappings.keys ():\n            value = mappings.get (key)\n        value.append (current_value)\n        mappings [key] = value\n    f.close ()\n    return mappings\n\n# print out some information about the mapping data structure\nmappings = read_mappings ()\nprint (len (mappings))\nprint (mappings.keys ())\nprint (mappings.get (\"1\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:44:40.049488Z","iopub.execute_input":"2023-12-10T15:44:40.049972Z","iopub.status.idle":"2023-12-10T15:44:40.086547Z","shell.execute_reply.started":"2023-12-10T15:44:40.049941Z","shell.execute_reply":"2023-12-10T15:44:40.084951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess the data in documents and queries**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport nltk\nfrom nltk import word_tokenize\n\n# text is converted to lowercase and split into words\ndef get_words (text):\n    word_list = [word for word in word_tokenize (text.lower ())]\n    return word_list\n    \ndoc_words = {}\nqry_words = {}\n\nfor doc_id in documents.keys ():\n    doc_words [doc_id] = get_words (documents.get (doc_id))\nfor qry_id in queries.keys ():\n    # entries in both documents and queries are represented as word lists\n    qry_words [qry_id] = get_words (queries.get (qry_id))\n    \n# print out the length of the dictionaries and check the first document and the fisrt query\nprint (len (doc_words))\nprint (doc_words.get (\"1\"))\nprint (len (doc_words.get (\"1\")))\nprint (len (qry_words))\nprint (qry_words.get (\"1\"))\nprint (len (qry_words.get(\"1\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:44:41.930107Z","iopub.execute_input":"2023-12-10T15:44:41.93049Z","iopub.status.idle":"2023-12-10T15:44:44.786555Z","shell.execute_reply.started":"2023-12-10T15:44:41.930458Z","shell.execute_reply":"2023-12-10T15:44:44.785376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Simple Biilean search algorithm**","metadata":{}},{"cell_type":"code","source":"# iterate through the documents\ndef retrieve_documents (doc_words, query):\n    docs = []\n    for doc_id in doc_words.keys ():\n        found = False\n        i = 0\n        while i<len(query) and not found: \n            word = query [i]\n            if word in doc_words.get (doc_id):\n                docs.append (doc_id)\n                found = True\n            else:\n                i+=1\n    return docs\n\n# check the results\ndocs = retrieve_documents (doc_words, qry_words.get(\"3\"))\nprint (docs [:100])\nprint (len (docs))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T17:20:23.423654Z","iopub.execute_input":"2023-12-09T17:20:23.424089Z","iopub.status.idle":"2023-12-09T17:20:23.445606Z","shell.execute_reply.started":"2023-12-09T17:20:23.424055Z","shell.execute_reply":"2023-12-09T17:20:23.444149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Begin the preprocessing - remove stopwords and punctuation marks**","metadata":{}},{"cell_type":"code","source":"# import python's string module that will help remove punctuation marks\nimport string\n\n# import the stopwords list\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef process (text):\n    stoplist = set (stopwords.words ('english'))\n    # only add tthe words if they are not included in the stoplist and are not puctuation marks\n    word_list = [word for word in word_tokenize (text.lower())\n                if not word in stoplist and not word in string.punctuation]\n    return word_list\n\n# check the results of these preprocessing steps on some documents or queries\nword_list = process (documents.get (\"1\"))\nprint (word_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:44:48.457786Z","iopub.execute_input":"2023-12-10T15:44:48.458179Z","iopub.status.idle":"2023-12-10T15:44:48.472545Z","shell.execute_reply.started":"2023-12-10T15:44:48.458149Z","shell.execute_reply":"2023-12-10T15:44:48.47113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next step in preprocessing - stemming**","metadata":{}},{"cell_type":"code","source":"# import the stemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\ndef process (text):\n    stoplist = set (stopwords.words ('english'))\n    # initialize the LancasterStemmer\n    st = LancasterStemmer ()\n    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n                if not word in stoplist and not word in string.punctuation]\n    return word_list\n\n# check the results on some document, query, or on a list of words\nword_list = process (documents.get(\"26\"))\nprint (word_list)\nword_list = process (\"organize, organizing, organizational, organ, organic, organizer\")\nprint (word_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:44:52.109572Z","iopub.execute_input":"2023-12-10T15:44:52.110617Z","iopub.status.idle":"2023-12-10T15:44:52.121971Z","shell.execute_reply.started":"2023-12-10T15:44:52.110578Z","shell.execute_reply":"2023-12-10T15:44:52.12075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Estimate term frequency in documents and queries**","metadata":{}},{"cell_type":"code","source":"def get_terms (text):\n    stoplist = set (stopwords.words ('english'))\n    terms = {}\n    st = LancasterStemmer ()\n    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n                if not word in stoplist and not word in string.punctuation]\n    for word in word_list:\n        terms [word] = terms.get (word, 0) + 1\n    return terms\n\ndoc_terms = {}\nqry_terms = {}\nfor doc_id in documents.keys ():\n    doc_terms [doc_id] = get_terms (documents.get (doc_id))\nfor qry_id in queries.keys ():\n    # populate the term frequency dictionaries for all documents and all queries\n    qry_terms [qry_id] = get_terms (queries.get (qry_id))\n    \n# check the results\nprint (len (doc_terms))\nprint (doc_terms.get (\"1\"))\nprint (len (doc_terms.get(\"1\")))\nprint (len (qry_terms))\nprint (qry_terms.get(\"1\"))\nprint (len (qry_terms.get(\"1\")))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:35:39.677777Z","iopub.execute_input":"2023-11-29T17:35:39.678725Z","iopub.status.idle":"2023-11-29T17:35:44.072663Z","shell.execute_reply.started":"2023-11-29T17:35:39.678678Z","shell.execute_reply":"2023-11-29T17:35:44.071346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to represent the datya in a shared space**","metadata":{}},{"cell_type":"code","source":"# collect the shared vocabulary of terms from documents and queries and return it as a sorted list\ndef collect_vocabulary ():\n    all_terms = []\n    for doc_id in doc_terms.keys ():\n        for term in doc_terms.get (doc_id).keys():\n            all_terms.append (term)\n    for qry_id in qry_terms.keys ():\n        for term in qry_terms.keys():\n            for term in qry_terms.get(qry_id).keys():\n                all_terms.append (term)\n    return sorted (set (all_terms))\n\n# print out the length of the shared vocabulary and check the first several terms in the vocabulary\nall_terms = collect_vocabulary ()\nprint (len (all_terms))\nprint (all_terms [:10])\n\ndef vectorize (input_features, vocabulary):\n    output = {}\n    for item_id in input_features.keys ():\n        features = input_features.get (item_id)\n        output_vector = []\n        for word in vocabulary:\n            if word in features.keys ():\n                output_vector.append (int (features.get (word)))\n            else:\n                output_vector.append (0)\n        output [item_id] = output_vector\n    return output\n\ndoc_vectors = vectorize (doc_terms, all_terms)\nqry_vectors = vectorize (qry_terms, all_terms)\n\n# print out some statistics on these data structures\nprint (len (doc_vectors))\nprint (len (doc_vectors.get (\"1450\")))\nprint (len (qry_vectors))\nprint (len (qry_vectors.get (\"110\")))","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:35:44.073994Z","iopub.execute_input":"2023-11-29T17:35:44.074334Z","iopub.status.idle":"2023-11-29T17:35:47.131899Z","shell.execute_reply.started":"2023-11-29T17:35:44.074299Z","shell.execute_reply":"2023-11-29T17:35:47.130626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate and apply inverse document frequency weighting**","metadata":{}},{"cell_type":"code","source":"# import library for math\nimport math\n\ndef calculate_idfs (vocabulary, doc_features):\n    doc_idfs = {}\n    for term in vocabulary:\n        doc_count = 0\n        for doc_id in doc_features.keys ():\n            terms = doc_features.get (doc_id)\n            if term in terms.keys ():\n                doc_count += 1\n        doc_idfs [term] = math.log (float (len (doc_features.keys ()))/\n                                    float (1 + doc_count), 10)\n    return doc_idfs\n\n# check the results - we should have idf values for all terms from the vocabulary\ndoc_idfs = calculate_idfs (all_terms, doc_terms)\nprint (len (doc_idfs))\nprint (doc_idfs.get (\"system\"))\n\n# define a function to apply idf weighing to the input_terms data structure\ndef vectorize_idf (input_terms, input_idfs, vocabulary):\n    output = {}\n    for item_id in input_terms.keys ():\n        terms = input_terms.get (item_id)\n        output_vector = []\n        for term in vocabulary:\n            if term in terms.keys ():\n                # multiply the term frequencies with idf weights if the term is present in document\n                output_vector.append (\n                input_idfs.get (term) * float (terms.get (term)))\n            else:\n                output_vector.append (float (0))\n        output [item_id] = output_vector\n    return output\n\n# apply idf weighing to doc_terms\ndoc_vectors = vectorize_idf (doc_terms, doc_idfs, all_terms)\n\n# print out some statistics, such as the number of documents and terms\nprint (len (doc_vectors))\nprint (len (doc_vectors.get (\"1460\")))","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:35:47.133196Z","iopub.execute_input":"2023-11-29T17:35:47.133512Z","iopub.status.idle":"2023-11-29T17:35:54.942497Z","shell.execute_reply.started":"2023-11-29T17:35:47.133486Z","shell.execute_reply":"2023-11-29T17:35:54.941332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run search algorithm for a given query on the set of the documents**","metadata":{}},{"cell_type":"code","source":"# the operator's itemgetter functionality helps sort Python dictionaries by keys or values\nfrom operator import itemgetter\n\n# calculate the length of the input vector\ndef length (vector):\n    sq_length = 0\n    for index in range (0, len(vector)):\n        sq_length += math.pow (vector [index], 2)\n    return math.sqrt (sq_length)\n\n# calculate the dot product of two vectors\ndef dot_product (vector1, vector2):\n    if len (vector1) == len (vector2):\n        dot_prod = 0\n        for index in range (0, len(vector1)):\n            if not vector1 [index] == 0 and not vector2 [index] == 0:\n                dot_prod += vector1 [index] * vector2 [index]\n        return dot_prod\n    else:\n        return \"Unmatching dimensionality\"\n    \ndef calculate_cosine (query, document):\n    cosine = dot_product (query, document) / (length (query) * length (document))\n    return cosine\n\nquery = qry_vectors.get (\"3\")\nresults = {}\n\nfor doc_id in doc_vectors.keys ():\n    document = doc_vectors.get (doc_id)\n    cosine = calculate_cosine (query, document)\n    results [doc_id] = cosine\n    \n# sort the results dictionary by cosine values in descending order and return the top n results\nfor items in sorted (results.items (), key = itemgetter (1), reverse = True) [:44]:\n    print (items [0])","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:35:54.943937Z","iopub.execute_input":"2023-11-29T17:35:54.944367Z","iopub.status.idle":"2023-11-29T17:36:01.320513Z","shell.execute_reply.started":"2023-11-29T17:35:54.944333Z","shell.execute_reply":"2023-11-29T17:36:01.319166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Estimate precision@k and ratio of cases with at least one relevant document**","metadata":{}},{"cell_type":"code","source":"# calculate the proportion of relevant documents from the gold standard in the top k returned results\ndef calculate_precision (model_output, gold_standard):\n    true_pos = 0\n    for item in model_output:\n        if item in gold_standard:\n            true_pos += 1\n    return float (true_pos) / float (len (model_output))\n\ndef calculate_found (model_output, gold_standard):\n    found = 0\n    for item in model_output:\n        if item in gold_standard:\n            found = 1\n    return float (found)\n\nprecision_all = 0.0\nfound_all = 0.0\nfor query_id in mappings.keys ():\n    # calculate mean values across all queries\n    gold_standard = mappings.get (str (query_id))\n    query = qry_vectors.get (str (query_id))\n    results = {}\n    model_output = []\n    for doc_id in doc_vectors.keys ():\n        document = doc_vectors.get (doc_id)\n        cosine = calculate_cosine (query, document)\n        # for each document, esimate its relevance to the query with cosine similarity as before\n        results [doc_id] = cosine\n    # sort the results and consider only top k (top 5) most relevant documents\n    for items in sorted (results.items (), key = itemgetter (1), reverse = True) [:5]:\n        model_output.append (items [0])\n    precision = calculate_precision (model_output, gold_standard)\n    found = calculate_found (model_output, gold_standard)\n    print (f\"{str (query_id)} : {str(precision)}\")\n    precision_all += precision\n    found_all += found\n    \n# estimate the mean values for all queries\nprint (precision_all / float (len (mappings.keys ())))\nprint (found_all / float (len (mappings.keys ())))    ","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:36:01.322144Z","iopub.execute_input":"2023-11-29T17:36:01.322659Z","iopub.status.idle":"2023-11-29T17:44:01.572157Z","shell.execute_reply.started":"2023-11-29T17:36:01.322619Z","shell.execute_reply":"2023-11-29T17:44:01.570997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On some queries the algorithm perform very well. For example, \"1 : 1.0\" shows that all top 5 documents returned for query 1 are relevant. However, on other queries the alforithm does not perform well.","metadata":{}},{"cell_type":"markdown","source":"**Estimate mean reciprocal rank**","metadata":{}},{"cell_type":"code","source":"rank_all = 0.0\nfor query_id in mappings.keys ():\n    gold_standard = mappings.get (str (query_id))\n    query = qry_vectors.get (str (query_id))\n    results = {}\n    for doc_id in doc_vectors.keys ():\n        document = doc_vectors.get (doc_id)\n        cosine = calculate_cosine (query, document)\n        results [doc_id] = cosine\n    sorted_results = sorted (results.items (),\n                            key=itemgetter (1), reverse = True)\n    index = 0\n    found = False\n    while found == False:\n        # set the flag found to False and switch it to True when we find the first relevant document\n        item = sorted_results [index]\n        # increment the index with each document in the results\n        index += 1\n        if index == len (sorted_results):\n            found = True\n        if item [0] in gold_standard:\n            # the document ID is the first element in the sorted tuples oof (document_id, similarity score)\n            found = True\n            print (f\"{str(query_id)}: {str(float (1) / float (index))}\")\n            rank_all += float(1) / float (index)\n            \n# print out the mean valur across all queries\nprint (rank_all / float (len (mappings.keys ())))","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:48:22.304475Z","iopub.execute_input":"2023-11-29T17:48:22.304863Z","iopub.status.idle":"2023-11-29T17:56:30.209754Z","shell.execute_reply.started":"2023-11-29T17:48:22.304832Z","shell.execute_reply":"2023-11-29T17:56:30.208582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Example how to run spaCy's processing pipeline**","metadata":{}},{"cell_type":"code","source":"# import library\nimport spacy\n\n# the spacy.load command initializes the nlp pipeline\nnlp = spacy.load (\"en_core_web_sm\")\ndoc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\nrows = []\n\n# print the output in a tabular format and add a header to the printout for clarity\nrows.append ([\"Word\", \"Position\", \"Lowercase\", \"Lemma\", \"POS\", \"Alphanumeric\", \"Stopword\"])\n\nfor token in doc:\n    rows.append ([token.text, str(token.i), token.lower_, token.lemma_,\n                 token.pos_, str(token.is_alpha), str (token.is_stop)])\n    \n# Python's zip function allows to reformat input from row representation\ncolumns = zip (*rows)\ncolumn_widths = [max (len (item) for item in col)\n                for col in columns]\n\n# calculate the maximum length of strings in each column to allow enough space in the printout\nfor row in rows:\n    print (''.join(' {:{width}} '.format (\n        row [i], width = column_widths [i])\n                  for i in range (0, len (row))))","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:57:03.639294Z","iopub.execute_input":"2023-12-02T18:57:03.640279Z","iopub.status.idle":"2023-12-02T18:57:10.339319Z","shell.execute_reply.started":"2023-12-02T18:57:03.640239Z","shell.execute_reply":"2023-12-02T18:57:10.338153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Identify all groups of nouns and the way they are realted to each other**","metadata":{}},{"cell_type":"code","source":"doc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\n\n# we can access noun phrases by doc.noun_chunks\nfor chunk in doc.noun_chunks:\n    # print out the phrase, its head, the type of relation to the next most important word, and the word itself\n    print ('\\t'.join ([chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text]))","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:57:14.233969Z","iopub.execute_input":"2023-12-02T18:57:14.234532Z","iopub.status.idle":"2023-12-02T18:57:14.254545Z","shell.execute_reply.started":"2023-12-02T18:57:14.234501Z","shell.execute_reply":"2023-12-02T18:57:14.253209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the dependency information**","metadata":{}},{"cell_type":"code","source":"# import spaCy's visualization tool displaCy\nfrom spacy import displacy\n# path helps define the location for the file to store the visualization\nfrom pathlib import Path\n\n# use displaCy to visualize dependecies over the input text with approptiate arguments\nsvg = displacy.render (doc, style = 'dep', jupyter = False)\nfile_name = '-'.join ([w.text for w in doc if not w.is_punct]) + \".svg\"\n\n# the the output us stored to simply uses the words from the sentence in its name\noutput_path = Path (file_name)\noutput_path.open (\"w\", encoding=\"utf-8\").write(svg)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:57:17.638027Z","iopub.execute_input":"2023-12-02T18:57:17.639166Z","iopub.status.idle":"2023-12-02T18:57:17.657642Z","shell.execute_reply.started":"2023-12-02T18:57:17.639126Z","shell.execute_reply":"2023-12-02T18:57:17.656442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Print out the information about head and dependents for each word**","metadata":{}},{"cell_type":"code","source":"# coode assumes that spaCy is imported and input text is already fed into the pipeline\nfor token in doc:\n    print (token.text, token.dep_, token.head.text,\n          token.head.pos_, [child for child in token.children])","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:57:20.919665Z","iopub.execute_input":"2023-12-02T18:57:20.920069Z","iopub.status.idle":"2023-12-02T18:57:20.92769Z","shell.execute_reply.started":"2023-12-02T18:57:20.920039Z","shell.execute_reply":"2023-12-02T18:57:20.926287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extarct participants of the actions**","metadata":{}},{"cell_type":"code","source":"# code assumes that spaCy is imported and input text is already fed into pipeline\nfor token in doc:\n    # check that the ROOT of the sentence is a verb with the base form (lemma) \"meet\"\n    if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\"\n       and token.dep_ == \"ROOT\"):\n        # this verb expresses the action itself\n        action = token.text\n        # extract the list of all dependents of this verb using token.children\n        children = [child for child in token.children]\n        participant1 = \"\"\n        participant2 = \"\"\n        for child1 in children:\n            if child1.dep_ == \"nsubj\":\n                participant1 = \" \".join (\n                [attr.text for attr in child1.children]\n                ) + \" \" + child1.text\n            elif child1.text == \"with\":\n                # check if the verb has preposition \"with\" as one of its dependents\n                action += \" \" + child1.text\n                child1_children = [child for child in child1.children]\n                for child2 in child1_children:\n                    if child2.pos_ == \"NOUN\":\n                        participant2 = \" \".join (\n                        [attr.text for attr in child2.children]\n                        ) + \" \" + child2.text\n                    \n# print out the results\nprint (f\"Participant1 = {participant1}\")\nprint (f\"Action = {action}\")\nprint (f\"Participant2 = {participant2}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:57:23.413514Z","iopub.execute_input":"2023-12-02T18:57:23.413963Z","iopub.status.idle":"2023-12-02T18:57:23.425759Z","shell.execute_reply.started":"2023-12-02T18:57:23.413905Z","shell.execute_reply":"2023-12-02T18:57:23.424458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build information extractor**","metadata":{}},{"cell_type":"code","source":"# provide diverse set of sentences\nsentences = [\"On monday students meet with researchers \" + \" and discuss future development their research.\", \n            \" Warren Baffet met with the President last week.\",\n            \"Elon Musk met with the President an White House.\",\n            \"The two bussinesmans also posed for photographs and \" + \n            \"the Vice President talked to reporters.\"]\n\n# define a function to apply all the steps in the information extraction algorithm\ndef extract_information (doc):\n    action = \"\"\n    participant1 = \"\"\n    for token in doc: \n         if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\" \n            and token.dep_ == \"ROOT\"):\n                action = token.text\n                children = [child for child in token.children]\n                for child1 in children:\n                    if child1.dep_ == \"nsubj\": \n                        patricipant1 = \" \".join (\n                [attr.text for attr in child1.children]\n                ) + \" \" + child1.text\n                    elif child1.text == \"with\":\n                        action += \" \" + child1.text\n                        child1_children = [child for child in child1.children]\n                        for child2 in child1_children:\n                            # extract participants expressed with proper nouns (PROPN) and common nouns (NOUN)\n                            if (child2.pos_ == \"NOUN\"\n                            or child2.pos_ == \"PROPN\"):\n                                participant2 = \" \".join (\n                        [attr.text for attr in child2.children]\n                        ) + \" \" + child2.text\n                    elif (child1.dep_ == \"dobj\"\n                        and (child1.pos_ == \"NOUN\"\n                            or child1.pos_ == \"PROPN\")):\n                        participant2 = \" \".join (\n                            [attr.text for attr in child1.children]\n                            ) + \" \" + child1.text\n    \n        \n# apply extract_information function to each sentence and print out the actions and participants\nfor sent in sentences:\n    print (f\"\\nSentence = {sent}\")\n    doc = nlp (sent)\n    extract_information (doc)\n    print (f\"Participant1 = {participant1}\")\n    print (f\"Action = {action}\")\n    print (f\"Participant2 = {participant2}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T18:58:55.461459Z","iopub.execute_input":"2023-12-02T18:58:55.461933Z","iopub.status.idle":"2023-12-02T18:58:55.51803Z","shell.execute_reply.started":"2023-12-02T18:58:55.461897Z","shell.execute_reply":"2023-12-02T18:58:55.516755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to extract literary works from Project Gutenberg**","metadata":{}},{"cell_type":"code","source":"nltk.download ('gutenberg')\nfrom nltk.corpus import gutenberg\n\n# print out the names of files\ngutenberg.fileids ()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:07.567022Z","iopub.execute_input":"2023-12-10T15:45:07.567432Z","iopub.status.idle":"2023-12-10T15:45:07.718208Z","shell.execute_reply.started":"2023-12-10T15:45:07.567401Z","shell.execute_reply":"2023-12-10T15:45:07.717209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define training and test sets**","metadata":{}},{"cell_type":"code","source":"nltk.download ('punkt')\n\nauthor1_train = gutenberg.sents ('chesterton-ball.txt') + gutenberg.sents ('chesterton-brown.txt')\nprint (author1_train)\nprint (len (author1_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:09.622897Z","iopub.execute_input":"2023-12-10T15:45:09.623558Z","iopub.status.idle":"2023-12-10T15:45:10.198378Z","shell.execute_reply.started":"2023-12-10T15:45:09.623518Z","shell.execute_reply":"2023-12-10T15:45:10.197368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the test set with the sentences from the third work by the author\nauthor1_test = gutenberg.sents ('chesterton-thursday.txt')\nprint (author1_test)\nprint (len (author1_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:11.304626Z","iopub.execute_input":"2023-12-10T15:45:11.305037Z","iopub.status.idle":"2023-12-10T15:45:11.511305Z","shell.execute_reply.started":"2023-12-10T15:45:11.305003Z","shell.execute_reply":"2023-12-10T15:45:11.510305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author2_train = gutenberg.sents ('shakespeare-caesar.txt') + gutenberg.sents ('shakespeare-hamlet.txt')\nprint (author2_train)\nprint (len (author2_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:13.606399Z","iopub.execute_input":"2023-12-10T15:45:13.606799Z","iopub.status.idle":"2023-12-10T15:45:13.856664Z","shell.execute_reply.started":"2023-12-10T15:45:13.606766Z","shell.execute_reply":"2023-12-10T15:45:13.855551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author2_test = gutenberg.sents ('shakespeare-macbeth.txt')\nprint (author2_test)\nprint (len (author2_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:15.128855Z","iopub.execute_input":"2023-12-10T15:45:15.129868Z","iopub.status.idle":"2023-12-10T15:45:15.219255Z","shell.execute_reply.started":"2023-12-10T15:45:15.12982Z","shell.execute_reply":"2023-12-10T15:45:15.218116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate simple statistics on texts**","metadata":{}},{"cell_type":"code","source":"def statistics (gutenberg_data):\n    for work in gutenberg_data:\n        # use NLTK's functionality to calculate statistics\n        num_chars = len (gutenberg.raw (work))\n        num_words = len (gutenberg.words (work))\n        num_sents = len (gutenberg.sents (work))\n        num_vocab = len (set (w.lower ()\n                             for w in gutenberg.words (work)))\n        print (round (num_chars / num_words),\n              round (num_words / num_sents),\n              round (num_words / num_vocab),\n              work)\n        \ngutenberg_data = ['chesterton-ball.txt','chesterton-brown.txt','chesterton-thursday.txt', \n                  'shakespeare-caesar.txt','shakespeare-hamlet.txt','shakespeare-macbeth.txt']\nstatistics (gutenberg_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:16.475188Z","iopub.execute_input":"2023-12-10T15:45:16.475903Z","iopub.status.idle":"2023-12-10T15:45:18.140109Z","shell.execute_reply.started":"2023-12-10T15:45:16.475857Z","shell.execute_reply":"2023-12-10T15:45:18.138941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run StratifiedShufflingSplit on the data**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport random\nimport sklearn\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nall_sents = [(sent, 'chesterton') for sent in author1_train]\nall_sents += [(sent, 'shakespeare') for sent in author2_train]\n# combine all sentences into a single list called all_sents, keeping the author label\nprint (f\"Dataset size = {str (len (all_sents))} sentences\")\n\n# keep the set of labels (authors) as values\nvalues = [author for (sent, author) in all_sents]\nsplit = StratifiedShuffleSplit (n_splits = 1, test_size = 0.2, random_state = 42)\nstrat_train_set = []\nstrat_pretest_set = []\nfor train_index, pretest_index in split.split (all_sents, values):\n    strat_train_set= [all_sents [index] for index in train_index]\n    strat_pretest_set = [all_sents [index]\n                        for index in pretest_index]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:20.31702Z","iopub.execute_input":"2023-12-10T15:45:20.317428Z","iopub.status.idle":"2023-12-10T15:45:21.166947Z","shell.execute_reply.started":"2023-12-10T15:45:20.317396Z","shell.execute_reply":"2023-12-10T15:45:21.165717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check the proportions of the data in the two classes**","metadata":{}},{"cell_type":"code","source":"# calculate the proportion of the entries in each class (category) in the given dataset data\ndef cat_proportions (data, cat):\n    count = 0\n    for item in data:\n        if item [1] == cat:\n            count += 1\n    return float (count) / float (len (data))\n\ncategories = ['chesterton', 'shakespeare']\nrows = []\nrows.append ([\"Category\", \"Overall\", \"Stratified train\", \"Stratified pretest\"])\n\nfor cat in categories:\n    rows.append ([cat, f\"{cat_proportions (all_sents, cat):.6f}\",\n                 f\"{cat_proportions (strat_train_set, cat):.6f}\",\n                 f\"{cat_proportions (strat_pretest_set, cat):.6f}\"])\n    \ncolumns = zip (*rows)\ncolumn_widths = [max (len (item) for item in col) for col in columns]\nfor row in rows:\n    print (''.join (' {:{width}} '.format (row [i], width = column_widths [i])\n                   for i in range (0, len (row))))","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:45:35.944839Z","iopub.execute_input":"2023-12-10T15:45:35.945256Z","iopub.status.idle":"2023-12-10T15:45:35.963076Z","shell.execute_reply.started":"2023-12-10T15:45:35.945223Z","shell.execute_reply":"2023-12-10T15:45:35.961809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create the test_set data structure**","metadata":{}},{"cell_type":"code","source":"test_set = [(sent, \"chesterton\") for sent in author1_test]\ntest_set += [(sent, \"shakespeare\") for sent in author2_test]\n\n# extract words as features\ndef get_features (text):\n    features = {}\n    word_list = [word for word in text]\n    for word in word_list:\n        features [word] = True\n    return features\n\n# extract features from training and pretest sets\ntrain_features = [(get_features (sents), label)\n                 for (sents, label) in strat_train_set]\npretest_features = [(get_features (sents), label)\n                   for (sents, label) in strat_pretest_set]\n\n# run some checks to see what the data contains\nprint (len (train_features))\nprint (train_features [0] [0])\nprint (train_features [100] [0])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T15:54:53.957134Z","iopub.execute_input":"2023-12-10T15:54:53.957536Z","iopub.status.idle":"2023-12-10T15:54:54.439496Z","shell.execute_reply.started":"2023-12-10T15:54:53.957499Z","shell.execute_reply":"2023-12-10T15:54:54.438343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Naive Bayes classifier on train and test on pretest set**","metadata":{}},{"cell_type":"code","source":"# import the classifier\nfrom nltk import NaiveBayesClassifier, classify\n\n# train the classifier on the training set\nprint (f\"Training set size = {str (len (train_features))} sentences\")\nprint (f\"Pretest set size = {str (len (pretest_features))} sentences\")\nclassifier = NaiveBayesClassifier.train (train_features)\n\nprint (f\"Accuracy on the training set = {str (classify.accuracy (classifier, train_features))}\")\nprint (f\"Accuracy on the pretest set = \" + \n      f\"{str (classify.accuracy (classifier, pretest_features))}\")\nclassifier.show_most_informative_features (50)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T16:03:22.865593Z","iopub.execute_input":"2023-12-10T16:03:22.865978Z","iopub.status.idle":"2023-12-10T16:03:24.814013Z","shell.execute_reply.started":"2023-12-10T16:03:22.865949Z","shell.execute_reply":"2023-12-10T16:03:24.812771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to extract words and sentence length statistics**","metadata":{}},{"cell_type":"code","source":"def avg_number_chars (text):\n    total_chars = 0.0\n    for word in text:\n        total_chars += len (word)\n    return float (total_chars) / float (len(text))\n\n# calculate the sentence length in terms of the number of words\ndef number_words (text):\n    return float (len (text))\n\nprint (avg_number_chars ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))\nprint (number_words ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))","metadata":{"execution":{"iopub.status.busy":"2023-12-02T19:15:06.005898Z","iopub.execute_input":"2023-12-02T19:15:06.006296Z","iopub.status.idle":"2023-12-02T19:15:06.015108Z","shell.execute_reply.started":"2023-12-02T19:15:06.006266Z","shell.execute_reply":"2023-12-02T19:15:06.013753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to extract features and map them to the labels**","metadata":{}},{"cell_type":"code","source":"# argument source denotes the dataset we are applying the feature extraction\ndef initialize_dataset (source):\n    all_features = []\n    targets = []\n    # iterate through all (sent, label) pairs in the given dataset\n    for (sent, label) in source:\n        feature_list = []\n        feature_list.append (avg_number_chars (sent))\n        feature_list.append (number_words (sent))\n        all_features.append (feature_list)\n        if label == \"chesterton\": targets.append (0)\n            else: targets.append (1)\n    return all_features, targets\n\ntrain_data, train_targets = initialize_dataset (strat_train_set)\npretest_data, pretest_targets = initialize_dataset (strat_pretest_set)\ntest_data, test_targets = initialize_dataset (test_set)\n\n# print out thr length of the structures\nprint (len (train_data), len (train_targets))\nprint (len (pretest_data), len (pretest_targets))\nprint (len (test_data), len (test_targets))","metadata":{},"execution_count":null,"outputs":[]}]}