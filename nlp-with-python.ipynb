{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/nlp-with-python?scriptVersionId=156092980\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"b4a6bb75","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-22T17:41:29.244929Z","iopub.status.busy":"2023-12-22T17:41:29.243578Z","iopub.status.idle":"2023-12-22T17:41:30.223448Z","shell.execute_reply":"2023-12-22T17:41:30.221934Z"},"papermill":{"duration":0.998169,"end_time":"2023-12-22T17:41:30.226184","exception":false,"start_time":"2023-12-22T17:41:29.228015","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL\n","/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL\n","/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY\n","/kaggle/input/religious-and-philosophical-texts/35895-0.txt\n","/kaggle/input/religious-and-philosophical-texts/pg2800.txt\n","/kaggle/input/religious-and-philosophical-texts/pg2680.txt\n","/kaggle/input/religious-and-philosophical-texts/pg10.txt\n","/kaggle/input/religious-and-philosophical-texts/pg17.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"bd576186","metadata":{"papermill":{"duration":0.012305,"end_time":"2023-12-22T17:41:30.251579","exception":false,"start_time":"2023-12-22T17:41:30.239274","status":"completed"},"tags":[]},"source":["**Code to populate the documents dictionary**"]},{"cell_type":"code","execution_count":2,"id":"253dff54","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:30.279107Z","iopub.status.busy":"2023-12-22T17:41:30.278638Z","iopub.status.idle":"2023-12-22T17:41:30.376231Z","shell.execute_reply":"2023-12-22T17:41:30.374963Z"},"papermill":{"duration":0.114897,"end_time":"2023-12-22T17:41:30.379225","exception":false,"start_time":"2023-12-22T17:41:30.264328","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1460\n"," 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"]}],"source":["def read_documents ():\n","    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL\")\n","    merged = \" \"\n","    # the string variable merged keeps the result of merging the field identifier with its content\n","    \n","    for a_line in f.readlines ():\n","        if a_line.startswith (\".\"):\n","            merged += \"\\n\" + a_line.strip ()\n","        else:\n","            merged += \" \" + a_line.strip ()\n","    # updates the merged variable using a for-loop\n","    \n","    documents = {}\n","    \n","    content = \"\"\n","    doc_id = \"\"\n","    # each entry in the dictioanry contains key = doc_id and value = content\n","    \n","    for a_line in merged.split (\"\\n\"):\n","        if a_line.startswith (\".I\"):\n","            doc_id = a_line.split (\" \") [1].strip()\n","        elif a_line.startswith (\".X\"):\n","            documents[doc_id] = content\n","            content = \"\"\n","            doc_id = \"\"\n","        else:\n","            content += a_line.strip ()[3:] + \" \"\n","    f.close ()\n","    return documents\n","\n","# print out the size of the dictionary and the content of the very first article\n","documents = read_documents ()\n","print (len (documents))\n","print (documents.get (\"1\"))\n","    "]},{"cell_type":"markdown","id":"c45d754a","metadata":{"papermill":{"duration":0.012292,"end_time":"2023-12-22T17:41:30.404655","exception":false,"start_time":"2023-12-22T17:41:30.392363","status":"completed"},"tags":[]},"source":["**Code to populate the queries dictionary**"]},{"cell_type":"code","execution_count":3,"id":"bdf3ceaa","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:30.432887Z","iopub.status.busy":"2023-12-22T17:41:30.431933Z","iopub.status.idle":"2023-12-22T17:41:30.448847Z","shell.execute_reply":"2023-12-22T17:41:30.447102Z"},"papermill":{"duration":0.034411,"end_time":"2023-12-22T17:41:30.452047","exception":false,"start_time":"2023-12-22T17:41:30.417636","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["112\n","What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"]}],"source":["def read_queries ():\n","    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY\")\n","    merged = \"\"\n","    \n","    # merge the conten of each field with its identifier and separate different fields with lune breaks\n","    for a_line in f.readlines ():\n","        if a_line.startswith (\".\"):\n","            merged += \"\\n\" + a_line.strip ()\n","        else:\n","            merged += \" \" + a_line.strip ()\n","    \n","    queries = {}\n","    \n","    # initialize queries dictionary with key = qry_id and value=content for each query in the dataset\n","    content = \"\"\n","    qry_id = \"\"\n","    \n","    for a_line in merged.split (\"\\n\"):\n","        if a_line.startswith (\".I\"):\n","            if not content == \"\":\n","                queries [qry_id] = content\n","                content = \"\"\n","                qry_id = \"\"\n","            # add an enrty to the dictionary when you encounter an .I identifier\n","            qry_id = a_line.split(\" \")[1].strip ()\n","        # otherwise, keep adding content to the content variable\n","        elif a_line.startswith (\".W\") or a_line.startswith (\".T\"):\n","            content += a_line.strip ()[3:] + \" \"\n","    queries [qry_id] = content\n","    f.close ()\n","    return queries\n","\n","# print out the length of the dictionary and the content of the first query\n","queries = read_queries ()\n","print (len (queries))\n","print (queries.get(\"1\"))"]},{"cell_type":"markdown","id":"0628df63","metadata":{"papermill":{"duration":0.013235,"end_time":"2023-12-22T17:41:30.479165","exception":false,"start_time":"2023-12-22T17:41:30.46593","status":"completed"},"tags":[]},"source":["**Code to populate the mappings dictionary**"]},{"cell_type":"code","execution_count":4,"id":"8c13d0f7","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:30.509027Z","iopub.status.busy":"2023-12-22T17:41:30.508641Z","iopub.status.idle":"2023-12-22T17:41:30.527549Z","shell.execute_reply":"2023-12-22T17:41:30.525324Z"},"papermill":{"duration":0.03723,"end_time":"2023-12-22T17:41:30.530284","exception":false,"start_time":"2023-12-22T17:41:30.493054","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["76\n","dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n","['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"]}],"source":["def read_mappings ():\n","    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL\")\n","    mappings = {}\n","    \n","    for a_line in f.readlines ():\n","        voc = a_line.strip ().split ()\n","        key = voc[0].strip ()\n","        current_value = voc[1].strip()\n","        value = []\n","        # update the entry in the mappings dictionary with the current value\n","        if key in mappings.keys ():\n","            value = mappings.get (key)\n","        value.append (current_value)\n","        mappings [key] = value\n","    f.close ()\n","    return mappings\n","\n","# print out some information about the mapping data structure\n","mappings = read_mappings ()\n","print (len (mappings))\n","print (mappings.keys ())\n","print (mappings.get (\"1\"))"]},{"cell_type":"markdown","id":"0c3a9501","metadata":{"papermill":{"duration":0.01314,"end_time":"2023-12-22T17:41:30.557364","exception":false,"start_time":"2023-12-22T17:41:30.544224","status":"completed"},"tags":[]},"source":["**Preprocess the data in documents and queries**"]},{"cell_type":"code","execution_count":5,"id":"8ed669cc","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:30.58615Z","iopub.status.busy":"2023-12-22T17:41:30.585741Z","iopub.status.idle":"2023-12-22T17:41:34.16021Z","shell.execute_reply":"2023-12-22T17:41:34.159204Z"},"papermill":{"duration":3.591699,"end_time":"2023-12-22T17:41:34.162537","exception":false,"start_time":"2023-12-22T17:41:30.570838","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1460\n","['18', 'editions', 'of', 'the', 'dewey', 'decimal', 'classifications', 'comaromi', ',', 'j.p.', 'the', 'present', 'study', 'is', 'a', 'history', 'of', 'the', 'dewey', 'decimal', 'classification', '.', 'the', 'first', 'edition', 'of', 'the', 'ddc', 'was', 'published', 'in', '1876', ',', 'the', 'eighteenth', 'edition', 'in', '1971', ',', 'and', 'future', 'editions', 'will', 'continue', 'to', 'appear', 'as', 'needed', '.', 'in', 'spite', 'of', 'the', 'ddc', \"'s\", 'long', 'and', 'healthy', 'life', ',', 'however', ',', 'its', 'full', 'story', 'has', 'never', 'been', 'told', '.', 'there', 'have', 'been', 'biographies', 'of', 'dewey', 'that', 'briefly', 'describe', 'his', 'system', ',', 'but', 'this', 'is', 'the', 'first', 'attempt', 'to', 'provide', 'a', 'detailed', 'history', 'of', 'the', 'work', 'that', 'more', 'than', 'any', 'other', 'has', 'spurred', 'the', 'growth', 'of', 'librarianship', 'in', 'this', 'country', 'and', 'abroad', '.']\n","113\n","112\n","['what', 'problems', 'and', 'concerns', 'are', 'there', 'in', 'making', 'up', 'descriptive', 'titles', '?', 'what', 'difficulties', 'are', 'involved', 'in', 'automatically', 'retrieving', 'articles', 'from', 'approximate', 'titles', '?', 'what', 'is', 'the', 'usual', 'relevance', 'of', 'the', 'content', 'of', 'articles', 'to', 'their', 'titles', '?']\n","38\n"]}],"source":["# import required libraries\n","import nltk\n","from nltk import word_tokenize\n","\n","# text is converted to lowercase and split into words\n","def get_words (text):\n","    word_list = [word for word in word_tokenize (text.lower ())]\n","    return word_list\n","    \n","doc_words = {}\n","qry_words = {}\n","\n","for doc_id in documents.keys ():\n","    doc_words [doc_id] = get_words (documents.get (doc_id))\n","for qry_id in queries.keys ():\n","    # entries in both documents and queries are represented as word lists\n","    qry_words [qry_id] = get_words (queries.get (qry_id))\n","    \n","# print out the length of the dictionaries and check the first document and the fisrt query\n","print (len (doc_words))\n","print (doc_words.get (\"1\"))\n","print (len (doc_words.get (\"1\")))\n","print (len (qry_words))\n","print (qry_words.get (\"1\"))\n","print (len (qry_words.get(\"1\")))"]},{"cell_type":"markdown","id":"ac5e4fa8","metadata":{"papermill":{"duration":0.01233,"end_time":"2023-12-22T17:41:34.188028","exception":false,"start_time":"2023-12-22T17:41:34.175698","status":"completed"},"tags":[]},"source":["**Simple Biilean search algorithm**"]},{"cell_type":"code","execution_count":6,"id":"4c0cee01","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:34.2151Z","iopub.status.busy":"2023-12-22T17:41:34.214737Z","iopub.status.idle":"2023-12-22T17:41:34.23127Z","shell.execute_reply":"2023-12-22T17:41:34.229749Z"},"papermill":{"duration":0.032613,"end_time":"2023-12-22T17:41:34.233501","exception":false,"start_time":"2023-12-22T17:41:34.200888","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102']\n","1397\n"]}],"source":["# iterate through the documents\n","def retrieve_documents (doc_words, query):\n","    docs = []\n","    for doc_id in doc_words.keys ():\n","        found = False\n","        i = 0\n","        while i<len(query) and not found: \n","            word = query [i]\n","            if word in doc_words.get (doc_id):\n","                docs.append (doc_id)\n","                found = True\n","            else:\n","                i+=1\n","    return docs\n","\n","# check the results\n","docs = retrieve_documents (doc_words, qry_words.get(\"3\"))\n","print (docs [:100])\n","print (len (docs))"]},{"cell_type":"markdown","id":"2fa66f02","metadata":{"papermill":{"duration":0.013479,"end_time":"2023-12-22T17:41:34.353011","exception":false,"start_time":"2023-12-22T17:41:34.339532","status":"completed"},"tags":[]},"source":["**Begin the preprocessing - remove stopwords and punctuation marks**"]},{"cell_type":"code","execution_count":7,"id":"0baf1a3e","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:34.382909Z","iopub.status.busy":"2023-12-22T17:41:34.38242Z","iopub.status.idle":"2023-12-22T17:41:34.394767Z","shell.execute_reply":"2023-12-22T17:41:34.393891Z"},"papermill":{"duration":0.029981,"end_time":"2023-12-22T17:41:34.397013","exception":false,"start_time":"2023-12-22T17:41:34.367032","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['18', 'editions', 'dewey', 'decimal', 'classifications', 'comaromi', 'j.p.', 'present', 'study', 'history', 'dewey', 'decimal', 'classification', 'first', 'edition', 'ddc', 'published', '1876', 'eighteenth', 'edition', '1971', 'future', 'editions', 'continue', 'appear', 'needed', 'spite', 'ddc', \"'s\", 'long', 'healthy', 'life', 'however', 'full', 'story', 'never', 'told', 'biographies', 'dewey', 'briefly', 'describe', 'system', 'first', 'attempt', 'provide', 'detailed', 'history', 'work', 'spurred', 'growth', 'librarianship', 'country', 'abroad']\n"]}],"source":["# import python's string module that will help remove punctuation marks\n","import string\n","\n","# import the stopwords list\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","\n","def process (text):\n","    stoplist = set (stopwords.words ('english'))\n","    # only add tthe words if they are not included in the stoplist and are not puctuation marks\n","    word_list = [word for word in word_tokenize (text.lower())\n","                if not word in stoplist and not word in string.punctuation]\n","    return word_list\n","\n","# check the results of these preprocessing steps on some documents or queries\n","word_list = process (documents.get (\"1\"))\n","print (word_list)"]},{"cell_type":"markdown","id":"11286940","metadata":{"papermill":{"duration":0.014094,"end_time":"2023-12-22T17:41:34.425341","exception":false,"start_time":"2023-12-22T17:41:34.411247","status":"completed"},"tags":[]},"source":["**Next step in preprocessing - stemming**"]},{"cell_type":"code","execution_count":8,"id":"30056aa3","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:34.459156Z","iopub.status.busy":"2023-12-22T17:41:34.457483Z","iopub.status.idle":"2023-12-22T17:41:34.471891Z","shell.execute_reply":"2023-12-22T17:41:34.470355Z"},"papermill":{"duration":0.034016,"end_time":"2023-12-22T17:41:34.473757","exception":false,"start_time":"2023-12-22T17:41:34.439741","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['index', 'abstract', 'assocy', 'doyl', 'l.b', 'artic', 'discuss', 'poss', 'exploit', 'stat', 'word', 'co-occurrence', 'text', 'purpos', 'docu', 'retriev', 'co-occurrence', 'defin', 'rel', 'ment', 'process', 'auth', 'read', 'sev', 'mean', 'quantit', 'meas', 'word', 'co-occurrence', 'scrutinized', 'shown', 'strongly', 'co-occurring', 'word', 'pair', 'theref', '``', 'assocy', \"''\", 'stat', 'sens', 'repres', 'form', '``', 'assocy', 'map', \"''\", 'last', 'half', 'artic', 'pres', 'two', 'mod', 'us', 'assocy', 'map', 'lit', 'search']\n","['org', 'org', 'org', 'org', 'org', 'org']\n"]}],"source":["# import the stemmer\n","from nltk.stem.lancaster import LancasterStemmer\n","\n","def process (text):\n","    stoplist = set (stopwords.words ('english'))\n","    # initialize the LancasterStemmer\n","    st = LancasterStemmer ()\n","    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n","                if not word in stoplist and not word in string.punctuation]\n","    return word_list\n","\n","# check the results on some document, query, or on a list of words\n","word_list = process (documents.get(\"26\"))\n","print (word_list)\n","word_list = process (\"organize, organizing, organizational, organ, organic, organizer\")\n","print (word_list)"]},{"cell_type":"markdown","id":"cd5ee04e","metadata":{"papermill":{"duration":0.013198,"end_time":"2023-12-22T17:41:34.500969","exception":false,"start_time":"2023-12-22T17:41:34.487771","status":"completed"},"tags":[]},"source":["**Estimate term frequency in documents and queries**"]},{"cell_type":"code","execution_count":9,"id":"0c2d27c6","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:34.530265Z","iopub.status.busy":"2023-12-22T17:41:34.529317Z","iopub.status.idle":"2023-12-22T17:41:38.321937Z","shell.execute_reply":"2023-12-22T17:41:38.319752Z"},"papermill":{"duration":3.81146,"end_time":"2023-12-22T17:41:38.325706","exception":false,"start_time":"2023-12-22T17:41:34.514246","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1460\n","{'18': 1, 'edit': 4, 'dewey': 3, 'decim': 2, 'class': 2, 'comarom': 1, 'j.p.': 1, 'pres': 1, 'study': 1, 'hist': 2, 'first': 2, 'ddc': 2, 'publ': 1, '1876': 1, 'eighteen': 1, '1971': 1, 'fut': 1, 'continu': 1, 'appear': 1, 'nee': 1, 'spit': 1, \"'s\": 1, 'long': 1, 'healthy': 1, 'lif': 1, 'howev': 1, 'ful': 1, 'story': 1, 'nev': 1, 'told': 1, 'biograph': 1, 'brief': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'grow': 1, 'libr': 1, 'country': 1, 'abroad': 1}\n","43\n","112\n","{'problem': 1, 'concern': 1, 'mak': 1, 'describ': 1, 'titl': 3, 'difficul': 1, 'involv': 1, 'autom': 1, 'retriev': 1, 'artic': 2, 'approxim': 1, 'us': 1, 'relev': 1, 'cont': 1}\n","14\n"]}],"source":["def get_terms (text):\n","    stoplist = set (stopwords.words ('english'))\n","    terms = {}\n","    st = LancasterStemmer ()\n","    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n","                if not word in stoplist and not word in string.punctuation]\n","    for word in word_list:\n","        terms [word] = terms.get (word, 0) + 1\n","    return terms\n","\n","doc_terms = {}\n","qry_terms = {}\n","for doc_id in documents.keys ():\n","    doc_terms [doc_id] = get_terms (documents.get (doc_id))\n","for qry_id in queries.keys ():\n","    # populate the term frequency dictionaries for all documents and all queries\n","    qry_terms [qry_id] = get_terms (queries.get (qry_id))\n","    \n","# check the results\n","print (len (doc_terms))\n","print (doc_terms.get (\"1\"))\n","print (len (doc_terms.get(\"1\")))\n","print (len (qry_terms))\n","print (qry_terms.get(\"1\"))\n","print (len (qry_terms.get(\"1\")))\n"]},{"cell_type":"markdown","id":"e2a2036a","metadata":{"papermill":{"duration":0.013677,"end_time":"2023-12-22T17:41:38.353712","exception":false,"start_time":"2023-12-22T17:41:38.340035","status":"completed"},"tags":[]},"source":["**Code to represent the datya in a shared space**"]},{"cell_type":"code","execution_count":10,"id":"7531a0b5","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:38.384484Z","iopub.status.busy":"2023-12-22T17:41:38.383997Z","iopub.status.idle":"2023-12-22T17:41:40.959277Z","shell.execute_reply":"2023-12-22T17:41:40.958374Z"},"papermill":{"duration":2.593768,"end_time":"2023-12-22T17:41:40.961605","exception":false,"start_time":"2023-12-22T17:41:38.367837","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["8881\n","[\"''\", \"'60\", \"'70\", \"'a\", \"'anyhow\", \"'apparent\", \"'b\", \"'basic\", \"'better\", \"'bibliograph\"]\n","1460\n","8881\n","112\n","8881\n"]}],"source":["# collect the shared vocabulary of terms from documents and queries and return it as a sorted list\n","def collect_vocabulary ():\n","    all_terms = []\n","    for doc_id in doc_terms.keys ():\n","        for term in doc_terms.get (doc_id).keys():\n","            all_terms.append (term)\n","    for qry_id in qry_terms.keys ():\n","        for term in qry_terms.keys():\n","            for term in qry_terms.get(qry_id).keys():\n","                all_terms.append (term)\n","    return sorted (set (all_terms))\n","\n","# print out the length of the shared vocabulary and check the first several terms in the vocabulary\n","all_terms = collect_vocabulary ()\n","print (len (all_terms))\n","print (all_terms [:10])\n","\n","def vectorize (input_features, vocabulary):\n","    output = {}\n","    for item_id in input_features.keys ():\n","        features = input_features.get (item_id)\n","        output_vector = []\n","        for word in vocabulary:\n","            if word in features.keys ():\n","                output_vector.append (int (features.get (word)))\n","            else:\n","                output_vector.append (0)\n","        output [item_id] = output_vector\n","    return output\n","\n","doc_vectors = vectorize (doc_terms, all_terms)\n","qry_vectors = vectorize (qry_terms, all_terms)\n","\n","# print out some statistics on these data structures\n","print (len (doc_vectors))\n","print (len (doc_vectors.get (\"1450\")))\n","print (len (qry_vectors))\n","print (len (qry_vectors.get (\"110\")))"]},{"cell_type":"markdown","id":"0a9abc07","metadata":{"papermill":{"duration":0.014496,"end_time":"2023-12-22T17:41:40.990909","exception":false,"start_time":"2023-12-22T17:41:40.976413","status":"completed"},"tags":[]},"source":["**Calculate and apply inverse document frequency weighting**"]},{"cell_type":"code","execution_count":11,"id":"75fdec69","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:41.023106Z","iopub.status.busy":"2023-12-22T17:41:41.022657Z","iopub.status.idle":"2023-12-22T17:41:47.843091Z","shell.execute_reply":"2023-12-22T17:41:47.842223Z"},"papermill":{"duration":6.838675,"end_time":"2023-12-22T17:41:47.845038","exception":false,"start_time":"2023-12-22T17:41:41.006363","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["8881\n","0.43844122348938885\n","1460\n","8881\n"]}],"source":["# import library for math\n","import math\n","\n","def calculate_idfs (vocabulary, doc_features):\n","    doc_idfs = {}\n","    for term in vocabulary:\n","        doc_count = 0\n","        for doc_id in doc_features.keys ():\n","            terms = doc_features.get (doc_id)\n","            if term in terms.keys ():\n","                doc_count += 1\n","        doc_idfs [term] = math.log (float (len (doc_features.keys ()))/\n","                                    float (1 + doc_count), 10)\n","    return doc_idfs\n","\n","# check the results - we should have idf values for all terms from the vocabulary\n","doc_idfs = calculate_idfs (all_terms, doc_terms)\n","print (len (doc_idfs))\n","print (doc_idfs.get (\"system\"))\n","\n","# define a function to apply idf weighing to the input_terms data structure\n","def vectorize_idf (input_terms, input_idfs, vocabulary):\n","    output = {}\n","    for item_id in input_terms.keys ():\n","        terms = input_terms.get (item_id)\n","        output_vector = []\n","        for term in vocabulary:\n","            if term in terms.keys ():\n","                # multiply the term frequencies with idf weights if the term is present in document\n","                output_vector.append (\n","                input_idfs.get (term) * float (terms.get (term)))\n","            else:\n","                output_vector.append (float (0))\n","        output [item_id] = output_vector\n","    return output\n","\n","# apply idf weighing to doc_terms\n","doc_vectors = vectorize_idf (doc_terms, doc_idfs, all_terms)\n","\n","# print out some statistics, such as the number of documents and terms\n","print (len (doc_vectors))\n","print (len (doc_vectors.get (\"1460\")))"]},{"cell_type":"markdown","id":"7e5a6569","metadata":{"papermill":{"duration":0.01298,"end_time":"2023-12-22T17:41:47.871626","exception":false,"start_time":"2023-12-22T17:41:47.858646","status":"completed"},"tags":[]},"source":["**Run search algorithm for a given query on the set of the documents**"]},{"cell_type":"code","execution_count":12,"id":"88e53741","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:47.900361Z","iopub.status.busy":"2023-12-22T17:41:47.899677Z","iopub.status.idle":"2023-12-22T17:41:53.443283Z","shell.execute_reply":"2023-12-22T17:41:53.440879Z"},"papermill":{"duration":5.561844,"end_time":"2023-12-22T17:41:53.446763","exception":false,"start_time":"2023-12-22T17:41:47.884919","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["469\n","1179\n","1142\n","1181\n","1190\n","445\n","1116\n","85\n","540\n","599\n","372\n","60\n","1030\n","241\n","1161\n","965\n","1191\n","899\n","137\n","535\n","456\n","803\n","95\n","1077\n","560\n","1095\n","166\n","544\n","1133\n","1080\n","640\n","163\n","837\n","686\n","1082\n","1297\n","839\n","1111\n","1428\n","1330\n","110\n","440\n","132\n","1137\n"]}],"source":["# the operator's itemgetter functionality helps sort Python dictionaries by keys or values\n","from operator import itemgetter\n","\n","# calculate the length of the input vector\n","def length (vector):\n","    sq_length = 0\n","    for index in range (0, len(vector)):\n","        sq_length += math.pow (vector [index], 2)\n","    return math.sqrt (sq_length)\n","\n","# calculate the dot product of two vectors\n","def dot_product (vector1, vector2):\n","    if len (vector1) == len (vector2):\n","        dot_prod = 0\n","        for index in range (0, len(vector1)):\n","            if not vector1 [index] == 0 and not vector2 [index] == 0:\n","                dot_prod += vector1 [index] * vector2 [index]\n","        return dot_prod\n","    else:\n","        return \"Unmatching dimensionality\"\n","    \n","def calculate_cosine (query, document):\n","    cosine = dot_product (query, document) / (length (query) * length (document))\n","    return cosine\n","\n","query = qry_vectors.get (\"3\")\n","results = {}\n","\n","for doc_id in doc_vectors.keys ():\n","    document = doc_vectors.get (doc_id)\n","    cosine = calculate_cosine (query, document)\n","    results [doc_id] = cosine\n","    \n","# sort the results dictionary by cosine values in descending order and return the top n results\n","for items in sorted (results.items (), key = itemgetter (1), reverse = True) [:44]:\n","    print (items [0])"]},{"cell_type":"markdown","id":"85150b21","metadata":{"papermill":{"duration":0.015296,"end_time":"2023-12-22T17:41:53.477387","exception":false,"start_time":"2023-12-22T17:41:53.462091","status":"completed"},"tags":[]},"source":["**Estimate precision@k and ratio of cases with at least one relevant document**"]},{"cell_type":"code","execution_count":13,"id":"01bac429","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:41:53.509854Z","iopub.status.busy":"2023-12-22T17:41:53.508098Z","iopub.status.idle":"2023-12-22T17:48:48.182077Z","shell.execute_reply":"2023-12-22T17:48:48.179913Z"},"papermill":{"duration":414.692703,"end_time":"2023-12-22T17:48:48.18448","exception":false,"start_time":"2023-12-22T17:41:53.491777","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1 : 1.0\n","2 : 0.2\n","3 : 0.8\n","4 : 0.0\n","5 : 0.0\n","6 : 0.0\n","7 : 0.0\n","8 : 0.0\n","9 : 0.6\n","10 : 0.6\n","11 : 0.4\n","12 : 0.0\n","13 : 0.6\n","14 : 0.0\n","15 : 0.2\n","16 : 0.0\n","17 : 0.2\n","18 : 0.2\n","19 : 0.2\n","20 : 0.4\n","21 : 0.0\n","22 : 0.0\n","23 : 0.4\n","24 : 1.0\n","25 : 0.0\n","26 : 0.8\n","27 : 0.6\n","28 : 0.6\n","29 : 0.6\n","30 : 0.8\n","31 : 0.6\n","32 : 0.2\n","33 : 0.0\n","34 : 0.4\n","35 : 0.2\n","37 : 0.4\n","39 : 0.2\n","41 : 0.6\n","42 : 0.4\n","43 : 0.2\n","44 : 0.6\n","45 : 0.4\n","46 : 0.6\n","49 : 0.2\n","50 : 0.8\n","52 : 0.8\n","54 : 0.2\n","55 : 1.0\n","56 : 0.6\n","57 : 0.0\n","58 : 0.8\n","61 : 0.2\n","62 : 0.8\n","65 : 0.6\n","66 : 1.0\n","67 : 0.2\n","69 : 0.4\n","71 : 0.4\n","76 : 1.0\n","79 : 0.2\n","81 : 0.2\n","82 : 0.2\n","84 : 0.0\n","90 : 0.0\n","92 : 0.6\n","95 : 0.6\n","96 : 0.0\n","97 : 0.4\n","98 : 0.6\n","99 : 0.6\n","100 : 0.0\n","101 : 0.0\n","102 : 0.8\n","104 : 0.2\n","109 : 0.6\n","111 : 0.6\n","0.38947368421052636\n","0.7631578947368421\n"]}],"source":["# calculate the proportion of relevant documents from the gold standard in the top k returned results\n","def calculate_precision (model_output, gold_standard):\n","    true_pos = 0\n","    for item in model_output:\n","        if item in gold_standard:\n","            true_pos += 1\n","    return float (true_pos) / float (len (model_output))\n","\n","def calculate_found (model_output, gold_standard):\n","    found = 0\n","    for item in model_output:\n","        if item in gold_standard:\n","            found = 1\n","    return float (found)\n","\n","precision_all = 0.0\n","found_all = 0.0\n","for query_id in mappings.keys ():\n","    # calculate mean values across all queries\n","    gold_standard = mappings.get (str (query_id))\n","    query = qry_vectors.get (str (query_id))\n","    results = {}\n","    model_output = []\n","    for doc_id in doc_vectors.keys ():\n","        document = doc_vectors.get (doc_id)\n","        cosine = calculate_cosine (query, document)\n","        # for each document, esimate its relevance to the query with cosine similarity as before\n","        results [doc_id] = cosine\n","    # sort the results and consider only top k (top 5) most relevant documents\n","    for items in sorted (results.items (), key = itemgetter (1), reverse = True) [:5]:\n","        model_output.append (items [0])\n","    precision = calculate_precision (model_output, gold_standard)\n","    found = calculate_found (model_output, gold_standard)\n","    print (f\"{str (query_id)} : {str(precision)}\")\n","    precision_all += precision\n","    found_all += found\n","    \n","# estimate the mean values for all queries\n","print (precision_all / float (len (mappings.keys ())))\n","print (found_all / float (len (mappings.keys ())))    "]},{"cell_type":"markdown","id":"1a04daf5","metadata":{"papermill":{"duration":0.017404,"end_time":"2023-12-22T17:48:48.220345","exception":false,"start_time":"2023-12-22T17:48:48.202941","status":"completed"},"tags":[]},"source":["On some queries the algorithm perform very well. For example, \"1 : 1.0\" shows that all top 5 documents returned for query 1 are relevant. However, on other queries the alforithm does not perform well."]},{"cell_type":"markdown","id":"bf2c29ae","metadata":{"papermill":{"duration":0.016969,"end_time":"2023-12-22T17:48:48.2551","exception":false,"start_time":"2023-12-22T17:48:48.238131","status":"completed"},"tags":[]},"source":["**Estimate mean reciprocal rank**"]},{"cell_type":"code","execution_count":14,"id":"2be619ca","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:48:48.292976Z","iopub.status.busy":"2023-12-22T17:48:48.292534Z","iopub.status.idle":"2023-12-22T17:55:42.088588Z","shell.execute_reply":"2023-12-22T17:55:42.086984Z"},"papermill":{"duration":413.819101,"end_time":"2023-12-22T17:55:42.091688","exception":false,"start_time":"2023-12-22T17:48:48.272587","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1: 1.0\n","2: 0.3333333333333333\n","3: 1.0\n","4: 0.08333333333333333\n","5: 0.125\n","6: 0.04\n","7: 0.05555555555555555\n","8: 0.03571428571428571\n","9: 0.5\n","10: 1.0\n","11: 1.0\n","12: 0.14285714285714285\n","13: 1.0\n","14: 0.011764705882352941\n","15: 0.2\n","16: 0.02857142857142857\n","17: 0.25\n","18: 0.25\n","19: 0.25\n","20: 0.5\n","21: 0.0625\n","22: 0.09090909090909091\n","23: 0.3333333333333333\n","24: 1.0\n","25: 0.14285714285714285\n","26: 1.0\n","27: 1.0\n","28: 1.0\n","29: 1.0\n","30: 1.0\n","31: 0.5\n","32: 0.3333333333333333\n","33: 0.07142857142857142\n","34: 1.0\n","35: 0.5\n","37: 1.0\n","39: 1.0\n","41: 1.0\n","42: 1.0\n","43: 0.2\n","44: 0.5\n","45: 1.0\n","46: 0.5\n","49: 0.3333333333333333\n","50: 1.0\n","52: 1.0\n","54: 0.3333333333333333\n","55: 1.0\n","56: 1.0\n","57: 0.1111111111111111\n","58: 1.0\n","61: 0.5\n","62: 1.0\n","65: 1.0\n","66: 1.0\n","67: 0.2\n","69: 1.0\n","71: 0.25\n","76: 1.0\n","79: 0.3333333333333333\n","81: 1.0\n","82: 0.5\n","84: 0.08333333333333333\n","90: 0.14285714285714285\n","92: 1.0\n","95: 0.5\n","96: 0.058823529411764705\n","97: 1.0\n","98: 1.0\n","99: 0.5\n","100: 0.1111111111111111\n","101: 0.027777777777777776\n","102: 1.0\n","104: 0.25\n","109: 0.5\n","111: 1.0\n","0.575993490298831\n"]}],"source":["rank_all = 0.0\n","for query_id in mappings.keys ():\n","    gold_standard = mappings.get (str (query_id))\n","    query = qry_vectors.get (str (query_id))\n","    results = {}\n","    for doc_id in doc_vectors.keys ():\n","        document = doc_vectors.get (doc_id)\n","        cosine = calculate_cosine (query, document)\n","        results [doc_id] = cosine\n","    sorted_results = sorted (results.items (),\n","                            key=itemgetter (1), reverse = True)\n","    index = 0\n","    found = False\n","    while found == False:\n","        # set the flag found to False and switch it to True when we find the first relevant document\n","        item = sorted_results [index]\n","        # increment the index with each document in the results\n","        index += 1\n","        if index == len (sorted_results):\n","            found = True\n","        if item [0] in gold_standard:\n","            # the document ID is the first element in the sorted tuples oof (document_id, similarity score)\n","            found = True\n","            print (f\"{str(query_id)}: {str(float (1) / float (index))}\")\n","            rank_all += float(1) / float (index)\n","            \n","# print out the mean valur across all queries\n","print (rank_all / float (len (mappings.keys ())))"]},{"cell_type":"markdown","id":"109540dc","metadata":{"papermill":{"duration":0.02292,"end_time":"2023-12-22T17:55:42.141258","exception":false,"start_time":"2023-12-22T17:55:42.118338","status":"completed"},"tags":[]},"source":["**Example how to run spaCy's processing pipeline**"]},{"cell_type":"code","execution_count":15,"id":"91beb259","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:42.189645Z","iopub.status.busy":"2023-12-22T17:55:42.189135Z","iopub.status.idle":"2023-12-22T17:55:48.841922Z","shell.execute_reply":"2023-12-22T17:55:48.840724Z"},"papermill":{"duration":6.680217,"end_time":"2023-12-22T17:55:48.844094","exception":false,"start_time":"2023-12-22T17:55:42.163877","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" Word         Position  Lowercase    Lemma        POS    Alphanumeric  Stopword \n"," On           0         on           on           ADP    True          True     \n"," monday       1         monday       monday       PROPN  True          False    \n"," students     2         students     student      NOUN   True          False    \n"," meet         3         meet         meet         VERB   True          False    \n"," with         4         with         with         ADP    True          True     \n"," researchers  5         researchers  researcher   NOUN   True          False    \n","              6                                   SPACE  False         False    \n"," and          7         and          and          CCONJ  True          True     \n"," discuss      8         discuss      discuss      VERB   True          False    \n"," future       9         future       future       ADJ    True          False    \n"," development  10        development  development  NOUN   True          False    \n"," their        11        their        their        PRON   True          True     \n"," research     12        research     research     NOUN   True          False    \n"," .            13        .            .            PUNCT  False         False    \n"]}],"source":["# import library\n","import spacy\n","\n","# the spacy.load command initializes the nlp pipeline\n","nlp = spacy.load (\"en_core_web_sm\")\n","doc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\n","rows = []\n","\n","# print the output in a tabular format and add a header to the printout for clarity\n","rows.append ([\"Word\", \"Position\", \"Lowercase\", \"Lemma\", \"POS\", \"Alphanumeric\", \"Stopword\"])\n","\n","for token in doc:\n","    rows.append ([token.text, str(token.i), token.lower_, token.lemma_,\n","                 token.pos_, str(token.is_alpha), str (token.is_stop)])\n","    \n","# Python's zip function allows to reformat input from row representation\n","columns = zip (*rows)\n","column_widths = [max (len (item) for item in col)\n","                for col in columns]\n","\n","# calculate the maximum length of strings in each column to allow enough space in the printout\n","for row in rows:\n","    print (''.join(' {:{width}} '.format (\n","        row [i], width = column_widths [i])\n","                  for i in range (0, len (row))))"]},{"cell_type":"markdown","id":"72398989","metadata":{"papermill":{"duration":0.024373,"end_time":"2023-12-22T17:55:48.897344","exception":false,"start_time":"2023-12-22T17:55:48.872971","status":"completed"},"tags":[]},"source":["**Identify all groups of nouns and the way they are realted to each other**"]},{"cell_type":"code","execution_count":16,"id":"0c82dcf9","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:48.947958Z","iopub.status.busy":"2023-12-22T17:55:48.946899Z","iopub.status.idle":"2023-12-22T17:55:48.960915Z","shell.execute_reply":"2023-12-22T17:55:48.959961Z"},"papermill":{"duration":0.041908,"end_time":"2023-12-22T17:55:48.963006","exception":false,"start_time":"2023-12-22T17:55:48.921098","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["monday students\tstudents\tnsubj\tmeet\n","researchers\tresearchers\tpobj\twith\n","future development\tdevelopment\tdobj\tdiscuss\n","their research\tresearch\tdobj\tdiscuss\n"]}],"source":["doc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\n","\n","# we can access noun phrases by doc.noun_chunks\n","for chunk in doc.noun_chunks:\n","    # print out the phrase, its head, the type of relation to the next most important word, and the word itself\n","    print ('\\t'.join ([chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text]))"]},{"cell_type":"markdown","id":"65de9a4a","metadata":{"papermill":{"duration":0.027157,"end_time":"2023-12-22T17:55:49.014022","exception":false,"start_time":"2023-12-22T17:55:48.986865","status":"completed"},"tags":[]},"source":["**Visualize the dependency information**"]},{"cell_type":"code","execution_count":17,"id":"5d0f2ba4","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:49.062543Z","iopub.status.busy":"2023-12-22T17:55:49.062128Z","iopub.status.idle":"2023-12-22T17:55:49.075814Z","shell.execute_reply":"2023-12-22T17:55:49.074767Z"},"papermill":{"duration":0.040212,"end_time":"2023-12-22T17:55:49.077772","exception":false,"start_time":"2023-12-22T17:55:49.03756","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["10561"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# import spaCy's visualization tool displaCy\n","from spacy import displacy\n","# path helps define the location for the file to store the visualization\n","from pathlib import Path\n","\n","# use displaCy to visualize dependecies over the input text with approptiate arguments\n","svg = displacy.render (doc, style = 'dep', jupyter = False)\n","file_name = '-'.join ([w.text for w in doc if not w.is_punct]) + \".svg\"\n","\n","# the the output us stored to simply uses the words from the sentence in its name\n","output_path = Path (file_name)\n","output_path.open (\"w\", encoding=\"utf-8\").write(svg)"]},{"cell_type":"markdown","id":"6930dbc9","metadata":{"papermill":{"duration":0.022382,"end_time":"2023-12-22T17:55:49.123065","exception":false,"start_time":"2023-12-22T17:55:49.100683","status":"completed"},"tags":[]},"source":["**Print out the information about head and dependents for each word**"]},{"cell_type":"code","execution_count":18,"id":"7403d977","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:49.172386Z","iopub.status.busy":"2023-12-22T17:55:49.17164Z","iopub.status.idle":"2023-12-22T17:55:49.178548Z","shell.execute_reply":"2023-12-22T17:55:49.177132Z"},"papermill":{"duration":0.033257,"end_time":"2023-12-22T17:55:49.18041","exception":false,"start_time":"2023-12-22T17:55:49.147153","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["On prep meet VERB []\n","monday compound students NOUN []\n","students nsubj meet VERB [monday]\n","meet ROOT meet VERB [On, students, with, and, discuss, .]\n","with prep meet VERB [researchers]\n","researchers pobj with ADP [ ]\n","  dep researchers NOUN []\n","and cc meet VERB []\n","discuss conj meet VERB [development, research]\n","future amod development NOUN []\n","development dobj discuss VERB [future]\n","their poss research NOUN []\n","research dobj discuss VERB [their]\n",". punct meet VERB []\n"]}],"source":["# coode assumes that spaCy is imported and input text is already fed into the pipeline\n","for token in doc:\n","    print (token.text, token.dep_, token.head.text,\n","          token.head.pos_, [child for child in token.children])"]},{"cell_type":"markdown","id":"697f7020","metadata":{"papermill":{"duration":0.022981,"end_time":"2023-12-22T17:55:49.226541","exception":false,"start_time":"2023-12-22T17:55:49.20356","status":"completed"},"tags":[]},"source":["**Extarct participants of the actions**"]},{"cell_type":"code","execution_count":19,"id":"426ade11","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:49.277077Z","iopub.status.busy":"2023-12-22T17:55:49.276679Z","iopub.status.idle":"2023-12-22T17:55:49.285475Z","shell.execute_reply":"2023-12-22T17:55:49.284152Z"},"papermill":{"duration":0.035604,"end_time":"2023-12-22T17:55:49.287617","exception":false,"start_time":"2023-12-22T17:55:49.252013","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Participant1 = monday students\n","Action = meet with\n","Participant2 =   researchers\n"]}],"source":["# code assumes that spaCy is imported and input text is already fed into pipeline\n","for token in doc:\n","    # check that the ROOT of the sentence is a verb with the base form (lemma) \"meet\"\n","    if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\"\n","       and token.dep_ == \"ROOT\"):\n","        # this verb expresses the action itself\n","        action = token.text\n","        # extract the list of all dependents of this verb using token.children\n","        children = [child for child in token.children]\n","        participant1 = \"\"\n","        participant2 = \"\"\n","        for child1 in children:\n","            if child1.dep_ == \"nsubj\":\n","                participant1 = \" \".join (\n","                [attr.text for attr in child1.children]\n","                ) + \" \" + child1.text\n","            elif child1.text == \"with\":\n","                # check if the verb has preposition \"with\" as one of its dependents\n","                action += \" \" + child1.text\n","                child1_children = [child for child in child1.children]\n","                for child2 in child1_children:\n","                    if child2.pos_ == \"NOUN\":\n","                        participant2 = \" \".join (\n","                        [attr.text for attr in child2.children]\n","                        ) + \" \" + child2.text\n","                    \n","# print out the results\n","print (f\"Participant1 = {participant1}\")\n","print (f\"Action = {action}\")\n","print (f\"Participant2 = {participant2}\")"]},{"cell_type":"markdown","id":"cce99e5a","metadata":{"papermill":{"duration":0.023397,"end_time":"2023-12-22T17:55:49.336232","exception":false,"start_time":"2023-12-22T17:55:49.312835","status":"completed"},"tags":[]},"source":["**Build information extractor**"]},{"cell_type":"code","execution_count":20,"id":"0bdc3c76","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:49.386627Z","iopub.status.busy":"2023-12-22T17:55:49.386108Z","iopub.status.idle":"2023-12-22T17:55:49.431249Z","shell.execute_reply":"2023-12-22T17:55:49.430065Z"},"papermill":{"duration":0.075186,"end_time":"2023-12-22T17:55:49.433598","exception":false,"start_time":"2023-12-22T17:55:49.358412","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Sentence = On monday students meet with researchers  and discuss future development their research.\n","Participant1 = monday students\n","Action = meet with\n","Participant2 =   researchers\n","\n","Sentence =  Warren Baffet met with the President last week.\n","Participant1 = monday students\n","Action = meet with\n","Participant2 =   researchers\n","\n","Sentence = Elon Musk met with the President an White House.\n","Participant1 = monday students\n","Action = meet with\n","Participant2 =   researchers\n","\n","Sentence = The two bussinesmans also posed for photographs and the Vice President talked to reporters.\n","Participant1 = monday students\n","Action = meet with\n","Participant2 =   researchers\n"]}],"source":["# provide diverse set of sentences\n","sentences = [\"On monday students meet with researchers \" + \" and discuss future development their research.\", \n","            \" Warren Baffet met with the President last week.\",\n","            \"Elon Musk met with the President an White House.\",\n","            \"The two bussinesmans also posed for photographs and \" + \n","            \"the Vice President talked to reporters.\"]\n","\n","# define a function to apply all the steps in the information extraction algorithm\n","def extract_information (doc):\n","    action = \"\"\n","    participant1 = \"\"\n","    for token in doc: \n","         if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\" \n","            and token.dep_ == \"ROOT\"):\n","                action = token.text\n","                children = [child for child in token.children]\n","                for child1 in children:\n","                    if child1.dep_ == \"nsubj\": \n","                        patricipant1 = \" \".join (\n","                [attr.text for attr in child1.children]\n","                ) + \" \" + child1.text\n","                    elif child1.text == \"with\":\n","                        action += \" \" + child1.text\n","                        child1_children = [child for child in child1.children]\n","                        for child2 in child1_children:\n","                            # extract participants expressed with proper nouns (PROPN) and common nouns (NOUN)\n","                            if (child2.pos_ == \"NOUN\"\n","                            or child2.pos_ == \"PROPN\"):\n","                                participant2 = \" \".join (\n","                        [attr.text for attr in child2.children]\n","                        ) + \" \" + child2.text\n","                    elif (child1.dep_ == \"dobj\"\n","                        and (child1.pos_ == \"NOUN\"\n","                            or child1.pos_ == \"PROPN\")):\n","                        participant2 = \" \".join (\n","                            [attr.text for attr in child1.children]\n","                            ) + \" \" + child1.text\n","    \n","        \n","# apply extract_information function to each sentence and print out the actions and participants\n","for sent in sentences:\n","    print (f\"\\nSentence = {sent}\")\n","    doc = nlp (sent)\n","    extract_information (doc)\n","    print (f\"Participant1 = {participant1}\")\n","    print (f\"Action = {action}\")\n","    print (f\"Participant2 = {participant2}\")"]},{"cell_type":"markdown","id":"ae697284","metadata":{"papermill":{"duration":0.02296,"end_time":"2023-12-22T17:55:49.481141","exception":false,"start_time":"2023-12-22T17:55:49.458181","status":"completed"},"tags":[]},"source":["**Code to extract literary works from Project Gutenberg**"]},{"cell_type":"code","execution_count":21,"id":"e280ef74","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:49.530095Z","iopub.status.busy":"2023-12-22T17:55:49.529585Z","iopub.status.idle":"2023-12-22T17:55:49.646914Z","shell.execute_reply":"2023-12-22T17:55:49.645793Z"},"papermill":{"duration":0.144296,"end_time":"2023-12-22T17:55:49.648973","exception":false,"start_time":"2023-12-22T17:55:49.504677","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /usr/share/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]},{"data":{"text/plain":["['austen-emma.txt',\n"," 'austen-persuasion.txt',\n"," 'austen-sense.txt',\n"," 'bible-kjv.txt',\n"," 'blake-poems.txt',\n"," 'bryant-stories.txt',\n"," 'burgess-busterbrown.txt',\n"," 'carroll-alice.txt',\n"," 'chesterton-ball.txt',\n"," 'chesterton-brown.txt',\n"," 'chesterton-thursday.txt',\n"," 'edgeworth-parents.txt',\n"," 'melville-moby_dick.txt',\n"," 'milton-paradise.txt',\n"," 'shakespeare-caesar.txt',\n"," 'shakespeare-hamlet.txt',\n"," 'shakespeare-macbeth.txt',\n"," 'whitman-leaves.txt']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download ('gutenberg')\n","from nltk.corpus import gutenberg\n","\n","# print out the names of files\n","gutenberg.fileids ()"]},{"cell_type":"markdown","id":"58d4caed","metadata":{"papermill":{"duration":0.023431,"end_time":"2023-12-22T17:55:49.696024","exception":false,"start_time":"2023-12-22T17:55:49.672593","status":"completed"},"tags":[]},"source":["**Define training and test sets**"]},{"cell_type":"code","execution_count":22,"id":"77d6aaa2","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:49.746497Z","iopub.status.busy":"2023-12-22T17:55:49.746019Z","iopub.status.idle":"2023-12-22T17:55:50.241373Z","shell.execute_reply":"2023-12-22T17:55:50.23951Z"},"papermill":{"duration":0.523668,"end_time":"2023-12-22T17:55:50.243982","exception":false,"start_time":"2023-12-22T17:55:49.720314","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[['[', 'The', 'Ball', 'and', 'The', 'Cross', 'by', 'G', '.', 'K', '.', 'Chesterton', '1909', ']'], ['I', '.'], ...]\n","8585\n"]}],"source":["nltk.download ('punkt')\n","\n","author1_train = gutenberg.sents ('chesterton-ball.txt') + gutenberg.sents ('chesterton-brown.txt')\n","print (author1_train)\n","print (len (author1_train))"]},{"cell_type":"code","execution_count":23,"id":"4dd20a9e","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:50.296117Z","iopub.status.busy":"2023-12-22T17:55:50.295664Z","iopub.status.idle":"2023-12-22T17:55:50.461108Z","shell.execute_reply":"2023-12-22T17:55:50.459013Z"},"papermill":{"duration":0.19436,"end_time":"2023-12-22T17:55:50.463528","exception":false,"start_time":"2023-12-22T17:55:50.269168","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[['[', 'The', 'Man', 'Who', 'Was', 'Thursday', 'by', 'G', '.', 'K', '.', 'Chesterton', '1908', ']'], ['To', 'Edmund', 'Clerihew', 'Bentley'], ...]\n","3742\n"]}],"source":["# initialize the test set with the sentences from the third work by the author\n","author1_test = gutenberg.sents ('chesterton-thursday.txt')\n","print (author1_test)\n","print (len (author1_test))"]},{"cell_type":"code","execution_count":24,"id":"6fcb0bc4","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:50.514598Z","iopub.status.busy":"2023-12-22T17:55:50.512624Z","iopub.status.idle":"2023-12-22T17:55:50.701448Z","shell.execute_reply":"2023-12-22T17:55:50.700337Z"},"papermill":{"duration":0.216264,"end_time":"2023-12-22T17:55:50.703594","exception":false,"start_time":"2023-12-22T17:55:50.48733","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']'], ['Actus', 'Primus', '.'], ...]\n","5269\n"]}],"source":["author2_train = gutenberg.sents ('shakespeare-caesar.txt') + gutenberg.sents ('shakespeare-hamlet.txt')\n","print (author2_train)\n","print (len (author2_train))"]},{"cell_type":"code","execution_count":25,"id":"6d01334c","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:50.754833Z","iopub.status.busy":"2023-12-22T17:55:50.754401Z","iopub.status.idle":"2023-12-22T17:55:50.825399Z","shell.execute_reply":"2023-12-22T17:55:50.824269Z"},"papermill":{"duration":0.100062,"end_time":"2023-12-22T17:55:50.828402","exception":false,"start_time":"2023-12-22T17:55:50.72834","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]\n","1907\n"]}],"source":["author2_test = gutenberg.sents ('shakespeare-macbeth.txt')\n","print (author2_test)\n","print (len (author2_test))"]},{"cell_type":"markdown","id":"d031bc0d","metadata":{"papermill":{"duration":0.024461,"end_time":"2023-12-22T17:55:50.8785","exception":false,"start_time":"2023-12-22T17:55:50.854039","status":"completed"},"tags":[]},"source":["**Calculate simple statistics on texts**"]},{"cell_type":"code","execution_count":26,"id":"01f3bc93","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:50.929886Z","iopub.status.busy":"2023-12-22T17:55:50.929407Z","iopub.status.idle":"2023-12-22T17:55:52.255773Z","shell.execute_reply":"2023-12-22T17:55:52.254521Z"},"papermill":{"duration":1.354943,"end_time":"2023-12-22T17:55:52.257928","exception":false,"start_time":"2023-12-22T17:55:50.902985","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["5 20 12 chesterton-ball.txt\n","5 23 11 chesterton-brown.txt\n","5 18 11 chesterton-thursday.txt\n","4 12 9 shakespeare-caesar.txt\n","4 12 8 shakespeare-hamlet.txt\n","4 12 7 shakespeare-macbeth.txt\n"]}],"source":["def statistics (gutenberg_data):\n","    for work in gutenberg_data:\n","        # use NLTK's functionality to calculate statistics\n","        num_chars = len (gutenberg.raw (work))\n","        num_words = len (gutenberg.words (work))\n","        num_sents = len (gutenberg.sents (work))\n","        num_vocab = len (set (w.lower ()\n","                             for w in gutenberg.words (work)))\n","        print (round (num_chars / num_words),\n","              round (num_words / num_sents),\n","              round (num_words / num_vocab),\n","              work)\n","        \n","gutenberg_data = ['chesterton-ball.txt','chesterton-brown.txt','chesterton-thursday.txt', \n","                  'shakespeare-caesar.txt','shakespeare-hamlet.txt','shakespeare-macbeth.txt']\n","statistics (gutenberg_data)"]},{"cell_type":"markdown","id":"da2d1499","metadata":{"papermill":{"duration":0.02442,"end_time":"2023-12-22T17:55:52.308184","exception":false,"start_time":"2023-12-22T17:55:52.283764","status":"completed"},"tags":[]},"source":["**Run StratifiedShufflingSplit on the data**"]},{"cell_type":"code","execution_count":27,"id":"62de8cab","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:52.360921Z","iopub.status.busy":"2023-12-22T17:55:52.360401Z","iopub.status.idle":"2023-12-22T17:55:52.978221Z","shell.execute_reply":"2023-12-22T17:55:52.975971Z"},"papermill":{"duration":0.648408,"end_time":"2023-12-22T17:55:52.981702","exception":false,"start_time":"2023-12-22T17:55:52.333294","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset size = 13854 sentences\n"]}],"source":["# import required libraries\n","import random\n","import sklearn\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","all_sents = [(sent, 'chesterton') for sent in author1_train]\n","all_sents += [(sent, 'shakespeare') for sent in author2_train]\n","# combine all sentences into a single list called all_sents, keeping the author label\n","print (f\"Dataset size = {str (len (all_sents))} sentences\")\n","\n","# keep the set of labels (authors) as values\n","values = [author for (sent, author) in all_sents]\n","split = StratifiedShuffleSplit (n_splits = 1, test_size = 0.2, random_state = 42)\n","strat_train_set = []\n","strat_pretest_set = []\n","for train_index, pretest_index in split.split (all_sents, values):\n","    strat_train_set= [all_sents [index] for index in train_index]\n","    strat_pretest_set = [all_sents [index]\n","                        for index in pretest_index]"]},{"cell_type":"markdown","id":"61801a1e","metadata":{"papermill":{"duration":0.024284,"end_time":"2023-12-22T17:55:53.032453","exception":false,"start_time":"2023-12-22T17:55:53.008169","status":"completed"},"tags":[]},"source":["**Check the proportions of the data in the two classes**"]},{"cell_type":"code","execution_count":28,"id":"624ed90c","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:53.085743Z","iopub.status.busy":"2023-12-22T17:55:53.084017Z","iopub.status.idle":"2023-12-22T17:55:53.100502Z","shell.execute_reply":"2023-12-22T17:55:53.099706Z"},"papermill":{"duration":0.045806,"end_time":"2023-12-22T17:55:53.103258","exception":false,"start_time":"2023-12-22T17:55:53.057452","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" Category     Overall   Stratified train  Stratified pretest \n"," chesterton   0.619677  0.619688          0.619632           \n"," shakespeare  0.380323  0.380312          0.380368           \n"]}],"source":["# calculate the proportion of the entries in each class (category) in the given dataset data\n","def cat_proportions (data, cat):\n","    count = 0\n","    for item in data:\n","        if item [1] == cat:\n","            count += 1\n","    return float (count) / float (len (data))\n","\n","categories = ['chesterton', 'shakespeare']\n","rows = []\n","rows.append ([\"Category\", \"Overall\", \"Stratified train\", \"Stratified pretest\"])\n","\n","for cat in categories:\n","    rows.append ([cat, f\"{cat_proportions (all_sents, cat):.6f}\",\n","                 f\"{cat_proportions (strat_train_set, cat):.6f}\",\n","                 f\"{cat_proportions (strat_pretest_set, cat):.6f}\"])\n","    \n","columns = zip (*rows)\n","column_widths = [max (len (item) for item in col) for col in columns]\n","for row in rows:\n","    print (''.join (' {:{width}} '.format (row [i], width = column_widths [i])\n","                   for i in range (0, len (row))))"]},{"cell_type":"markdown","id":"d9865dfa","metadata":{"papermill":{"duration":0.025276,"end_time":"2023-12-22T17:55:53.1534","exception":false,"start_time":"2023-12-22T17:55:53.128124","status":"completed"},"tags":[]},"source":["**Create the test_set data structure**"]},{"cell_type":"code","execution_count":29,"id":"045dfbac","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:53.205413Z","iopub.status.busy":"2023-12-22T17:55:53.204987Z","iopub.status.idle":"2023-12-22T17:55:53.903126Z","shell.execute_reply":"2023-12-22T17:55:53.902134Z"},"papermill":{"duration":0.726893,"end_time":"2023-12-22T17:55:53.905448","exception":false,"start_time":"2023-12-22T17:55:53.178555","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["11083\n","{'\"': True, 'Oh': True, ',': True, 'you': True, 'know': True, 'what': True, 'I': True, 'mean': True, ',\"': True, 'said': True, 'Turnbull': True, 'impatiently': True, '.': True}\n","{'The': True, 'shop': True, 'and': True, 'the': True, 'Cross': True, 'were': True, 'equally': True, 'uplifted': True, 'alone': True, 'in': True, 'empty': True, 'heavens': True, '.': True}\n"]}],"source":["test_set = [(sent, \"chesterton\") for sent in author1_test]\n","test_set += [(sent, \"shakespeare\") for sent in author2_test]\n","\n","# extract words as features\n","def get_features (text):\n","    features = {}\n","    word_list = [word for word in text]\n","    for word in word_list:\n","        features [word] = True\n","    return features\n","\n","# extract features from training and pretest sets\n","train_features = [(get_features (sents), label)\n","                 for (sents, label) in strat_train_set]\n","pretest_features = [(get_features (sents), label)\n","                   for (sents, label) in strat_pretest_set]\n","\n","# run some checks to see what the data contains\n","print (len (train_features))\n","print (train_features [0] [0])\n","print (train_features [100] [0])"]},{"cell_type":"markdown","id":"c56eac63","metadata":{"papermill":{"duration":0.022435,"end_time":"2023-12-22T17:55:53.951622","exception":false,"start_time":"2023-12-22T17:55:53.929187","status":"completed"},"tags":[]},"source":["**Train the Naive Bayes classifier on train and test on pretest set**"]},{"cell_type":"code","execution_count":30,"id":"c23df462","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:54.007024Z","iopub.status.busy":"2023-12-22T17:55:54.006543Z","iopub.status.idle":"2023-12-22T17:55:56.094353Z","shell.execute_reply":"2023-12-22T17:55:56.092735Z"},"papermill":{"duration":2.122069,"end_time":"2023-12-22T17:55:56.097355","exception":false,"start_time":"2023-12-22T17:55:53.975286","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size = 11083 sentences\n","Pretest set size = 2771 sentences\n","Accuracy on the training set = 0.9770820175042858\n","Accuracy on the pretest set = 0.9581378563695416\n","Most Informative Features\n","                  Caesar = True           shakes : cheste =     61.7 : 1.0\n","                   voice = True           cheste : shakes =     56.3 : 1.0\n","                      st = True           shakes : cheste =     51.6 : 1.0\n","                    thee = True           shakes : cheste =     51.2 : 1.0\n","                    thou = True           shakes : cheste =     49.4 : 1.0\n","                   quite = True           cheste : shakes =     41.1 : 1.0\n","                    been = True           cheste : shakes =     40.6 : 1.0\n","                 because = True           cheste : shakes =     39.5 : 1.0\n","                     thy = True           shakes : cheste =     37.5 : 1.0\n","                    said = True           cheste : shakes =     35.6 : 1.0\n","                   white = True           cheste : shakes =     35.4 : 1.0\n","                     sea = True           cheste : shakes =     34.6 : 1.0\n","                 looking = True           cheste : shakes =     34.2 : 1.0\n","                  behind = True           cheste : shakes =     33.8 : 1.0\n","                     got = True           cheste : shakes =     31.3 : 1.0\n","                    mean = True           cheste : shakes =     31.3 : 1.0\n","                 Caesars = True           shakes : cheste =     29.9 : 1.0\n","                       O = True           shakes : cheste =     29.5 : 1.0\n","                   small = True           cheste : shakes =     28.8 : 1.0\n","                     has = True           cheste : shakes =     28.8 : 1.0\n","                    grey = True           cheste : shakes =     26.8 : 1.0\n","                     own = True           cheste : shakes =     25.9 : 1.0\n","                   cried = True           cheste : shakes =     25.7 : 1.0\n","                     Nay = True           shakes : cheste =     25.5 : 1.0\n","                   think = True           cheste : shakes =     24.2 : 1.0\n","                    back = True           cheste : shakes =     23.6 : 1.0\n","                  moment = True           cheste : shakes =     23.0 : 1.0\n","                  Mother = True           shakes : cheste =     22.5 : 1.0\n","                    want = True           cheste : shakes =     22.5 : 1.0\n","                       d = True           shakes : cheste =     22.1 : 1.0\n","                     far = True           cheste : shakes =     19.8 : 1.0\n","                   large = True           cheste : shakes =     19.8 : 1.0\n","                   front = True           cheste : shakes =     19.0 : 1.0\n","                    mind = True           cheste : shakes =     19.0 : 1.0\n","                      ha = True           shakes : cheste =     19.0 : 1.0\n","                    talk = True           cheste : shakes =     18.2 : 1.0\n","                   broke = True           cheste : shakes =     17.8 : 1.0\n","                    side = True           cheste : shakes =     17.1 : 1.0\n","                  except = True           cheste : shakes =     17.0 : 1.0\n","                       4 = True           shakes : cheste =     16.8 : 1.0\n","                 whether = True           cheste : shakes =     16.2 : 1.0\n","                    seen = True           cheste : shakes =     15.8 : 1.0\n","                    ship = True           cheste : shakes =     15.8 : 1.0\n","                  coming = True           cheste : shakes =     15.8 : 1.0\n","                    wild = True           cheste : shakes =     15.8 : 1.0\n","                   World = True           shakes : cheste =     15.7 : 1.0\n","                     bee = True           shakes : cheste =     15.7 : 1.0\n","                 instant = True           cheste : shakes =     15.6 : 1.0\n","                answered = True           cheste : shakes =     15.5 : 1.0\n","                     low = True           cheste : shakes =     14.9 : 1.0\n"]}],"source":["# import the classifier\n","from nltk import NaiveBayesClassifier, classify\n","\n","# train the classifier on the training set\n","print (f\"Training set size = {str (len (train_features))} sentences\")\n","print (f\"Pretest set size = {str (len (pretest_features))} sentences\")\n","classifier = NaiveBayesClassifier.train (train_features)\n","\n","print (f\"Accuracy on the training set = {str (classify.accuracy (classifier, train_features))}\")\n","print (f\"Accuracy on the pretest set = \" + \n","      f\"{str (classify.accuracy (classifier, pretest_features))}\")\n","classifier.show_most_informative_features (50)"]},{"cell_type":"markdown","id":"64d81ae2","metadata":{"papermill":{"duration":0.025275,"end_time":"2023-12-22T17:55:56.148492","exception":false,"start_time":"2023-12-22T17:55:56.123217","status":"completed"},"tags":[]},"source":["**Code to extract words and sentence length statistics**"]},{"cell_type":"code","execution_count":31,"id":"e474c0ee","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:56.203108Z","iopub.status.busy":"2023-12-22T17:55:56.202639Z","iopub.status.idle":"2023-12-22T17:55:56.211642Z","shell.execute_reply":"2023-12-22T17:55:56.209865Z"},"papermill":{"duration":0.039905,"end_time":"2023-12-22T17:55:56.215015","exception":false,"start_time":"2023-12-22T17:55:56.17511","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["3.5714285714285716\n","7.0\n"]}],"source":["def avg_number_chars (text):\n","    total_chars = 0.0\n","    for word in text:\n","        total_chars += len (word)\n","    return float (total_chars) / float (len(text))\n","\n","# calculate the sentence length in terms of the number of words\n","def number_words (text):\n","    return float (len (text))\n","\n","print (avg_number_chars ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))\n","print (number_words ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))"]},{"cell_type":"markdown","id":"5a242a0f","metadata":{"papermill":{"duration":0.025001,"end_time":"2023-12-22T17:55:56.266061","exception":false,"start_time":"2023-12-22T17:55:56.24106","status":"completed"},"tags":[]},"source":["**Code to extract features and map them to the labels**"]},{"cell_type":"code","execution_count":32,"id":"9128de4b","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:56.320455Z","iopub.status.busy":"2023-12-22T17:55:56.319227Z","iopub.status.idle":"2023-12-22T17:55:56.396709Z","shell.execute_reply":"2023-12-22T17:55:56.394832Z"},"papermill":{"duration":0.108198,"end_time":"2023-12-22T17:55:56.399924","exception":false,"start_time":"2023-12-22T17:55:56.291726","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["11083 11083\n","2771 2771\n","5649 5649\n"]}],"source":["# argument source denotes the dataset we are applying the feature extraction\n","def initialize_dataset (source):\n","    all_features = []\n","    targets = []\n","    # iterate through all (sent, label) pairs in the given dataset\n","    for (sent, label) in source:\n","        feature_list = []\n","        feature_list.append (avg_number_chars (sent))\n","        feature_list.append (number_words (sent))\n","        all_features.append (feature_list)\n","        if label == \"chesterton\": targets.append (0)\n","        else: targets.append (1)\n","    return all_features, targets\n","\n","train_data, train_targets = initialize_dataset (strat_train_set)\n","pretest_data, pretest_targets = initialize_dataset (strat_pretest_set)\n","test_data, test_targets = initialize_dataset (test_set)\n","\n","# print out thr length of the structures\n","print (len (train_data), len (train_targets))\n","print (len (pretest_data), len (pretest_targets))\n","print (len (test_data), len (test_targets))"]},{"cell_type":"markdown","id":"fcf38eb2","metadata":{"papermill":{"duration":0.025961,"end_time":"2023-12-22T17:55:56.452357","exception":false,"start_time":"2023-12-22T17:55:56.426396","status":"completed"},"tags":[]},"source":["**Train and test a classifier with sklearn**"]},{"cell_type":"code","execution_count":33,"id":"3f66aae8","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:56.507672Z","iopub.status.busy":"2023-12-22T17:55:56.507233Z","iopub.status.idle":"2023-12-22T17:55:56.658586Z","shell.execute_reply":"2023-12-22T17:55:56.657701Z"},"papermill":{"duration":0.183125,"end_time":"2023-12-22T17:55:56.661616","exception":false,"start_time":"2023-12-22T17:55:56.478491","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7470227354745579\n","[[1485  232]\n"," [ 469  585]]\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.86      0.81      1717\n","           1       0.72      0.56      0.63      1054\n","\n","    accuracy                           0.75      2771\n","   macro avg       0.74      0.71      0.72      2771\n","weighted avg       0.74      0.75      0.74      2771\n","\n","0.7603115595680652\n","[[3194  548]\n"," [ 806 1101]]\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.85      0.83      3742\n","           1       0.67      0.58      0.62      1907\n","\n","    accuracy                           0.76      5649\n","   macro avg       0.73      0.72      0.72      5649\n","weighted avg       0.75      0.76      0.76      5649\n","\n"]}],"source":["# import decision tree classifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# initialization\n","text_clf = DecisionTreeClassifier (random_state = 42)\n","\n","# train the classifier using the fit method\n","text_clf.fit (train_data, train_targets)\n","\n","# test the classifier using the predict method\n","predicted = text_clf.predict (pretest_data)\n","\n","# evaluating the classifier\n","# import numpy and sklearn's metrics funcvtionality\n","import numpy as np\n","from sklearn import metrics\n","\n","def evaluate (predicted, targets):\n","    # use numpy.mean to estimate the accuracy of the classifier\n","    print (np.mean (predicted == targets))\n","    print (metrics.confusion_matrix (targets, predicted))\n","    print (metrics.classification_report (targets, predicted))\n","    \n","evaluate (predicted, pretest_targets)\n","\n","# apply the same routine to the test set\n","predicted = text_clf.predict (test_data)\n","evaluate (predicted, test_targets)"]},{"cell_type":"markdown","id":"8eec9018","metadata":{"papermill":{"duration":0.02847,"end_time":"2023-12-22T17:55:56.715696","exception":false,"start_time":"2023-12-22T17:55:56.687226","status":"completed"},"tags":[]},"source":["**Calculate the number and proportion of times certain words occur**"]},{"cell_type":"code","execution_count":34,"id":"146d89ff","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:56.769472Z","iopub.status.busy":"2023-12-22T17:55:56.769009Z","iopub.status.idle":"2023-12-22T17:55:56.774875Z","shell.execute_reply":"2023-12-22T17:55:56.773853Z"},"papermill":{"duration":0.035174,"end_time":"2023-12-22T17:55:56.776668","exception":false,"start_time":"2023-12-22T17:55:56.741494","status":"completed"},"tags":[]},"outputs":[],"source":["def word_counts(text):\n","    counts = {}\n","    for word in text:\n","        counts[word.lower()] = counts.get(word.lower(), 0) + 1\n","    return counts\n","\n","def proportion_words(text, wordlist):\n","    count = 0\n","    for word in text:\n","        if word.lower() in wordlist:\n","            count += 1\n","    return float(count)/float(len(text))"]},{"cell_type":"markdown","id":"d29659a6","metadata":{"papermill":{"duration":0.023795,"end_time":"2023-12-22T17:55:56.827155","exception":false,"start_time":"2023-12-22T17:55:56.80336","status":"completed"},"tags":[]},"source":["**Adding stopword counts and proportion as features**"]},{"cell_type":"code","execution_count":35,"id":"7dd58952","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:55:56.878966Z","iopub.status.busy":"2023-12-22T17:55:56.878544Z","iopub.status.idle":"2023-12-22T17:56:02.590879Z","shell.execute_reply":"2023-12-22T17:56:02.588559Z"},"papermill":{"duration":5.741931,"end_time":"2023-12-22T17:56:02.593457","exception":false,"start_time":"2023-12-22T17:55:56.851526","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["11083 11083\n","2771 2771\n","5649 5649\n"]}],"source":["import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","# add spaCys functionality to the code and upload the stopwords list\n","nlp = spacy.load('en_core_web_lg')\n","def initialize_dataset(source):\n","    all_features = []\n","    targets = []\n","    for (sent, label) in source:\n","        feature_list=[]\n","        feature_list.append(avg_number_chars(sent))\n","        feature_list.append(number_words(sent))\n","        counts = word_counts(sent)\n","        for word in STOP_WORDS:\n","            if word in counts.keys():\n","                feature_list.append(counts.get(word))\n","            else:\n","                feature_list.append(0)\n","        feature_list.append(proportion_words(sent, STOP_WORDS))\n","        all_features.append(feature_list)\n","        if label==\"austen\": targets.append(0)\n","        else: targets.append(1)\n","    return all_features, targets\n","\n","train_data, train_targets = initialize_dataset(strat_train_set)\n","pretest_data, pretest_targets = initialize_dataset(strat_pretest_set)\n","test_data, test_targets = initialize_dataset(test_set)\n","\n","# Print out the length of the feature lists and targets lists\n","print (len(train_data), len(train_targets))\n","print (len(pretest_data), len(pretest_targets))\n","print (len(test_data), len(test_targets))"]},{"cell_type":"markdown","id":"1f2332c6","metadata":{"papermill":{"duration":0.023851,"end_time":"2023-12-22T17:56:02.642844","exception":false,"start_time":"2023-12-22T17:56:02.618993","status":"completed"},"tags":[]},"source":["**Evaluate the results**"]},{"cell_type":"code","execution_count":36,"id":"2466026e","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:56:02.694261Z","iopub.status.busy":"2023-12-22T17:56:02.693758Z","iopub.status.idle":"2023-12-22T17:56:02.994419Z","shell.execute_reply":"2023-12-22T17:56:02.992279Z"},"papermill":{"duration":0.330133,"end_time":"2023-12-22T17:56:02.996973","exception":false,"start_time":"2023-12-22T17:56:02.66684","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n","[[2771]]\n","              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00      2771\n","\n","    accuracy                           1.00      2771\n","   macro avg       1.00      1.00      1.00      2771\n","weighted avg       1.00      1.00      1.00      2771\n","\n","1.0\n","[[5649]]\n","              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00      5649\n","\n","    accuracy                           1.00      5649\n","   macro avg       1.00      1.00      1.00      5649\n","weighted avg       1.00      1.00      1.00      5649\n","\n"]}],"source":["# train the classifier on the training data\n","text_clf = DecisionTreeClassifier(random_state=42)\n","text_clf.fit(train_data, train_targets)\n","\n","# test on the pretest set\n","predicted = text_clf.predict(pretest_data)\n","evaluate(predicted, pretest_targets)\n","\n","# apply the same routine to the test set\n","predicted = text_clf.predict(test_data)\n","evaluate(predicted, test_targets)"]},{"cell_type":"markdown","id":"c20a813f","metadata":{"papermill":{"duration":0.025829,"end_time":"2023-12-22T17:56:03.051329","exception":false,"start_time":"2023-12-22T17:56:03.0255","status":"completed"},"tags":[]},"source":["**Applying spaCy preprocessing**"]},{"cell_type":"code","execution_count":37,"id":"2e2dfdbe","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:56:03.105591Z","iopub.status.busy":"2023-12-22T17:56:03.105162Z","iopub.status.idle":"2023-12-22T17:58:29.831537Z","shell.execute_reply":"2023-12-22T17:58:29.829718Z"},"papermill":{"duration":146.756589,"end_time":"2023-12-22T17:58:29.83384","exception":false,"start_time":"2023-12-22T17:56:03.077251","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2000 texts processed\n","4000 texts processed\n","6000 texts processed\n","8000 texts processed\n","10000 texts processed\n","Dataset processed\n","2000 texts processed\n","Dataset processed\n","2000 texts processed\n","4000 texts processed\n","Dataset processed\n"]}],"source":["# provide the preprocess function with the original sentences from the datasets\n","def preprocess(source):\n","    source_docs = {}\n","    index = 0\n","    for (sent, label) in source:\n","        text = \" \".join(sent)\n","        source_docs[text] = nlp(text)\n","        if index>0 and (index%2000)==0:\n","            print(str(index) + \" texts processed\")\n","        index += 1\n","    print(\"Dataset processed\")\n","    return source_docs\n","\n","# apply the preprocess function to the three original datasets\n","train_docs = preprocess(strat_train_set)\n","pretest_docs = preprocess(strat_pretest_set)\n","test_docs = preprocess(test_set)"]},{"cell_type":"markdown","id":"d61cdcf9","metadata":{"papermill":{"duration":0.027028,"end_time":"2023-12-22T17:58:29.888852","exception":false,"start_time":"2023-12-22T17:58:29.861824","status":"completed"},"tags":[]},"source":["**Adding distribution of part-of-speech tags as features**"]},{"cell_type":"code","execution_count":38,"id":"5b08c61c","metadata":{"execution":{"iopub.execute_input":"2023-12-22T17:58:29.945252Z","iopub.status.busy":"2023-12-22T17:58:29.94476Z","iopub.status.idle":"2023-12-22T17:58:29.954701Z","shell.execute_reply":"2023-12-22T17:58:29.953803Z"},"papermill":{"duration":0.04078,"end_time":"2023-12-22T17:58:29.956529","exception":false,"start_time":"2023-12-22T17:58:29.915749","status":"completed"},"tags":[]},"outputs":[],"source":["# import Pythons Counter functionality to simplify counting procedures\n","from collections import Counter\n","pos_list = [\"C\", \"D\", \"E\", \"F\", \"I\", \"J\", \"M\",\n","            \"N\", \"P\", \"R\", \"T\", \"U\", \"V\", \"W\"]\n","\n","def pos_counts(text, source_docs, pos_list):\n","    pos_counts = {}\n","    doc = source_docs.get(\" \".join(text))\n","    tags = []\n","    for word in doc:\n","        tags.append(str(word.tag_)[0])\n","    counts = Counter(tags)\n","    for pos in pos_list:\n","        if pos in counts.keys():\n","            pos_counts[pos] = counts.get(pos)\n","        # Populate the pos_counts dictionary using the counts\n","        # of the part-of-speech tags or inserting 0    \n","        else: pos_counts[pos] = 0\n","    return pos_counts\n","\n","def initialize_dataset(source, source_docs):\n","    all_features = []\n","    targets = []\n","    for (sent, label) in source:\n","        feature_list=[]\n","        feature_list.append(avg_number_chars(sent))\n","        feature_list.append(number_words(sent))\n","        counts = word_counts(sent)\n","        for word in STOP_WORDS:\n","            if word in counts.keys():\n","                feature_list.append(counts.get(word))\n","            else:\n","                feature_list.append(0)\n","        feature_list.append(proportion_words(sent, STOP_WORDS))\n","        # extract the previous 308 features as before\n","        p_counts = pos_counts(sent, source_docs, pos_list)\n","        for pos in p_counts.keys():\n","            feature_list.append(float(p_counts.get(pos))/float(len(sent)))\n","        all_features.append(feature_list)\n","        if label==\"austen\": targets.append(0)\n","        else: targets.append(1)\n","    return all_features, targets"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":150,"sourceId":799971,"sourceType":"datasetVersion"},{"datasetId":576263,"sourceId":1043323,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":1026.979739,"end_time":"2023-12-22T17:58:33.087859","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-22T17:41:26.10812","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}