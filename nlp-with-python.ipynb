{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":799971,"sourceType":"datasetVersion","datasetId":150},{"sourceId":1043323,"sourceType":"datasetVersion","datasetId":576263}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/nlp-with-python?scriptVersionId=197051836\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-31T15:25:50.061941Z","iopub.execute_input":"2023-12-31T15:25:50.06231Z","iopub.status.idle":"2023-12-31T15:25:50.478027Z","shell.execute_reply.started":"2023-12-31T15:25:50.062281Z","shell.execute_reply":"2023-12-31T15:25:50.47705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to populate the documents dictionary**","metadata":{}},{"cell_type":"code","source":"def read_documents ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL\")\n    merged = \" \"\n    # the string variable merged keeps the result of merging the field identifier with its content\n    \n    for a_line in f.readlines ():\n        if a_line.startswith (\".\"):\n            merged += \"\\n\" + a_line.strip ()\n        else:\n            merged += \" \" + a_line.strip ()\n    # updates the merged variable using a for-loop\n    \n    documents = {}\n    \n    content = \"\"\n    doc_id = \"\"\n    # each entry in the dictioanry contains key = doc_id and value = content\n    \n    for a_line in merged.split (\"\\n\"):\n        if a_line.startswith (\".I\"):\n            doc_id = a_line.split (\" \") [1].strip()\n        elif a_line.startswith (\".X\"):\n            documents[doc_id] = content\n            content = \"\"\n            doc_id = \"\"\n        else:\n            content += a_line.strip ()[3:] + \" \"\n    f.close ()\n    return documents\n\n# print out the size of the dictionary and the content of the very first article\ndocuments = read_documents ()\nprint (len (documents))\nprint (documents.get (\"1\"))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:50.480747Z","iopub.execute_input":"2023-12-31T15:25:50.481324Z","iopub.status.idle":"2023-12-31T15:25:50.605889Z","shell.execute_reply.started":"2023-12-31T15:25:50.481283Z","shell.execute_reply":"2023-12-31T15:25:50.604877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to populate the queries dictionary**","metadata":{}},{"cell_type":"code","source":"def read_queries ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY\")\n    merged = \"\"\n    \n    # merge the conten of each field with its identifier and separate different fields with lune breaks\n    for a_line in f.readlines ():\n        if a_line.startswith (\".\"):\n            merged += \"\\n\" + a_line.strip ()\n        else:\n            merged += \" \" + a_line.strip ()\n    \n    queries = {}\n    \n    # initialize queries dictionary with key = qry_id and value=content for each query in the dataset\n    content = \"\"\n    qry_id = \"\"\n    \n    for a_line in merged.split (\"\\n\"):\n        if a_line.startswith (\".I\"):\n            if not content == \"\":\n                queries [qry_id] = content\n                content = \"\"\n                qry_id = \"\"\n            # add an enrty to the dictionary when you encounter an .I identifier\n            qry_id = a_line.split(\" \")[1].strip ()\n        # otherwise, keep adding content to the content variable\n        elif a_line.startswith (\".W\") or a_line.startswith (\".T\"):\n            content += a_line.strip ()[3:] + \" \"\n    queries [qry_id] = content\n    f.close ()\n    return queries\n\n# print out the length of the dictionary and the content of the first query\nqueries = read_queries ()\nprint (len (queries))\nprint (queries.get(\"1\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:50.607478Z","iopub.execute_input":"2023-12-31T15:25:50.607883Z","iopub.status.idle":"2023-12-31T15:25:50.628821Z","shell.execute_reply.started":"2023-12-31T15:25:50.607845Z","shell.execute_reply":"2023-12-31T15:25:50.627571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to populate the mappings dictionary**","metadata":{}},{"cell_type":"code","source":"def read_mappings ():\n    f = open (\"/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL\")\n    mappings = {}\n    \n    for a_line in f.readlines ():\n        voc = a_line.strip ().split ()\n        key = voc[0].strip ()\n        current_value = voc[1].strip()\n        value = []\n        # update the entry in the mappings dictionary with the current value\n        if key in mappings.keys ():\n            value = mappings.get (key)\n        value.append (current_value)\n        mappings [key] = value\n    f.close ()\n    return mappings\n\n# print out some information about the mapping data structure\nmappings = read_mappings ()\nprint (len (mappings))\nprint (mappings.keys ())\nprint (mappings.get (\"1\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:50.631662Z","iopub.execute_input":"2023-12-31T15:25:50.632316Z","iopub.status.idle":"2023-12-31T15:25:50.652556Z","shell.execute_reply.started":"2023-12-31T15:25:50.632265Z","shell.execute_reply":"2023-12-31T15:25:50.651524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess the data in documents and queries**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport nltk\nfrom nltk import word_tokenize\n\n# text is converted to lowercase and split into words\ndef get_words (text):\n    word_list = [word for word in word_tokenize (text.lower ())]\n    return word_list\n    \ndoc_words = {}\nqry_words = {}\n\nfor doc_id in documents.keys ():\n    doc_words [doc_id] = get_words (documents.get (doc_id))\nfor qry_id in queries.keys ():\n    # entries in both documents and queries are represented as word lists\n    qry_words [qry_id] = get_words (queries.get (qry_id))\n    \n# print out the length of the dictionaries and check the first document and the fisrt query\nprint (len (doc_words))\nprint (doc_words.get (\"1\"))\nprint (len (doc_words.get (\"1\")))\nprint (len (qry_words))\nprint (qry_words.get (\"1\"))\nprint (len (qry_words.get(\"1\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:50.654119Z","iopub.execute_input":"2023-12-31T15:25:50.654766Z","iopub.status.idle":"2023-12-31T15:25:54.656286Z","shell.execute_reply.started":"2023-12-31T15:25:50.654726Z","shell.execute_reply":"2023-12-31T15:25:54.655272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Simple Biilean search algorithm**","metadata":{}},{"cell_type":"code","source":"# iterate through the documents\ndef retrieve_documents (doc_words, query):\n    docs = []\n    for doc_id in doc_words.keys ():\n        found = False\n        i = 0\n        while i<len(query) and not found: \n            word = query [i]\n            if word in doc_words.get (doc_id):\n                docs.append (doc_id)\n                found = True\n            else:\n                i+=1\n    return docs\n\n# check the results\ndocs = retrieve_documents (doc_words, qry_words.get(\"3\"))\nprint (docs [:100])\nprint (len (docs))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:54.657595Z","iopub.execute_input":"2023-12-31T15:25:54.657925Z","iopub.status.idle":"2023-12-31T15:25:54.67673Z","shell.execute_reply.started":"2023-12-31T15:25:54.657885Z","shell.execute_reply":"2023-12-31T15:25:54.675126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Begin the preprocessing - remove stopwords and punctuation marks**","metadata":{}},{"cell_type":"code","source":"# import python's string module that will help remove punctuation marks\nimport string\n\n# import the stopwords list\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef process (text):\n    stoplist = set (stopwords.words ('english'))\n    # only add tthe words if they are not included in the stoplist and are not puctuation marks\n    word_list = [word for word in word_tokenize (text.lower())\n                if not word in stoplist and not word in string.punctuation]\n    return word_list\n\n# check the results of these preprocessing steps on some documents or queries\nword_list = process (documents.get (\"1\"))\nprint (word_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:54.678954Z","iopub.execute_input":"2023-12-31T15:25:54.679601Z","iopub.status.idle":"2023-12-31T15:25:54.696717Z","shell.execute_reply.started":"2023-12-31T15:25:54.67956Z","shell.execute_reply":"2023-12-31T15:25:54.695944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next step in preprocessing - stemming**","metadata":{}},{"cell_type":"code","source":"# import the stemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\ndef process (text):\n    stoplist = set (stopwords.words ('english'))\n    # initialize the LancasterStemmer\n    st = LancasterStemmer ()\n    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n                if not word in stoplist and not word in string.punctuation]\n    return word_list\n\n# check the results on some document, query, or on a list of words\nword_list = process (documents.get(\"26\"))\nprint (word_list)\nword_list = process (\"organize, organizing, organizational, organ, organic, organizer\")\nprint (word_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:54.69788Z","iopub.execute_input":"2023-12-31T15:25:54.698711Z","iopub.status.idle":"2023-12-31T15:25:54.711917Z","shell.execute_reply.started":"2023-12-31T15:25:54.698679Z","shell.execute_reply":"2023-12-31T15:25:54.710652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Estimate term frequency in documents and queries**","metadata":{}},{"cell_type":"code","source":"def get_terms (text):\n    stoplist = set (stopwords.words ('english'))\n    terms = {}\n    st = LancasterStemmer ()\n    word_list = [st.stem(word) for word in word_tokenize (text.lower ())\n                if not word in stoplist and not word in string.punctuation]\n    for word in word_list:\n        terms [word] = terms.get (word, 0) + 1\n    return terms\n\ndoc_terms = {}\nqry_terms = {}\nfor doc_id in documents.keys ():\n    doc_terms [doc_id] = get_terms (documents.get (doc_id))\nfor qry_id in queries.keys ():\n    # populate the term frequency dictionaries for all documents and all queries\n    qry_terms [qry_id] = get_terms (queries.get (qry_id))\n    \n# check the results\nprint (len (doc_terms))\nprint (doc_terms.get (\"1\"))\nprint (len (doc_terms.get(\"1\")))\nprint (len (qry_terms))\nprint (qry_terms.get(\"1\"))\nprint (len (qry_terms.get(\"1\")))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:54.713623Z","iopub.execute_input":"2023-12-31T15:25:54.714028Z","iopub.status.idle":"2023-12-31T15:25:59.126555Z","shell.execute_reply.started":"2023-12-31T15:25:54.71399Z","shell.execute_reply":"2023-12-31T15:25:59.125458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to represent the datya in a shared space**","metadata":{}},{"cell_type":"code","source":"# collect the shared vocabulary of terms from documents and queries and return it as a sorted list\ndef collect_vocabulary ():\n    all_terms = []\n    for doc_id in doc_terms.keys ():\n        for term in doc_terms.get (doc_id).keys():\n            all_terms.append (term)\n    for qry_id in qry_terms.keys ():\n        for term in qry_terms.keys():\n            for term in qry_terms.get(qry_id).keys():\n                all_terms.append (term)\n    return sorted (set (all_terms))\n\n# print out the length of the shared vocabulary and check the first several terms in the vocabulary\nall_terms = collect_vocabulary ()\nprint (len (all_terms))\nprint (all_terms [:10])\n\ndef vectorize (input_features, vocabulary):\n    output = {}\n    for item_id in input_features.keys ():\n        features = input_features.get (item_id)\n        output_vector = []\n        for word in vocabulary:\n            if word in features.keys ():\n                output_vector.append (int (features.get (word)))\n            else:\n                output_vector.append (0)\n        output [item_id] = output_vector\n    return output\n\ndoc_vectors = vectorize (doc_terms, all_terms)\nqry_vectors = vectorize (qry_terms, all_terms)\n\n# print out some statistics on these data structures\nprint (len (doc_vectors))\nprint (len (doc_vectors.get (\"1450\")))\nprint (len (qry_vectors))\nprint (len (qry_vectors.get (\"110\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:25:59.13216Z","iopub.execute_input":"2023-12-31T15:25:59.132503Z","iopub.status.idle":"2023-12-31T15:26:02.166069Z","shell.execute_reply.started":"2023-12-31T15:25:59.132474Z","shell.execute_reply":"2023-12-31T15:26:02.164931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate and apply inverse document frequency weighting**","metadata":{}},{"cell_type":"code","source":"# import library for math\nimport math\n\ndef calculate_idfs (vocabulary, doc_features):\n    doc_idfs = {}\n    for term in vocabulary:\n        doc_count = 0\n        for doc_id in doc_features.keys ():\n            terms = doc_features.get (doc_id)\n            if term in terms.keys ():\n                doc_count += 1\n        doc_idfs [term] = math.log (float (len (doc_features.keys ()))/\n                                    float (1 + doc_count), 10)\n    return doc_idfs\n\n# check the results - we should have idf values for all terms from the vocabulary\ndoc_idfs = calculate_idfs (all_terms, doc_terms)\nprint (len (doc_idfs))\nprint (doc_idfs.get (\"system\"))\n\n# define a function to apply idf weighing to the input_terms data structure\ndef vectorize_idf (input_terms, input_idfs, vocabulary):\n    output = {}\n    for item_id in input_terms.keys ():\n        terms = input_terms.get (item_id)\n        output_vector = []\n        for term in vocabulary:\n            if term in terms.keys ():\n                # multiply the term frequencies with idf weights if the term is present in document\n                output_vector.append (\n                input_idfs.get (term) * float (terms.get (term)))\n            else:\n                output_vector.append (float (0))\n        output [item_id] = output_vector\n    return output\n\n# apply idf weighing to doc_terms\ndoc_vectors = vectorize_idf (doc_terms, doc_idfs, all_terms)\n\n# print out some statistics, such as the number of documents and terms\nprint (len (doc_vectors))\nprint (len (doc_vectors.get (\"1460\")))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:26:02.167452Z","iopub.execute_input":"2023-12-31T15:26:02.168436Z","iopub.status.idle":"2023-12-31T15:26:09.250172Z","shell.execute_reply.started":"2023-12-31T15:26:02.168398Z","shell.execute_reply":"2023-12-31T15:26:09.249033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run search algorithm for a given query on the set of the documents**","metadata":{}},{"cell_type":"code","source":"# the operator's itemgetter functionality helps sort Python dictionaries by keys or values\nfrom operator import itemgetter\n\n# calculate the length of the input vector\ndef length (vector):\n    sq_length = 0\n    for index in range (0, len(vector)):\n        sq_length += math.pow (vector [index], 2)\n    return math.sqrt (sq_length)\n\n# calculate the dot product of two vectors\ndef dot_product (vector1, vector2):\n    if len (vector1) == len (vector2):\n        dot_prod = 0\n        for index in range (0, len(vector1)):\n            if not vector1 [index] == 0 and not vector2 [index] == 0:\n                dot_prod += vector1 [index] * vector2 [index]\n        return dot_prod\n    else:\n        return \"Unmatching dimensionality\"\n    \ndef calculate_cosine (query, document):\n    cosine = dot_product (query, document) / (length (query) * length (document))\n    return cosine\n\nquery = qry_vectors.get (\"3\")\nresults = {}\n\nfor doc_id in doc_vectors.keys ():\n    document = doc_vectors.get (doc_id)\n    cosine = calculate_cosine (query, document)\n    results [doc_id] = cosine\n    \n# sort the results dictionary by cosine values in descending order and return the top n results\nfor items in sorted (results.items (), key = itemgetter (1), reverse = True) [:44]:\n    print (items [0])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:26:09.251542Z","iopub.execute_input":"2023-12-31T15:26:09.252573Z","iopub.status.idle":"2023-12-31T15:26:15.702241Z","shell.execute_reply.started":"2023-12-31T15:26:09.25253Z","shell.execute_reply":"2023-12-31T15:26:15.701151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Estimate precision@k and ratio of cases with at least one relevant document**","metadata":{}},{"cell_type":"code","source":"# calculate the proportion of relevant documents from the gold standard in the top k returned results\ndef calculate_precision (model_output, gold_standard):\n    true_pos = 0\n    for item in model_output:\n        if item in gold_standard:\n            true_pos += 1\n    return float (true_pos) / float (len (model_output))\n\ndef calculate_found (model_output, gold_standard):\n    found = 0\n    for item in model_output:\n        if item in gold_standard:\n            found = 1\n    return float (found)\n\nprecision_all = 0.0\nfound_all = 0.0\nfor query_id in mappings.keys ():\n    # calculate mean values across all queries\n    gold_standard = mappings.get (str (query_id))\n    query = qry_vectors.get (str (query_id))\n    results = {}\n    model_output = []\n    for doc_id in doc_vectors.keys ():\n        document = doc_vectors.get (doc_id)\n        cosine = calculate_cosine (query, document)\n        # for each document, esimate its relevance to the query with cosine similarity as before\n        results [doc_id] = cosine\n    # sort the results and consider only top k (top 5) most relevant documents\n    for items in sorted (results.items (), key = itemgetter (1), reverse = True) [:5]:\n        model_output.append (items [0])\n    precision = calculate_precision (model_output, gold_standard)\n    found = calculate_found (model_output, gold_standard)\n    print (f\"{str (query_id)} : {str(precision)}\")\n    precision_all += precision\n    found_all += found\n    \n# estimate the mean values for all queries\nprint (precision_all / float (len (mappings.keys ())))\nprint (found_all / float (len (mappings.keys ())))    ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:26:15.703731Z","iopub.execute_input":"2023-12-31T15:26:15.704021Z","iopub.status.idle":"2023-12-31T15:34:20.237673Z","shell.execute_reply.started":"2023-12-31T15:26:15.703996Z","shell.execute_reply":"2023-12-31T15:34:20.236462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On some queries the algorithm perform very well. For example, \"1 : 1.0\" shows that all top 5 documents returned for query 1 are relevant. However, on other queries the alforithm does not perform well.","metadata":{}},{"cell_type":"markdown","source":"**Estimate mean reciprocal rank**","metadata":{}},{"cell_type":"code","source":"rank_all = 0.0\nfor query_id in mappings.keys ():\n    gold_standard = mappings.get (str (query_id))\n    query = qry_vectors.get (str (query_id))\n    results = {}\n    for doc_id in doc_vectors.keys ():\n        document = doc_vectors.get (doc_id)\n        cosine = calculate_cosine (query, document)\n        results [doc_id] = cosine\n    sorted_results = sorted (results.items (),\n                            key=itemgetter (1), reverse = True)\n    index = 0\n    found = False\n    while found == False:\n        # set the flag found to False and switch it to True when we find the first relevant document\n        item = sorted_results [index]\n        # increment the index with each document in the results\n        index += 1\n        if index == len (sorted_results):\n            found = True\n        if item [0] in gold_standard:\n            # the document ID is the first element in the sorted tuples oof (document_id, similarity score)\n            found = True\n            print (f\"{str(query_id)}: {str(float (1) / float (index))}\")\n            rank_all += float(1) / float (index)\n            \n# print out the mean valur across all queries\nprint (rank_all / float (len (mappings.keys ())))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:34:20.239439Z","iopub.execute_input":"2023-12-31T15:34:20.23991Z","iopub.status.idle":"2023-12-31T15:42:29.128031Z","shell.execute_reply.started":"2023-12-31T15:34:20.239868Z","shell.execute_reply":"2023-12-31T15:42:29.126927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Example how to run spaCy's processing pipeline**","metadata":{}},{"cell_type":"code","source":"# import library\nimport spacy\n\n# the spacy.load command initializes the nlp pipeline\nnlp = spacy.load (\"en_core_web_sm\")\ndoc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\nrows = []\n\n# print the output in a tabular format and add a header to the printout for clarity\nrows.append ([\"Word\", \"Position\", \"Lowercase\", \"Lemma\", \"POS\", \"Alphanumeric\", \"Stopword\"])\n\nfor token in doc:\n    rows.append ([token.text, str(token.i), token.lower_, token.lemma_,\n                 token.pos_, str(token.is_alpha), str (token.is_stop)])\n    \n# Python's zip function allows to reformat input from row representation\ncolumns = zip (*rows)\ncolumn_widths = [max (len (item) for item in col)\n                for col in columns]\n\n# calculate the maximum length of strings in each column to allow enough space in the printout\nfor row in rows:\n    print (''.join(' {:{width}} '.format (\n        row [i], width = column_widths [i])\n                  for i in range (0, len (row))))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:29.130002Z","iopub.execute_input":"2023-12-31T15:42:29.130449Z","iopub.status.idle":"2023-12-31T15:42:33.223655Z","shell.execute_reply.started":"2023-12-31T15:42:29.130409Z","shell.execute_reply":"2023-12-31T15:42:33.222447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Identify all groups of nouns and the way they are realted to each other**","metadata":{}},{"cell_type":"code","source":"doc = nlp (\"On monday students meet with researchers \" + \" and discuss future development their research.\")\n\n# we can access noun phrases by doc.noun_chunks\nfor chunk in doc.noun_chunks:\n    # print out the phrase, its head, the type of relation to the next most important word, and the word itself\n    print ('\\t'.join ([chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text]))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.224884Z","iopub.execute_input":"2023-12-31T15:42:33.225728Z","iopub.status.idle":"2023-12-31T15:42:33.241558Z","shell.execute_reply.started":"2023-12-31T15:42:33.225692Z","shell.execute_reply":"2023-12-31T15:42:33.240336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the dependency information**","metadata":{}},{"cell_type":"code","source":"# import spaCy's visualization tool displaCy\nfrom spacy import displacy\n# path helps define the location for the file to store the visualization\nfrom pathlib import Path\n\n# use displaCy to visualize dependecies over the input text with approptiate arguments\nsvg = displacy.render (doc, style = 'dep', jupyter = False)\nfile_name = '-'.join ([w.text for w in doc if not w.is_punct]) + \".svg\"\n\n# the the output us stored to simply uses the words from the sentence in its name\noutput_path = Path (file_name)\noutput_path.open (\"w\", encoding=\"utf-8\").write(svg)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.24308Z","iopub.execute_input":"2023-12-31T15:42:33.243682Z","iopub.status.idle":"2023-12-31T15:42:33.259093Z","shell.execute_reply.started":"2023-12-31T15:42:33.243651Z","shell.execute_reply":"2023-12-31T15:42:33.257896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Print out the information about head and dependents for each word**","metadata":{}},{"cell_type":"code","source":"# coode assumes that spaCy is imported and input text is already fed into the pipeline\nfor token in doc:\n    print (token.text, token.dep_, token.head.text,\n          token.head.pos_, [child for child in token.children])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.260633Z","iopub.execute_input":"2023-12-31T15:42:33.261271Z","iopub.status.idle":"2023-12-31T15:42:33.268543Z","shell.execute_reply.started":"2023-12-31T15:42:33.261228Z","shell.execute_reply":"2023-12-31T15:42:33.267219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extarct participants of the actions**","metadata":{}},{"cell_type":"code","source":"# code assumes that spaCy is imported and input text is already fed into pipeline\nfor token in doc:\n    # check that the ROOT of the sentence is a verb with the base form (lemma) \"meet\"\n    if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\"\n       and token.dep_ == \"ROOT\"):\n        # this verb expresses the action itself\n        action = token.text\n        # extract the list of all dependents of this verb using token.children\n        children = [child for child in token.children]\n        participant1 = \"\"\n        participant2 = \"\"\n        for child1 in children:\n            if child1.dep_ == \"nsubj\":\n                participant1 = \" \".join (\n                [attr.text for attr in child1.children]\n                ) + \" \" + child1.text\n            elif child1.text == \"with\":\n                # check if the verb has preposition \"with\" as one of its dependents\n                action += \" \" + child1.text\n                child1_children = [child for child in child1.children]\n                for child2 in child1_children:\n                    if child2.pos_ == \"NOUN\":\n                        participant2 = \" \".join (\n                        [attr.text for attr in child2.children]\n                        ) + \" \" + child2.text\n                    \n# print out the results\nprint (f\"Participant1 = {participant1}\")\nprint (f\"Action = {action}\")\nprint (f\"Participant2 = {participant2}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.270408Z","iopub.execute_input":"2023-12-31T15:42:33.270937Z","iopub.status.idle":"2023-12-31T15:42:33.284611Z","shell.execute_reply.started":"2023-12-31T15:42:33.270907Z","shell.execute_reply":"2023-12-31T15:42:33.283444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build information extractor**","metadata":{}},{"cell_type":"code","source":"# provide diverse set of sentences\nsentences = [\"On monday students meet with researchers \" + \" and discuss future development their research.\", \n            \" Warren Baffet met with the President last week.\",\n            \"Elon Musk met with the President an White House.\",\n            \"The two bussinesmans also posed for photographs and \" + \n            \"the Vice President talked to reporters.\"]\n\n# define a function to apply all the steps in the information extraction algorithm\ndef extract_information (doc):\n    action = \"\"\n    participant1 = \"\"\n    for token in doc: \n         if (token.lemma_ == \"meet\" and token.pos_ == \"VERB\" \n            and token.dep_ == \"ROOT\"):\n                action = token.text\n                children = [child for child in token.children]\n                for child1 in children:\n                    if child1.dep_ == \"nsubj\": \n                        patricipant1 = \" \".join (\n                [attr.text for attr in child1.children]\n                ) + \" \" + child1.text\n                    elif child1.text == \"with\":\n                        action += \" \" + child1.text\n                        child1_children = [child for child in child1.children]\n                        for child2 in child1_children:\n                            # extract participants expressed with proper nouns (PROPN) and common nouns (NOUN)\n                            if (child2.pos_ == \"NOUN\"\n                            or child2.pos_ == \"PROPN\"):\n                                participant2 = \" \".join (\n                        [attr.text for attr in child2.children]\n                        ) + \" \" + child2.text\n                    elif (child1.dep_ == \"dobj\"\n                        and (child1.pos_ == \"NOUN\"\n                            or child1.pos_ == \"PROPN\")):\n                        participant2 = \" \".join (\n                            [attr.text for attr in child1.children]\n                            ) + \" \" + child1.text\n    \n        \n# apply extract_information function to each sentence and print out the actions and participants\nfor sent in sentences:\n    print (f\"\\nSentence = {sent}\")\n    doc = nlp (sent)\n    extract_information (doc)\n    print (f\"Participant1 = {participant1}\")\n    print (f\"Action = {action}\")\n    print (f\"Participant2 = {participant2}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.28623Z","iopub.execute_input":"2023-12-31T15:42:33.286747Z","iopub.status.idle":"2023-12-31T15:42:33.341638Z","shell.execute_reply.started":"2023-12-31T15:42:33.286705Z","shell.execute_reply":"2023-12-31T15:42:33.340748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to extract literary works from Project Gutenberg**","metadata":{}},{"cell_type":"code","source":"nltk.download ('gutenberg')\nfrom nltk.corpus import gutenberg\n\n# print out the names of files\ngutenberg.fileids ()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.342645Z","iopub.execute_input":"2023-12-31T15:42:33.343201Z","iopub.status.idle":"2023-12-31T15:42:33.44129Z","shell.execute_reply.started":"2023-12-31T15:42:33.343165Z","shell.execute_reply":"2023-12-31T15:42:33.44026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define training and test sets**","metadata":{}},{"cell_type":"code","source":"nltk.download ('punkt')\n\nauthor1_train = gutenberg.sents ('chesterton-ball.txt') + gutenberg.sents ('chesterton-brown.txt')\nprint (author1_train)\nprint (len (author1_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:33.443088Z","iopub.execute_input":"2023-12-31T15:42:33.443819Z","iopub.status.idle":"2023-12-31T15:42:34.027535Z","shell.execute_reply.started":"2023-12-31T15:42:33.443776Z","shell.execute_reply":"2023-12-31T15:42:34.026236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the test set with the sentences from the third work by the author\nauthor1_test = gutenberg.sents ('chesterton-thursday.txt')\nprint (author1_test)\nprint (len (author1_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:34.02894Z","iopub.execute_input":"2023-12-31T15:42:34.029412Z","iopub.status.idle":"2023-12-31T15:42:34.239938Z","shell.execute_reply.started":"2023-12-31T15:42:34.029369Z","shell.execute_reply":"2023-12-31T15:42:34.238796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author2_train = gutenberg.sents ('shakespeare-caesar.txt') + gutenberg.sents ('shakespeare-hamlet.txt')\nprint (author2_train)\nprint (len (author2_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:34.241274Z","iopub.execute_input":"2023-12-31T15:42:34.241611Z","iopub.status.idle":"2023-12-31T15:42:34.471234Z","shell.execute_reply.started":"2023-12-31T15:42:34.241581Z","shell.execute_reply":"2023-12-31T15:42:34.470095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"author2_test = gutenberg.sents ('shakespeare-macbeth.txt')\nprint (author2_test)\nprint (len (author2_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:34.472611Z","iopub.execute_input":"2023-12-31T15:42:34.473015Z","iopub.status.idle":"2023-12-31T15:42:34.562453Z","shell.execute_reply.started":"2023-12-31T15:42:34.472975Z","shell.execute_reply":"2023-12-31T15:42:34.561346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate simple statistics on texts**","metadata":{}},{"cell_type":"code","source":"def statistics (gutenberg_data):\n    for work in gutenberg_data:\n        # use NLTK's functionality to calculate statistics\n        num_chars = len (gutenberg.raw (work))\n        num_words = len (gutenberg.words (work))\n        num_sents = len (gutenberg.sents (work))\n        num_vocab = len (set (w.lower ()\n                             for w in gutenberg.words (work)))\n        print (round (num_chars / num_words),\n              round (num_words / num_sents),\n              round (num_words / num_vocab),\n              work)\n        \ngutenberg_data = ['chesterton-ball.txt','chesterton-brown.txt','chesterton-thursday.txt', \n                  'shakespeare-caesar.txt','shakespeare-hamlet.txt','shakespeare-macbeth.txt']\nstatistics (gutenberg_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:34.563947Z","iopub.execute_input":"2023-12-31T15:42:34.564548Z","iopub.status.idle":"2023-12-31T15:42:36.257623Z","shell.execute_reply.started":"2023-12-31T15:42:34.564505Z","shell.execute_reply":"2023-12-31T15:42:36.256602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run StratifiedShufflingSplit on the data**","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport random\nimport sklearn\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nall_sents = [(sent, 'chesterton') for sent in author1_train]\nall_sents += [(sent, 'shakespeare') for sent in author2_train]\n# combine all sentences into a single list called all_sents, keeping the author label\nprint (f\"Dataset size = {str (len (all_sents))} sentences\")\n\n# keep the set of labels (authors) as values\nvalues = [author for (sent, author) in all_sents]\nsplit = StratifiedShuffleSplit (n_splits = 1, test_size = 0.2, random_state = 42)\nstrat_train_set = []\nstrat_pretest_set = []\nfor train_index, pretest_index in split.split (all_sents, values):\n    strat_train_set= [all_sents [index] for index in train_index]\n    strat_pretest_set = [all_sents [index]\n                        for index in pretest_index]","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:36.258905Z","iopub.execute_input":"2023-12-31T15:42:36.259274Z","iopub.status.idle":"2023-12-31T15:42:37.041769Z","shell.execute_reply.started":"2023-12-31T15:42:36.259242Z","shell.execute_reply":"2023-12-31T15:42:37.040563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check the proportions of the data in the two classes**","metadata":{}},{"cell_type":"code","source":"# calculate the proportion of the entries in each class (category) in the given dataset data\ndef cat_proportions (data, cat):\n    count = 0\n    for item in data:\n        if item [1] == cat:\n            count += 1\n    return float (count) / float (len (data))\n\ncategories = ['chesterton', 'shakespeare']\nrows = []\nrows.append ([\"Category\", \"Overall\", \"Stratified train\", \"Stratified pretest\"])\n\nfor cat in categories:\n    rows.append ([cat, f\"{cat_proportions (all_sents, cat):.6f}\",\n                 f\"{cat_proportions (strat_train_set, cat):.6f}\",\n                 f\"{cat_proportions (strat_pretest_set, cat):.6f}\"])\n    \ncolumns = zip (*rows)\ncolumn_widths = [max (len (item) for item in col) for col in columns]\nfor row in rows:\n    print (''.join (' {:{width}} '.format (row [i], width = column_widths [i])\n                   for i in range (0, len (row))))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:37.049908Z","iopub.execute_input":"2023-12-31T15:42:37.050592Z","iopub.status.idle":"2023-12-31T15:42:37.071441Z","shell.execute_reply.started":"2023-12-31T15:42:37.050547Z","shell.execute_reply":"2023-12-31T15:42:37.070605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create the test_set data structure**","metadata":{}},{"cell_type":"code","source":"test_set = [(sent, \"chesterton\") for sent in author1_test]\ntest_set += [(sent, \"shakespeare\") for sent in author2_test]\n\n# extract words as features\ndef get_features (text):\n    features = {}\n    word_list = [word for word in text]\n    for word in word_list:\n        features [word] = True\n    return features\n\n# extract features from training and pretest sets\ntrain_features = [(get_features (sents), label)\n                 for (sents, label) in strat_train_set]\npretest_features = [(get_features (sents), label)\n                   for (sents, label) in strat_pretest_set]\n\n# run some checks to see what the data contains\nprint (len (train_features))\nprint (train_features [0] [0])\nprint (train_features [100] [0])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:37.072454Z","iopub.execute_input":"2023-12-31T15:42:37.073407Z","iopub.status.idle":"2023-12-31T15:42:37.803072Z","shell.execute_reply.started":"2023-12-31T15:42:37.073372Z","shell.execute_reply":"2023-12-31T15:42:37.801974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Naive Bayes classifier on train and test on pretest set**","metadata":{}},{"cell_type":"code","source":"# import the classifier\nfrom nltk import NaiveBayesClassifier, classify\n\n# train the classifier on the training set\nprint (f\"Training set size = {str (len (train_features))} sentences\")\nprint (f\"Pretest set size = {str (len (pretest_features))} sentences\")\nclassifier = NaiveBayesClassifier.train (train_features)\n\nprint (f\"Accuracy on the training set = {str (classify.accuracy (classifier, train_features))}\")\nprint (f\"Accuracy on the pretest set = \" + \n      f\"{str (classify.accuracy (classifier, pretest_features))}\")\nclassifier.show_most_informative_features (50)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:37.804722Z","iopub.execute_input":"2023-12-31T15:42:37.805364Z","iopub.status.idle":"2023-12-31T15:42:40.01691Z","shell.execute_reply.started":"2023-12-31T15:42:37.805325Z","shell.execute_reply":"2023-12-31T15:42:40.016089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to extract words and sentence length statistics**","metadata":{}},{"cell_type":"code","source":"def avg_number_chars (text):\n    total_chars = 0.0\n    for word in text:\n        total_chars += len (word)\n    return float (total_chars) / float (len(text))\n\n# calculate the sentence length in terms of the number of words\ndef number_words (text):\n    return float (len (text))\n\nprint (avg_number_chars ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))\nprint (number_words ([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:40.018181Z","iopub.execute_input":"2023-12-31T15:42:40.018503Z","iopub.status.idle":"2023-12-31T15:42:40.026602Z","shell.execute_reply.started":"2023-12-31T15:42:40.018474Z","shell.execute_reply":"2023-12-31T15:42:40.025459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code to extract features and map them to the labels**","metadata":{}},{"cell_type":"code","source":"# argument source denotes the dataset we are applying the feature extraction\ndef initialize_dataset (source):\n    all_features = []\n    targets = []\n    # iterate through all (sent, label) pairs in the given dataset\n    for (sent, label) in source:\n        feature_list = []\n        feature_list.append (avg_number_chars (sent))\n        feature_list.append (number_words (sent))\n        all_features.append (feature_list)\n        if label == \"chesterton\": targets.append (0)\n        else: targets.append (1)\n    return all_features, targets\n\ntrain_data, train_targets = initialize_dataset (strat_train_set)\npretest_data, pretest_targets = initialize_dataset (strat_pretest_set)\ntest_data, test_targets = initialize_dataset (test_set)\n\n# print out thr length of the structures\nprint (len (train_data), len (train_targets))\nprint (len (pretest_data), len (pretest_targets))\nprint (len (test_data), len (test_targets))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:40.02806Z","iopub.execute_input":"2023-12-31T15:42:40.028354Z","iopub.status.idle":"2023-12-31T15:42:40.112665Z","shell.execute_reply.started":"2023-12-31T15:42:40.028328Z","shell.execute_reply":"2023-12-31T15:42:40.111478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train and test a classifier with sklearn**","metadata":{}},{"cell_type":"code","source":"# import decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# initialization\ntext_clf = DecisionTreeClassifier (random_state = 42)\n\n# train the classifier using the fit method\ntext_clf.fit (train_data, train_targets)\n\n# test the classifier using the predict method\npredicted = text_clf.predict (pretest_data)\n\n# evaluating the classifier\n# import numpy and sklearn's metrics funcvtionality\nimport numpy as np\nfrom sklearn import metrics\n\ndef evaluate (predicted, targets):\n    # use numpy.mean to estimate the accuracy of the classifier\n    print (np.mean (predicted == targets))\n    print (metrics.confusion_matrix (targets, predicted))\n    print (metrics.classification_report (targets, predicted))\n    \nevaluate (predicted, pretest_targets)\n\n# apply the same routine to the test set\npredicted = text_clf.predict (test_data)\nevaluate (predicted, test_targets)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:40.113956Z","iopub.execute_input":"2023-12-31T15:42:40.114311Z","iopub.status.idle":"2023-12-31T15:42:40.283439Z","shell.execute_reply.started":"2023-12-31T15:42:40.114283Z","shell.execute_reply":"2023-12-31T15:42:40.282363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate the number and proportion of times certain words occur**","metadata":{}},{"cell_type":"code","source":"def word_counts(text):\n    counts = {}\n    for word in text:\n        counts[word.lower()] = counts.get(word.lower(), 0) + 1\n    return counts\n\ndef proportion_words(text, wordlist):\n    count = 0\n    for word in text:\n        if word.lower() in wordlist:\n            count += 1\n    return float(count)/float(len(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:40.284577Z","iopub.execute_input":"2023-12-31T15:42:40.284967Z","iopub.status.idle":"2023-12-31T15:42:40.291034Z","shell.execute_reply.started":"2023-12-31T15:42:40.284928Z","shell.execute_reply":"2023-12-31T15:42:40.290013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding stopword counts and proportion as features**","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# add spaCy’s functionality to the code and upload the stopwords list\nnlp = spacy.load('en_core_web_lg')\ndef initialize_dataset(source):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list=[]\n        feature_list.append(avg_number_chars(sent))\n        feature_list.append(number_words(sent))\n        counts = word_counts(sent)\n        for word in STOP_WORDS:\n            if word in counts.keys():\n                feature_list.append(counts.get(word))\n            else:\n                feature_list.append(0)\n        feature_list.append(proportion_words(sent, STOP_WORDS))\n        all_features.append(feature_list)\n        if label==\"austen\": targets.append(0)\n        else: targets.append(1)\n    return all_features, targets\n\ntrain_data, train_targets = initialize_dataset(strat_train_set)\npretest_data, pretest_targets = initialize_dataset(strat_pretest_set)\ntest_data, test_targets = initialize_dataset(test_set)\n\n# Print out the length of the feature lists and targets lists\nprint (len(train_data), len(train_targets))\nprint (len(pretest_data), len(pretest_targets))\nprint (len(test_data), len(test_targets))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:40.292629Z","iopub.execute_input":"2023-12-31T15:42:40.293014Z","iopub.status.idle":"2023-12-31T15:42:46.468761Z","shell.execute_reply.started":"2023-12-31T15:42:40.292976Z","shell.execute_reply":"2023-12-31T15:42:46.46733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate the results**","metadata":{}},{"cell_type":"code","source":"# train the classifier on the training data\ntext_clf = DecisionTreeClassifier(random_state=42)\ntext_clf.fit(train_data, train_targets)\n\n# test on the pretest set\npredicted = text_clf.predict(pretest_data)\nevaluate(predicted, pretest_targets)\n\n# apply the same routine to the test set\npredicted = text_clf.predict(test_data)\nevaluate(predicted, test_targets)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:46.470676Z","iopub.execute_input":"2023-12-31T15:42:46.471144Z","iopub.status.idle":"2023-12-31T15:42:46.90046Z","shell.execute_reply.started":"2023-12-31T15:42:46.471101Z","shell.execute_reply":"2023-12-31T15:42:46.899244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying spaCy preprocessing**","metadata":{}},{"cell_type":"code","source":"# provide the preprocess function with the original sentences from the datasets\ndef preprocess(source):\n    source_docs = {}\n    index = 0\n    for (sent, label) in source:\n        text = \" \".join(sent)\n        source_docs[text] = nlp(text)\n        if index>0 and (index%2000)==0:\n            print(str(index) + \" texts processed\")\n        index += 1\n    print(\"Dataset processed\")\n    return source_docs\n\n# apply the preprocess function to the three original datasets\ntrain_docs = preprocess(strat_train_set)\npretest_docs = preprocess(strat_pretest_set)\ntest_docs = preprocess(test_set)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:42:46.901668Z","iopub.execute_input":"2023-12-31T15:42:46.901976Z","iopub.status.idle":"2023-12-31T15:45:42.364213Z","shell.execute_reply.started":"2023-12-31T15:42:46.901949Z","shell.execute_reply":"2023-12-31T15:45:42.363099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding distribution of part-of-speech tags as features**","metadata":{}},{"cell_type":"code","source":"# import Python’s Counter functionality to simplify counting procedures\nfrom collections import Counter\npos_list = [\"C\", \"D\", \"E\", \"F\", \"I\", \"J\", \"M\",\n            \"N\", \"P\", \"R\", \"T\", \"U\", \"V\", \"W\"]\n\ndef pos_counts(text, source_docs, pos_list):\n    pos_counts = {}\n    doc = source_docs.get(\" \".join(text))\n    tags = []\n    for word in doc:\n        tags.append(str(word.tag_)[0])\n    counts = Counter(tags)\n    for pos in pos_list:\n        if pos in counts.keys():\n            pos_counts[pos] = counts.get(pos)\n        # Populate the pos_counts dictionary using the counts\n        # of the part-of-speech tags or inserting 0    \n        else: pos_counts[pos] = 0\n    return pos_counts\n\ndef initialize_dataset(source, source_docs):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list=[]\n        feature_list.append(avg_number_chars(sent))\n        feature_list.append(number_words(sent))\n        counts = word_counts(sent)\n        for word in STOP_WORDS:\n            if word in counts.keys():\n                feature_list.append(counts.get(word))\n            else:\n                feature_list.append(0)\n        feature_list.append(proportion_words(sent, STOP_WORDS))\n        # extract the previous 308 features as before\n        p_counts = pos_counts(sent, source_docs, pos_list)\n        for pos in p_counts.keys():\n            feature_list.append(float(p_counts.get(pos))/float(len(sent)))\n        all_features.append(feature_list)\n        if label==\"austen\": targets.append(0)\n        else: targets.append(1)\n    return all_features, targets","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:45:42.366074Z","iopub.execute_input":"2023-12-31T15:45:42.366836Z","iopub.status.idle":"2023-12-31T15:45:42.378537Z","shell.execute_reply.started":"2023-12-31T15:45:42.366796Z","shell.execute_reply":"2023-12-31T15:45:42.377493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Run the train-test-evaluate routine**","metadata":{}},{"cell_type":"code","source":"def run():\n    train_data, train_targets = initialize_dataset(strat_train_set, train_docs)\n    pretest_data, pretest_targets = initialize_dataset(strat_pretest_set, \n                                                       pretest_docs)\n    test_data, test_targets = initialize_dataset(test_set, test_docs)\n    print (len(train_data), len(train_targets))\n    print (len(pretest_data), len(pretest_targets))\n    print (len(test_data), len(test_targets))\n    print ()\n    text_clf = DecisionTreeClassifier(random_state=42)\n    text_clf.fit(train_data, train_targets)\n    predicted = text_clf.predict(pretest_data)\n    evaluate(predicted, pretest_targets)\n    predicted = text_clf.predict(test_data)\n    evaluate(predicted, test_targets)\n    \nrun()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:45:42.379715Z","iopub.execute_input":"2023-12-31T15:45:42.380168Z","iopub.status.idle":"2023-12-31T15:45:45.050107Z","shell.execute_reply.started":"2023-12-31T15:45:42.380137Z","shell.execute_reply":"2023-12-31T15:45:45.049077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Collecting the most frequent suffixes from the data**","metadata":{}},{"cell_type":"code","source":"# import python operator functionality\nimport operator\n\n# create the function\ndef select_suffixes (cutoff):\n    all_suffixes = []\n    # iterate through the list of values in the train_docs.values ()\n    for doc in train_docs.values ():\n        for word in doc:\n            all_suffixes.append (str (word.suffix_).lower ())\n    counts = Counter (all_suffixes)\n    # store the frequency of all the suffixes in the counts dictionary and the sort it\n    sorted_counts = sorted (counts.items (), key = operator.itemgetter (1),\n                           reverse = True)\n    selected_suffixes = []\n    for i in range (0, round (len (counts)*cutoff)):\n        selected_suffixes.append (sorted_counts [i][0])\n    return selected_suffixes\n\nselected_suffixes = select_suffixes (0.4)\nprint (len (selected_suffixes))\nprint (selected_suffixes)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:45:45.051673Z","iopub.execute_input":"2023-12-31T15:45:45.052643Z","iopub.status.idle":"2023-12-31T15:45:45.227778Z","shell.execute_reply.started":"2023-12-31T15:45:45.052608Z","shell.execute_reply":"2023-12-31T15:45:45.226784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Add new, suffix-based features**","metadata":{}},{"cell_type":"code","source":"# create function that returns the counts of suffixes from suffix_list in the given sentence (text)\ndef suffix_counts (text, source_docs, suffix_list):\n    suffix_counts = {}\n    doc = source_docs.get (\" \".join (text))\n    suffixes = []\n    for word in doc:\n        suffixes.append (str (word.suffix_))\n    counts = Counter (suffixes)\n    for suffix in suffix_list:\n        if suffix in counts.keys ():\n            suffix_counts [suffix] = counts.get(suffix)\n        else: suffix_counts [suffix] = 0\n    return suffix_counts\n\ndef initialize_dataset (source, source_docs):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list = []\n        feature_list.append (avg_number_chars (sent))\n        feature_list.append (number_words (sent))\n        counts = word_counts (sent)\n        for word in STOP_WORDS:\n            if word in counts.keys ():\n                feature_list.append (counts.get (word))\n            else:\n                feature_list.append (0)\n        feature_list.append (proportion_words (sent, STOP_WORDS))\n        p_counts = pos_counts (sent, source_docs, pos_list)\n        for pos in p_counts.keys ():\n            feature_list.append (float (p_counts.get (pos))/float (len (sent)))\n        s_counts = suffix_counts (sent, source_docs, selected_suffixes)\n        for suffix in s_counts.keys ():\n            # append the new 690 suffix distribution features by calculating the proportion\n            # of words containing the suffixes\n            feature_list.append (float (s_counts.get (suffix))/float (len (sent)))\n            \n        all_features.append (feature_list)\n        if label == 'chesterton': targets.append (0)\n        else: targets.append (1)\n    return all_features, targets\n\n# apply the train-test-evaluate routine\nrun ()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:45:45.229312Z","iopub.execute_input":"2023-12-31T15:45:45.229692Z","iopub.status.idle":"2023-12-31T15:46:04.691793Z","shell.execute_reply.started":"2023-12-31T15:45:45.229662Z","shell.execute_reply":"2023-12-31T15:46:04.690406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Collect 50% most frequent unique words per author**","metadata":{}},{"cell_type":"code","source":"# function for collect full vocabularies for each author\ndef unique_vocabulary (label1, label2, cutoff):\n    voc1 = []\n    voc2 = []\n    for (sent, label) in strat_train_set:\n        if label == label1:\n            for word in sent:\n                voc1.append (word.lower ())\n        elif label == label2:\n            for word in sent:\n                voc2.append (word.lower ())\n    counts1 = Counter (voc1)\n    sorted_counts1 = sorted (counts1.items (), key = operator.itemgetter (1),\n                             reverse = True)\n    counts2 = Counter (voc2)\n    sorted_counts2 = sorted (counts2.items (), key = operator.itemgetter (1),\n                            reverse = True)\n    unique_voc = []\n    # the unique_voc list stores the most frequent words for each author\n    # if they are never used by the other author\n    for i in range (0, round (len (sorted_counts1)*cutoff)):\n        if not sorted_counts1[i] [0] in counts2.keys ():\n            unique_voc.append (sorted_counts1[i] [0])\n    for i in range (0, round (len (sorted_counts2)*cutoff)):\n        if not sorted_counts2 [i] [0] in counts1.keys ():\n            unique_voc.append (sorted_counts2 [i] [0])\n    return unique_voc\n\n# print out the unique_voc list using 50% as the cutoff\nunique_voc = unique_vocabulary (\"chesterton\", \"shakespeare\", 0.5)\nprint (len (unique_voc))\nprint (unique_voc)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:46:04.692755Z","iopub.execute_input":"2023-12-31T15:46:04.693336Z","iopub.status.idle":"2023-12-31T15:46:04.795286Z","shell.execute_reply.started":"2023-12-31T15:46:04.693308Z","shell.execute_reply":"2023-12-31T15:46:04.794367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding new word-based features, then train and test the classifier**","metadata":{}},{"cell_type":"code","source":"# the unique_counts function returns the counts of unique words from the unique_voc list\n# in the given sentence\n\ndef unique_counts (text, unique_voc):\n    unique_counts = {}\n    words = []\n    for word in text:\n        words.append (word.lower ())\n    counts = Counter (words)\n    for word in unique_voc:\n        # for each word from the unique_voc, use its count in the sentence\n        # if it occurs or 0 otherwise\n        if word in counts.keys ():\n            unique_counts [word] = counts.get (word)\n        else: unique_counts [word] = 0\n    return unique_counts\n\ndef initialize_dataset (source, source_docs):\n    all_features = []\n    targets = []\n    for (sent, label) in source:\n        feature_list = []\n        feature_list.append (avg_number_chars (sent))\n        feature_list.append (number_words (sent))\n        counts = word_counts (sent)\n        for word in STOP_WORDS:\n            if word in counts.keys ():\n                feature_list.append (counts.get (word))\n            else:\n                feature_list.append (0)\n        feature_list.append (proportion_words (sent, STOP_WORDS))\n        p_counts = pos_counts (sent, source_docs, pos_list)\n        for pos in p_counts.keys ():\n            feature_list.append (float (p_counts.get (pos))/float (len (sent)))\n        s_counts = suffix_counts (sent, source_docs, selected_suffixes)\n        for suffix in s_counts.keys ():\n            # add the previous 690 features as before\n            feature_list.append (float (s_counts.get (suffix))/float (len (sent)))\n        u_counts = unique_counts (sent, unique_voc)\n        # add the new 5253 unique word counts features\n        for word in u_counts.keys ():\n            feature_list.append (u_counts.get (word))\n        all_features.append (feature_list)\n        if label == 'chesterton': targets.append (0)\n        else: targets.append (1)\n    return all_features, targets\n\n# apply the train-test-evaluate routine\nrun ()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T15:46:04.797212Z","iopub.execute_input":"2023-12-31T15:46:04.798144Z","iopub.status.idle":"2023-12-31T15:47:44.053696Z","shell.execute_reply.started":"2023-12-31T15:46:04.798093Z","shell.execute_reply":"2023-12-31T15:47:44.052616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating of topic analyzer**","metadata":{}},{"cell_type":"code","source":"# import sklearn's functionality\nfrom sklearn.datasets import fetch_20newsgroups\n\n# define the load_dataset function to return the data extracted\n# according to predefined restrictions\ndef load_dataset(sset, cats):\n    if cats==[]:\n        newsgroups_dset = fetch_20newsgroups(subset=sset,\n                                             remove=('headers', 'footers', 'quotes'),\n                                             shuffle=True)\n    else:\n        newsgroups_dset = fetch_20newsgroups(subset=sset, categories=cats,\n                                             remove=('headers', 'footers', 'quotes'),\n                                             shuffle=True)\n    return newsgroups_dset\n\ncategories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\"]\ncategories += [\"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"]\ncategories += [\"sci.crypt\", \"sci.med\", \"sci.space\"]\ncategories += [\"talk.politics.mideast\"]\n\n# to access both training and test sets, use \"all\" as the first argument\nnewsgroups_all = load_dataset('all', categories)\nprint(len(newsgroups_all.data))","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:08:35.712096Z","iopub.execute_input":"2024-01-06T18:08:35.712517Z","iopub.status.idle":"2024-01-06T18:08:50.689894Z","shell.execute_reply.started":"2024-01-06T18:08:35.712485Z","shell.execute_reply":"2024-01-06T18:08:50.688874Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"9850\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Preprocess data using NLTK and gensim**","metadata":{}},{"cell_type":"code","source":"import nltk\nimport gensim\nfrom nltk.stem import SnowballStemmer\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing \\\nimport STOPWORDS as stopwords\nstemmer = SnowballStemmer(\"english\")\n\ndef stem(text):\n    return stemmer.stem(text)\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text, min_len=4):\n        if token not in stopwords:\n            result.append(stem(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:08:52.915849Z","iopub.execute_input":"2024-01-06T18:08:52.916294Z","iopub.status.idle":"2024-01-06T18:08:52.923561Z","shell.execute_reply.started":"2024-01-06T18:08:52.916261Z","shell.execute_reply":"2024-01-06T18:08:52.922437Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Inspecting the results of the preprocessing step**","metadata":{}},{"cell_type":"code","source":"doc_sample = newsgroups_all.data [0]\nprint ('Original document: ')\nprint (doc_sample)\n\nprint ('\\n\\nTokenized document: ')\nwords = []\nfor token in gensim.utils.tokenize (doc_sample):\n    words.append (token)\nprint (words)\n\n# check the output of the preprocess function\nprint ('\\n\\nPreprocessed document:  ')\nprint (preprocess (doc_sample))","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:08:55.193603Z","iopub.execute_input":"2024-01-06T18:08:55.194058Z","iopub.status.idle":"2024-01-06T18:08:55.201324Z","shell.execute_reply.started":"2024-01-06T18:08:55.194022Z","shell.execute_reply":"2024-01-06T18:08:55.200324Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Original document: \nHi Xperts!\n\nHow can I move the cursor with the keyboard (i.e. cursor keys), \nif no mouse is available?\n\nAny hints welcome.\n\nThanks.\n\n\nTokenized document: \n['Hi', 'Xperts', 'How', 'can', 'I', 'move', 'the', 'cursor', 'with', 'the', 'keyboard', 'i', 'e', 'cursor', 'keys', 'if', 'no', 'mouse', 'is', 'available', 'Any', 'hints', 'welcome', 'Thanks']\n\n\nPreprocessed document:  \n['xpert', 'cursor', 'keyboard', 'cursor', 'key', 'mous', 'avail', 'hint', 'welcom', 'thank']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Inspect the processing output for a group of documents**","metadata":{}},{"cell_type":"code","source":"# iterate through the documents, such as through the list of the first 10 ones\nfor i in range (0, 15):\n    print (str(i) + \"\\t\" + \", \".join (preprocess (newsgroups_all.data[i]) [:15]))","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:08:57.549707Z","iopub.execute_input":"2024-01-06T18:08:57.55012Z","iopub.status.idle":"2024-01-06T18:08:57.579979Z","shell.execute_reply.started":"2024-01-06T18:08:57.550088Z","shell.execute_reply":"2024-01-06T18:08:57.579071Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0\txpert, cursor, keyboard, cursor, key, mous, avail, hint, welcom, thank\n1\tobtain, copi, open, look, widget, obtain, need, order, copi, thank, help, email\n2\tright, signal, strong, live, west, philadelphia, perfect, sport, fan, dream, especi, person, want, hear, team\n3\tcanadian, thing, coach, boston, bruin, colorado, rocki, summari, post, gather, ongo, beef, year, convent, wisdom\n4\theck, feel, like, time, includ, cafeteria, work, half, time, headach, intensifi, away, throw, imagin, guess\n5\tdamn, right, late, climb, meet, morn, bother, right, foot, asleep, remind, fold, underneath, crunch, metatars\n6\tolympus, stylus, pocket, camera, smallest, class, includ, time, date, stamp, batteri, carri, case, reduct, timer\n7\tinclud, follow, chmos, clock, generat, driver, processor, chmos, eras, prom, power, chmos, dynam, chmos, programm\n8\tchang, intel, discov, xclient, xload, longer, work, bomb, messag, error, open, display, unix, correct, share\n9\ttermin, like, power, server, run, window, manag, special, client, program, call, window, manag, motif, openlook\n10\tworld, greatest, expert, chigger, type, mite, indigen, south, certain, spent, time, contempl, littl, bugger, past\n11\tpost, question, color, motif, widget, work, fine, xdefault, file, work, fine, normal, case, thank\n12\tthank, post, refer, normal, read, post, request, refer, articl, cross, post, cultur, jewish, alleg, jewish\n13\tgoali, refer, clint, malarchuk, time, play, sabr, team, immedi, prior, washington, capit, recov, continu, play\n14\thappen, avail, form, year, heard, talk\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Converitng word content of the documents into a dictionary**","metadata":{}},{"cell_type":"code","source":"processed_docs = []\nfor i in range (0, len (newsgroups_all.data)):\n    processed_docs.append (preprocess (newsgroups_all.data [i]))\nprint (len (processed_docs))\n\ndictionary = gensim.corpora.Dictionary (processed_docs)\nprint (len (dictionary))\n\n# check what is stored in this dictionary (iterate through the first 10 items)\nindex = 0\nfor key, value in dictionary.iteritems ():\n    print (key, value)\n    index += 1\n    if index > 15:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:08:59.677573Z","iopub.execute_input":"2024-01-06T18:08:59.678237Z","iopub.status.idle":"2024-01-06T18:09:23.028841Z","shell.execute_reply.started":"2024-01-06T18:08:59.678202Z","shell.execute_reply":"2024-01-06T18:09:23.027615Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"9850\n39350\n0 avail\n1 cursor\n2 hint\n3 key\n4 keyboard\n5 mous\n6 thank\n7 welcom\n8 xpert\n9 copi\n10 email\n11 help\n12 look\n13 need\n14 obtain\n15 open\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Performing further dimenisionality reduction on the documents**","metadata":{}},{"cell_type":"code","source":"# convert each document in the collection into a list of tuples and check the output\ndictionary.filter_extremes (no_below = 10, no_above = 0.5, keep_n = 10000)\nprint (len (dictionary))\n\nbow_corpus = [dictionary.doc2bow (doc) for doc in processed_docs]\nprint (bow_corpus [0])","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:11:53.61768Z","iopub.execute_input":"2024-01-06T18:11:53.618102Z","iopub.status.idle":"2024-01-06T18:11:54.265902Z","shell.execute_reply.started":"2024-01-06T18:11:53.618073Z","shell.execute_reply":"2024-01-06T18:11:54.265136Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"5868\n[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Check word stems behind IDs from the dictionary**","metadata":{}},{"cell_type":"code","source":"# extract a particular document from bow_corpus\nbow_doc = bow_corpus [0]\n\n# print out the IDs, corresponding word stems, and the number of occurences of these word stems\nfor i in range (len (bow_doc)):\n    print (f\"Key {bow_doc [i] [0]} =\\\"{dictionary [bow_doc [i] [0]]}\\\":\\\n    occurences = {bow_doc [i] [1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:11:55.858525Z","iopub.execute_input":"2024-01-06T18:11:55.859025Z","iopub.status.idle":"2024-01-06T18:11:55.866913Z","shell.execute_reply.started":"2024-01-06T18:11:55.85899Z","shell.execute_reply":"2024-01-06T18:11:55.865683Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Key 0 =\"avail\":    occurences = 1\nKey 1 =\"cursor\":    occurences = 2\nKey 2 =\"hint\":    occurences = 1\nKey 3 =\"key\":    occurences = 1\nKey 4 =\"keyboard\":    occurences = 1\nKey 5 =\"mous\":    occurences = 1\nKey 6 =\"thank\":    occurences = 1\nKey 7 =\"welcom\":    occurences = 1\nKey 8 =\"xpert\":    occurences = 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Run the LDA algorithm on our documents**","metadata":{}},{"cell_type":"code","source":"# initialize id2word to the dictionary where each word stem is mapped to a unique ID\nid2word = dictionary\n\n# initialize corpus to the bow_corpus  \ncorpus = bow_corpus\n\n# create the algorithm\nlda_model = gensim.models.ldamodel.LdaModel (corpus = corpus, id2word = id2word,\n                                            num_topics = 10, random_state = 100,\n                                            update_every = 1, chunksize = 1000,\n                                            passes = 10, alpha = \"symmetric\",\n                                            iterations = 100, per_word_topics = True)\n\n# output all topics and for each of them print out its index and the most informative words identified\nfor index, topic in lda_model.print_topics (-1):\n    print (f\"Topic: {index} \\nWords: {topic}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:11:58.426834Z","iopub.execute_input":"2024-01-06T18:11:58.427279Z","iopub.status.idle":"2024-01-06T18:13:01.72494Z","shell.execute_reply.started":"2024-01-06T18:11:58.427245Z","shell.execute_reply":"2024-01-06T18:13:01.723889Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Topic: 0 \nWords: 0.021*\"encrypt\" + 0.018*\"secur\" + 0.018*\"chip\" + 0.016*\"govern\" + 0.013*\"clipper\" + 0.012*\"public\" + 0.010*\"privaci\" + 0.010*\"key\" + 0.010*\"phone\" + 0.009*\"algorithm\"\nTopic: 1 \nWords: 0.017*\"appear\" + 0.014*\"copi\" + 0.013*\"cover\" + 0.013*\"star\" + 0.013*\"book\" + 0.011*\"penalti\" + 0.010*\"black\" + 0.009*\"comic\" + 0.008*\"blue\" + 0.008*\"green\"\nTopic: 2 \nWords: 0.031*\"window\" + 0.015*\"server\" + 0.012*\"program\" + 0.012*\"file\" + 0.012*\"applic\" + 0.012*\"display\" + 0.011*\"widget\" + 0.010*\"version\" + 0.010*\"motif\" + 0.010*\"support\"\nTopic: 3 \nWords: 0.015*\"space\" + 0.007*\"launch\" + 0.007*\"year\" + 0.007*\"medic\" + 0.006*\"patient\" + 0.006*\"orbit\" + 0.006*\"research\" + 0.006*\"diseas\" + 0.005*\"develop\" + 0.005*\"nasa\"\nTopic: 4 \nWords: 0.018*\"armenian\" + 0.011*\"peopl\" + 0.008*\"kill\" + 0.008*\"said\" + 0.007*\"turkish\" + 0.006*\"muslim\" + 0.006*\"jew\" + 0.006*\"govern\" + 0.005*\"state\" + 0.005*\"greek\"\nTopic: 5 \nWords: 0.024*\"price\" + 0.021*\"sale\" + 0.020*\"offer\" + 0.017*\"drive\" + 0.017*\"sell\" + 0.016*\"includ\" + 0.013*\"ship\" + 0.013*\"interest\" + 0.011*\"ask\" + 0.010*\"condit\"\nTopic: 6 \nWords: 0.018*\"mail\" + 0.016*\"list\" + 0.015*\"file\" + 0.015*\"inform\" + 0.013*\"send\" + 0.012*\"post\" + 0.012*\"avail\" + 0.010*\"request\" + 0.010*\"program\" + 0.009*\"includ\"\nTopic: 7 \nWords: 0.019*\"like\" + 0.016*\"know\" + 0.011*\"time\" + 0.011*\"look\" + 0.010*\"think\" + 0.008*\"want\" + 0.008*\"thing\" + 0.008*\"good\" + 0.007*\"go\" + 0.007*\"bike\"\nTopic: 8 \nWords: 0.033*\"game\" + 0.022*\"team\" + 0.017*\"play\" + 0.015*\"year\" + 0.013*\"player\" + 0.011*\"season\" + 0.008*\"hockey\" + 0.008*\"score\" + 0.007*\"leagu\" + 0.007*\"goal\"\nTopic: 9 \nWords: 0.013*\"peopl\" + 0.012*\"think\" + 0.011*\"like\" + 0.009*\"time\" + 0.009*\"right\" + 0.009*\"israel\" + 0.009*\"know\" + 0.006*\"reason\" + 0.006*\"point\" + 0.006*\"thing\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Identify the main topic for each document in the collection**","metadata":{}},{"cell_type":"code","source":"# fucntion takes as input the LDA model, corpus, and the original collection of texts\ndef analyse_topics(ldamodel, corpus, texts):\n    main_topic = {}\n    percentage = {}\n    keywords = {}\n    text_snippets = {}\n    for i, topic_list in enumerate(ldamodel[corpus]):\n        topic = topic_list[0]\n        topic = sorted(topic, key=lambda x: (x[1]), reverse=True)\n    for j, (topic_num, prop_topic) in enumerate(topic):\n        if j == 0:\n            wp = ldamodel.show_topic(topic_num)\n            topic_keywords = \", \".join([word for word, prop in wp[:5]])\n            main_topic[i] = int(topic_num)\n            percentage[i] = round(prop_topic,4)\n            keywords[i] = topic_keywords\n            text_snippets[i] = texts[i][:8]\n        else:\n            break\n    return main_topic, percentage, keywords, text_snippets\n\nmain_topic, percentage, keywords, text_snippets = analyse_topics(\n    lda_model, bow_corpus, processed_docs)","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:13:04.825269Z","iopub.execute_input":"2024-01-06T18:13:04.825715Z","iopub.status.idle":"2024-01-06T18:13:29.515293Z","shell.execute_reply.started":"2024-01-06T18:13:04.82568Z","shell.execute_reply":"2024-01-06T18:13:29.514364Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Print out the main topic for each document in the collection**","metadata":{}},{"cell_type":"code","source":"indexes = []\nrows = []\nfor i in range(0, 10):\n    indexes.append(i)\n    rows.append(['ID', 'Main Topic', 'Contribution (%)', 'Keywords', 'Snippet'])\n    \nfor idx in indexes:\n    rows.append([str(idx), f\"{main_topic.get(idx)}\",\n                 f\"{percentage.get(idx)}:.4f\",\n                 f\"{keywords.get(idx)}\\n\",\n                 f\"{text_snippets.get(idx)}\"])\ncolumns = zip(*rows)\ncolumn_widths = [max(len(item) for item in col) for col in columns]\nfor row in rows:\n    print(''.join(' width{} .'.format(row[i], width=column_widths[i])\n                  for i in range(0, len(row))))","metadata":{"execution":{"iopub.status.busy":"2024-01-06T18:44:15.354414Z","iopub.execute_input":"2024-01-06T18:44:15.355199Z","iopub.status.idle":"2024-01-06T18:44:15.364659Z","shell.execute_reply.started":"2024-01-06T18:44:15.35516Z","shell.execute_reply":"2024-01-06T18:44:15.363651Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":" widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n widthID . widthMain Topic . widthContribution (%) . widthKeywords . widthSnippet .\n width0 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width1 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width2 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width3 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width4 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width5 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width6 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width7 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width8 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n width9 . widthNone . widthNone:.4f . widthNone\n . widthNone .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Visualize the output of LDA using pyLDAvis**","metadata":{}},{"cell_type":"code","source":"pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","metadata":{"execution":{"iopub.status.busy":"2024-01-06T19:11:55.414944Z","iopub.execute_input":"2024-01-06T19:11:55.416269Z","iopub.status.idle":"2024-01-06T19:11:55.425523Z","shell.execute_reply.started":"2024-01-06T19:11:55.416229Z","shell.execute_reply":"2024-01-06T19:11:55.424335Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[(0,\n  '0.021*\"encrypt\" + 0.018*\"secur\" + 0.018*\"chip\" + 0.016*\"govern\" + '\n  '0.013*\"clipper\" + 0.012*\"public\" + 0.010*\"privaci\" + 0.010*\"key\" + '\n  '0.010*\"phone\" + 0.009*\"algorithm\"'),\n (1,\n  '0.017*\"appear\" + 0.014*\"copi\" + 0.013*\"cover\" + 0.013*\"star\" + 0.013*\"book\" '\n  '+ 0.011*\"penalti\" + 0.010*\"black\" + 0.009*\"comic\" + 0.008*\"blue\" + '\n  '0.008*\"green\"'),\n (2,\n  '0.031*\"window\" + 0.015*\"server\" + 0.012*\"program\" + 0.012*\"file\" + '\n  '0.012*\"applic\" + 0.012*\"display\" + 0.011*\"widget\" + 0.010*\"version\" + '\n  '0.010*\"motif\" + 0.010*\"support\"'),\n (3,\n  '0.015*\"space\" + 0.007*\"launch\" + 0.007*\"year\" + 0.007*\"medic\" + '\n  '0.006*\"patient\" + 0.006*\"orbit\" + 0.006*\"research\" + 0.006*\"diseas\" + '\n  '0.005*\"develop\" + 0.005*\"nasa\"'),\n (4,\n  '0.018*\"armenian\" + 0.011*\"peopl\" + 0.008*\"kill\" + 0.008*\"said\" + '\n  '0.007*\"turkish\" + 0.006*\"muslim\" + 0.006*\"jew\" + 0.006*\"govern\" + '\n  '0.005*\"state\" + 0.005*\"greek\"'),\n (5,\n  '0.024*\"price\" + 0.021*\"sale\" + 0.020*\"offer\" + 0.017*\"drive\" + 0.017*\"sell\" '\n  '+ 0.016*\"includ\" + 0.013*\"ship\" + 0.013*\"interest\" + 0.011*\"ask\" + '\n  '0.010*\"condit\"'),\n (6,\n  '0.018*\"mail\" + 0.016*\"list\" + 0.015*\"file\" + 0.015*\"inform\" + 0.013*\"send\" '\n  '+ 0.012*\"post\" + 0.012*\"avail\" + 0.010*\"request\" + 0.010*\"program\" + '\n  '0.009*\"includ\"'),\n (7,\n  '0.019*\"like\" + 0.016*\"know\" + 0.011*\"time\" + 0.011*\"look\" + 0.010*\"think\" + '\n  '0.008*\"want\" + 0.008*\"thing\" + 0.008*\"good\" + 0.007*\"go\" + 0.007*\"bike\"'),\n (8,\n  '0.033*\"game\" + 0.022*\"team\" + 0.017*\"play\" + 0.015*\"year\" + 0.013*\"player\" '\n  '+ 0.011*\"season\" + 0.008*\"hockey\" + 0.008*\"score\" + 0.007*\"leagu\" + '\n  '0.007*\"goal\"'),\n (9,\n  '0.013*\"peopl\" + 0.012*\"think\" + 0.011*\"like\" + 0.009*\"time\" + 0.009*\"right\" '\n  '+ 0.009*\"israel\" + 0.009*\"know\" + 0.006*\"reason\" + 0.006*\"point\" + '\n  '0.006*\"thing\"')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Transformers**","metadata":{}},{"cell_type":"code","source":"# Transformer and Torch Installation\ntry:\n  import transformers\nexcept:\n  print(\"Installing transformers\")\n  !pip -qq install transformers\n\ntry:\n  import torch\nexcept:\n  print(\"Installing Torch\")\n  !pip -qq install torch","metadata":{"execution":{"iopub.status.busy":"2024-09-17T17:13:42.834791Z","iopub.execute_input":"2024-09-17T17:13:42.835148Z","iopub.status.idle":"2024-09-17T17:13:48.035106Z","shell.execute_reply.started":"2024-09-17T17:13:42.835119Z","shell.execute_reply":"2024-09-17T17:13:48.034049Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# SST-2 Binary Classification\nfrom transformers import pipeline\n\nnlp = pipeline(\"sentiment-analysis\")\n\nprint(nlp(\"If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\"),\"If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\")\nprint(nlp(\"Effective but too-tepid biopic.\"),\"Effective but too-tepid biopic.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T17:14:11.160282Z","iopub.execute_input":"2024-09-17T17:14:11.161214Z","iopub.status.idle":"2024-09-17T17:14:31.341908Z","shell.execute_reply.started":"2024-09-17T17:14:11.161171Z","shell.execute_reply":"2024-09-17T17:14:31.34063Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a0f801e05f042ecaefd5932892ad65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c3ef3f5f2c4d22ae1143873987eba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c75b85e79740f8a960af8b128cfab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b785b387ac4eda97eb486684263f99"}},"metadata":{}},{"name":"stdout","text":"[{'label': 'POSITIVE', 'score': 0.9998257756233215}] If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\n[{'label': 'NEGATIVE', 'score': 0.9974064230918884}] Effective but too-tepid biopic.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sequence Classification : paraphrase classification\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nimport tensorflow as tf\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n\nclasses = [\"not paraphrase\", \"is paraphrase\"]\n\nsequence_A = \"The DVD-CCA then appealed to the state Supreme Court.\"\nsequence_B = \"The DVD CCA appealed that decision to the U.S. Supreme Court.\"\n\nparaphrase = tokenizer.encode_plus(sequence_A, sequence_B, return_tensors=\"tf\")\n\nparaphrase_classification_logits = model(paraphrase)[0]\n\nparaphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n\nprint(sequence_B, \"should be a paraphrase\")\nfor i in range(len(classes)):\n    print(f\"{classes[i]}: {round(paraphrase_results[i] * 100)}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T17:14:38.282579Z","iopub.execute_input":"2024-09-17T17:14:38.283305Z","iopub.status.idle":"2024-09-17T17:15:00.083963Z","shell.execute_reply.started":"2024-09-17T17:14:38.283244Z","shell.execute_reply":"2024-09-17T17:15:00.082915Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6d13df64db4dc6b0161cffd820e52a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b163e6d32ffb4241a28893516c2e1615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb29565269e4105972f5f9a416af40a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6c8f34398b4d8ca2c492ff829db969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ec769966a44085b5a982979a577320"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nAll the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"The DVD CCA appealed that decision to the U.S. Supreme Court. should be a paraphrase\nnot paraphrase: 8%\nis paraphrase: 92%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Named Entity Recognition(NER)\nfrom transformers import pipeline\nnlp = pipeline(\"ner\")\nsequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n           \"close to the Manhattan Bridge which is visible from the window.\"\nprint(nlp(sequence))","metadata":{"execution":{"iopub.status.busy":"2024-09-17T17:15:06.322341Z","iopub.execute_input":"2024-09-17T17:15:06.322743Z","iopub.status.idle":"2024-09-17T17:15:15.728684Z","shell.execute_reply.started":"2024-09-17T17:15:06.322711Z","shell.execute_reply":"2024-09-17T17:15:15.727632Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f0e5fd86c74775be9fc067bdd2fe8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0536d7e98c0a459bb4dfdddda6e1904c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9b21c488754991aac4de543cadc8ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d093ed321f6449bb3e730409f17ef49"}},"metadata":{}},{"name":"stdout","text":"[{'entity': 'I-ORG', 'score': 0.9995635, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}, {'entity': 'I-ORG', 'score': 0.99159384, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}, {'entity': 'I-ORG', 'score': 0.99826705, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}, {'entity': 'I-ORG', 'score': 0.9994404, 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}, {'entity': 'I-LOC', 'score': 0.99943465, 'index': 11, 'word': 'New', 'start': 40, 'end': 43}, {'entity': 'I-LOC', 'score': 0.99932706, 'index': 12, 'word': 'York', 'start': 44, 'end': 48}, {'entity': 'I-LOC', 'score': 0.9993864, 'index': 13, 'word': 'City', 'start': 49, 'end': 53}, {'entity': 'I-LOC', 'score': 0.98256207, 'index': 19, 'word': 'D', 'start': 79, 'end': 80}, {'entity': 'I-LOC', 'score': 0.93698275, 'index': 20, 'word': '##UM', 'start': 80, 'end': 82}, {'entity': 'I-LOC', 'score': 0.89870965, 'index': 21, 'word': '##BO', 'start': 82, 'end': 84}, {'entity': 'I-LOC', 'score': 0.97582406, 'index': 29, 'word': 'Manhattan', 'start': 113, 'end': 122}, {'entity': 'I-LOC', 'score': 0.99024945, 'index': 30, 'word': 'Bridge', 'start': 123, 'end': 129}]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Winograd\nfrom transformers import pipeline\ntranslator = pipeline(\"translation_en_to_fr\")","metadata":{"execution":{"iopub.status.busy":"2024-09-17T17:15:33.169008Z","iopub.execute_input":"2024-09-17T17:15:33.169456Z","iopub.status.idle":"2024-09-17T17:15:40.920919Z","shell.execute_reply.started":"2024-09-17T17:15:33.169417Z","shell.execute_reply":"2024-09-17T17:15:40.919071Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92eb4bf606a24e4382da029ae4cbaae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001475e449b34efabb3be4e3ef756fc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb561ea084745ed91b4ca3250d26111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e262f5792db47ec9211ad1e72a867bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5765d134458e4243b414a9bfb7b222d9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(translator(\"The terminators of the Skenet go to the attack.\", max_length=40))","metadata":{"execution":{"iopub.status.busy":"2024-09-17T17:16:43.646518Z","iopub.execute_input":"2024-09-17T17:16:43.646942Z","iopub.status.idle":"2024-09-17T17:16:44.820499Z","shell.execute_reply.started":"2024-09-17T17:16:43.646907Z","shell.execute_reply":"2024-09-17T17:16:44.819413Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[{'translation_text': \"Les terminateurs du Skenet vont à l'attaque.\"}]\n","output_type":"stream"}]}]}